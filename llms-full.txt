# AbstractFlow (llms-full)

> Generated file: a single, offline-friendly context pack for agents. It inlines local files linked from `llms.txt` (including `Optional`). Prefer `llms.txt` for navigation and treat in-repo files as source of truth.

Generated by: `scripts/generate_llms_full.py`

Included local files (repo-root relative, in order):
- llms.txt
- README.md
- docs/README.md
- docs/getting-started.md
- docs/api.md
- docs/architecture.md
- docs/faq.md
- docs/visualflow.md
- abstractflow/visual/models.py
- abstractflow/visual/interfaces.py
- abstractflow/__init__.py
- abstractflow/runner.py
- abstractflow/visual/executor.py
- abstractflow/visual/session_runner.py
- abstractflow/visual/workspace_scoped_tools.py
- abstractflow/compiler.py
- abstractflow/workflow_bundle.py
- docs/web-editor.md
- web/backend/routes/ws.py
- web/backend/routes/flows.py
- web/backend/routes/tools.py
- web/backend/services/paths.py
- web/backend/services/execution_workspace.py
- web/backend/main.py
- docs/cli.md
- abstractflow/cli.py
- pyproject.toml
- web/frontend/package.json
- tests/test_runner.py
- tests/test_workflow_bundle_pack.py
- tests/test_visual_ws_subflow.py
- CONTRIBUTING.md
- SECURITY.md
- ACKNOWLEDMENTS.md
- CHANGELOG.md
- scripts/generate_llms_full.py
- web/flows/ac-echo.json

---
file: llms.txt
---

# AbstractFlow

> Diagram-based, durable AI workflows for Python (portable VisualFlow JSON, AbstractRuntime-backed execution, reference web editor).

This repository ships:
- the Python package `abstractflow/` (PyPI: `abstractflow`)
- a reference visual editor app in `web/` (FastAPI backend + React UI)

AbstractFlow is part of the AbstractFramework ecosystem:
- https://github.com/lpalbou/AbstractFramework
- built on AbstractRuntime: https://github.com/lpalbou/abstractruntime
- and AbstractCore: https://github.com/lpalbou/abstractcore

Agent usage tips:
- All linked paths are repo-root relative.
- If you need the smallest useful context, read **Start Here** and skip **Optional**.
- Prefer `docs/` for stable, user-facing behavior; treat the linked code files as source of truth.
- This project is **Pre-alpha** (APIs may change); use tests as executable behavior specs.
- To validate changes locally, run: `pytest -q`.
- `llms-full.txt` is generated from this file and intended for offline/RAG use. It may exceed a single context window; chunk it if needed.
- After updating docs or linked files, regenerate: `python scripts/generate_llms_full.py`.

## Start Here

- [README](README.md): What AbstractFlow is + install + quickstarts
- [Docs index](docs/README.md): Documentation map
- [Getting started](docs/getting-started.md): Programmatic + VisualFlow execution
- [API reference](docs/api.md): Public Python API overview
- [Architecture](docs/architecture.md): How authoring/execution fit together (diagram)
- [FAQ](docs/faq.md): Common questions + operational details

## Formats

- [VisualFlow format](docs/visualflow.md): Portable JSON workflow document
- [VisualFlow schema](abstractflow/visual/models.py): Pydantic models (`VisualFlow`, `NodeType`, `PinType`, …)
- [Interfaces/contracts](abstractflow/visual/interfaces.py): `VisualFlow.interfaces` markers + validation/scaffolding (e.g. `abstractcode.agent.v1`)

## Key Source (Python)

- [Public API exports](abstractflow/__init__.py): what `from abstractflow import ...` provides
- [FlowRunner](abstractflow/runner.py): execution + output normalization + durable “waiting” shape
- [VisualFlow runtime wiring](abstractflow/visual/executor.py): `create_visual_runner`, registry + integrations, `execute_visual_flow`
- [VisualSessionRunner](abstractflow/visual/session_runner.py): session-scoped custom events (`on_event` / `emit_event`)
- [Workspace-scoped tools](abstractflow/visual/workspace_scoped_tools.py): default tool executor wiring (host-side)
- [Compiler shim](abstractflow/compiler.py): re-exports from `abstractruntime.visualflow_compiler`
- [WorkflowBundle helpers](abstractflow/workflow_bundle.py): `.flow` pack/inspect/unpack thin wrapper

## Web Editor Host (FastAPI)

- [Web editor guide](docs/web-editor.md): Run backend/frontend, storage locations, gateway env vars
- [WebSocket execution](web/backend/routes/ws.py): `{type:"run"|"resume"|"control"}` protocol and event stream
- [Flow CRUD + publish](web/backend/routes/flows.py): flow persistence (`ABSTRACTFLOW_FLOWS_DIR`) + bundle publishing
- [Tool discovery](web/backend/routes/tools.py): `GET /api/tools` used by the UI allowlist picker
- [Runtime persistence paths](web/backend/services/paths.py): `ABSTRACTFLOW_RUNTIME_DIR` defaulting rules
- [Execution workspaces](web/backend/services/execution_workspace.py): `ABSTRACTFLOW_BASE_EXECUTION` and workspace aliasing
- [Backend app](web/backend/main.py): FastAPI app + static serving rules

## CLI

- [CLI guide](docs/cli.md): WorkflowBundle commands + `abstractflow serve`
- [CLI implementation](abstractflow/cli.py): `abstractflow bundle …` and `abstractflow serve`

## Manifests

- [Python package manifest](pyproject.toml): version, dependencies/extras, CLI scripts
- [Frontend manifest](web/frontend/package.json): npm package name and dev/build scripts

## Tests (executable specs)

- [Runner tests](tests/test_runner.py): FlowRunner behavior and output normalization
- [Bundle tests](tests/test_workflow_bundle_pack.py): `.flow` packing semantics
- [WS execution tests](tests/test_visual_ws_subflow.py): WebSocket run/resume + subflow behavior

## Repo Policies

- [Contributing](CONTRIBUTING.md): Dev setup, tests, and contribution guidelines
- [Security policy](SECURITY.md): How to report vulnerabilities responsibly
- [Acknowledgments](ACKNOWLEDMENTS.md): dependency overview + upstream credits
- [Changelog](CHANGELOG.md): user-visible changes

## Optional

- [Full context pack](llms-full.txt): Concatenated docs for offline use (generated)
- [Pack generator](scripts/generate_llms_full.py): Regenerate `llms-full.txt`
- [Sample flow: ac-echo](web/flows/ac-echo.json): small VisualFlow JSON used in CLI examples

---
file: README.md
---

# AbstractFlow

Diagram-based, **durable** AI workflows for Python.

AbstractFlow is part of the [AbstractFramework ecosystem](https://github.com/lpalbou/AbstractFramework) and is built on:
- [AbstractRuntime](https://github.com/lpalbou/abstractruntime): durable runs, waits, subworkflows, stores
- [AbstractCore](https://github.com/lpalbou/abstractcore): providers/models/tools (used via runtime integrations)

It provides:
- A small programmatic API (`Flow`, `FlowRunner`) for building and running flows in Python.
- A portable workflow format (`VisualFlow` JSON) + helpers to execute it from any host (`abstractflow.visual`).
- A reference visual editor app in `web/` (FastAPI backend + React frontend).

Project status: **Pre-alpha** (`pyproject.toml`: `Development Status :: 2 - Pre-Alpha`). Expect breaking changes.

Evidence (code): [abstractflow/runner.py](abstractflow/runner.py), [abstractflow/visual/executor.py](abstractflow/visual/executor.py), [abstractflow/cli.py](abstractflow/cli.py), [web/backend/routes/ws.py](web/backend/routes/ws.py).

## Diagram (how it fits together)

```mermaid
flowchart LR
  UI[Visual editor UI<br/>npx @abstractframework/flow] <-->|/api/*| BE[Editor backend<br/>FastAPI (optional)]
  BE -->|save/load| FLOWS[(flows/*.json)]

  HOST[Any host process<br/>CLI / server / notebook] --> VF[VisualFlow models<br/>abstractflow/visual/models.py]
  HOST --> RUN[create_visual_runner / execute_visual_flow<br/>abstractflow/visual/executor.py]
  RUN --> RT[AbstractRuntime Runtime]
  RT -->|effects| AC[AbstractCore]
  RT --> STORES[(Run/Ledger/Artifacts stores)]
```

## Docs

- Getting started: [docs/getting-started.md](docs/getting-started.md)
- API (high-level): [docs/api.md](docs/api.md)
- Architecture: [docs/architecture.md](docs/architecture.md)
- FAQ: [docs/faq.md](docs/faq.md)
- VisualFlow JSON format: [docs/visualflow.md](docs/visualflow.md)
- CLI: [docs/cli.md](docs/cli.md)
- Visual editor: [docs/web-editor.md](docs/web-editor.md)
- Docs index: [docs/README.md](docs/README.md)

## Installation

```bash
pip install abstractflow
```

Requirements: Python **3.10+** (`pyproject.toml`: `requires-python`).

Optional extras (declared in `pyproject.toml`):
- Agent nodes (Visual Agent node support): `pip install "abstractflow[agent]"`
- Visual editor backend (FastAPI): `pip install "abstractflow[server]"`
- Visual editor backend + Agent nodes: `pip install "abstractflow[editor]"`
- Dev tools: `pip install "abstractflow[dev]"`

Notes:
- Runtime deps include `AbstractRuntime` + `abstractcore[tools]` (see `pyproject.toml`).
- Some VisualFlow nodes require extra packages at runtime (e.g. `memory_kg_*` nodes require `abstractmemory`).

## Quickstart (programmatic)

```python
from abstractflow import Flow, FlowRunner

flow = Flow("linear")
flow.add_node("double", lambda x: x * 2, input_key="value", output_key="doubled")
flow.add_node("add_ten", lambda x: x + 10, input_key="doubled", output_key="final")
flow.add_edge("double", "add_ten")
flow.set_entry("double")

print(FlowRunner(flow).run({"value": 5}))
# {"success": True, "result": 20}
```

## Quickstart (execute a VisualFlow JSON)

```python
import json
from abstractflow.visual import VisualFlow, execute_visual_flow

with open("my-flow.json", "r", encoding="utf-8") as f:
    vf = VisualFlow.model_validate(json.load(f))

print(execute_visual_flow(vf, {"prompt": "Hello"}, flows={vf.id: vf}))
```

If your flow uses subflows, load all referenced `*.json` into the `flows={...}` mapping (see [docs/getting-started.md](docs/getting-started.md)).

## Visual editor (local)

The visual editor is split into:
- a **Python backend** (FastAPI) shipped with `abstractflow[editor]` (or `abstractflow[server]`)
- a **JS frontend** published as `@abstractframework/flow` (run via `npx`)

```bash
# Terminal 1: editor backend (FastAPI)
pip install "abstractflow[editor]"
abstractflow serve --reload --port 8080

# Terminal 2: editor UI (static server + /api proxy)
npx @abstractframework/flow
```

Open:
- UI: http://localhost:3003
- Backend health: http://localhost:8080/api/health

Optional: run an AbstractGateway at http://127.0.0.1:8081 and configure it in the UI “Connect” modal (used for embeddings-backed memory KG and bundle publishing). See [docs/web-editor.md](docs/web-editor.md) and [docs/architecture.md](docs/architecture.md).

## CLI (WorkflowBundle `.flow`)

```bash
abstractflow bundle pack web/flows/ac-echo.json --out /tmp/ac-echo.flow
abstractflow bundle inspect /tmp/ac-echo.flow
abstractflow bundle unpack /tmp/ac-echo.flow --dir /tmp/ac-echo
```

See [docs/cli.md](docs/cli.md) and `abstractflow/cli.py`.

## Related projects

- AbstractFramework: https://github.com/lpalbou/AbstractFramework
- AbstractRuntime: https://github.com/lpalbou/abstractruntime
- AbstractCore: https://github.com/lpalbou/abstractcore
- AbstractAgent (optional): https://github.com/lpalbou/AbstractAgent

## Repo policies

- Changelog: [CHANGELOG.md](CHANGELOG.md)
- Contributing: [CONTRIBUTING.md](CONTRIBUTING.md)
- Security: [SECURITY.md](SECURITY.md)
- Acknowledgments: [ACKNOWLEDMENTS.md](ACKNOWLEDMENTS.md)
- License: [LICENSE](LICENSE)

---
file: docs/README.md
---

# AbstractFlow documentation

AbstractFlow is a Python library (plus a reference web UI) for authoring and executing **durable** AI workflows.

This doc set is intentionally:
- **actionable** (commands and entrypoints you can run)
- **evidence-based** (each page points to the code that implements the described behavior)

## Start here

- Project overview + install: [../README.md](../README.md)
- Getting started (quickstarts + “waiting” runs): [getting-started.md](getting-started.md)

## Find what you need

- API reference (high-level): [api.md](api.md)
- Architecture (how the pieces fit): [architecture.md](architecture.md)
- FAQ (common questions): [faq.md](faq.md)
- VisualFlow JSON format: [visualflow.md](visualflow.md)
- Visual editor (run the reference UI): [web-editor.md](web-editor.md)
- CLI (`bundle`, `serve`): [cli.md](cli.md)

## Repo policies

- Changelog: [../CHANGELOG.md](../CHANGELOG.md)
- Contributing: [../CONTRIBUTING.md](../CONTRIBUTING.md)
- Security reporting: [../SECURITY.md](../SECURITY.md)
- Acknowledgments: [../ACKNOWLEDMENTS.md](../ACKNOWLEDMENTS.md)

## Code map (evidence)

- Public Python API exports: [../abstractflow/__init__.py](../abstractflow/__init__.py)
- Programmatic flows (IR re-export from AbstractRuntime): [../abstractflow/core/flow.py](../abstractflow/core/flow.py)
- Flow execution convenience: [../abstractflow/runner.py](../abstractflow/runner.py) (`FlowRunner`)
- VisualFlow schema (portable JSON): [../abstractflow/visual/models.py](../abstractflow/visual/models.py)
- VisualFlow host wiring + execution: [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py)
- VisualFlow interface contracts: [../abstractflow/visual/interfaces.py](../abstractflow/visual/interfaces.py)
- WorkflowBundle helpers (thin wrapper): [../abstractflow/workflow_bundle.py](../abstractflow/workflow_bundle.py)
- CLI entrypoint: [../abstractflow/cli.py](../abstractflow/cli.py)
- Web backend (FastAPI): [../web/backend/main.py](../web/backend/main.py), [../web/backend/routes/](../web/backend/routes/)
- Web frontend (React): [../web/frontend/src/](../web/frontend/src/)

---
file: docs/getting-started.md
---

# Getting started

This guide covers the two primary ways to use AbstractFlow:
- **Programmatic flows** (`Flow` + `FlowRunner`)
- **Visual flows** (portable `VisualFlow` JSON authored by the editor in `web/`)

See also: [README](../README.md), [docs index](README.md), [api.md](api.md), [faq.md](faq.md), [visualflow.md](visualflow.md), [web-editor.md](web-editor.md), [cli.md](cli.md), [architecture.md](architecture.md).

## Requirements

- Python **3.10+** (`pyproject.toml`: `requires-python`)

## Install

```bash
# From PyPI
pip install abstractflow
```

Optional extras:
- Agent nodes (ReAct workflows): `pip install "abstractflow[agent]"`
- Visual editor backend (FastAPI): `pip install "abstractflow[server]"`
- Visual editor backend + Agent nodes (recommended): `pip install "abstractflow[editor]"`
- Dev tools: `pip install "abstractflow[dev]"`

From source (repo root):

```bash
pip install -e .
```

Evidence: dependencies and extras are declared in [../pyproject.toml](../pyproject.toml).

## Programmatic flow (FlowRunner)

```python
from abstractflow import Flow, FlowRunner

flow = Flow("linear")
flow.add_node("double", lambda x: x * 2, input_key="value", output_key="doubled")
flow.add_node("add_ten", lambda x: x + 10, input_key="doubled", output_key="final")
flow.add_edge("double", "add_ten")
flow.set_entry("double")

print(FlowRunner(flow).run({"value": 5}))
# {"success": True, "result": 20}
```

Evidence:
- `Flow` / `FlowNode` / `FlowEdge` are re-exported from AbstractRuntime: [../abstractflow/core/flow.py](../abstractflow/core/flow.py)
- `FlowRunner` output normalization and “waiting” shape: [../abstractflow/runner.py](../abstractflow/runner.py)
- Baseline behavior: [../tests/test_runner.py](../tests/test_runner.py)

## Execute a VisualFlow JSON

Visual flows are JSON documents matching the Pydantic models in `abstractflow/visual/models.py`.

Minimal example (single flow, no subflows):

```python
import json
from abstractflow.visual import VisualFlow, execute_visual_flow

with open("my-flow.json", "r", encoding="utf-8") as f:
    vf = VisualFlow.model_validate(json.load(f))
print(execute_visual_flow(vf, {"prompt": "Hello"}, flows={vf.id: vf}))
```

If your flow uses subflows:
- load **all referenced** `*.json` flows into the `flows={flow_id: VisualFlow}` mapping, or
- package them as a WorkflowBundle (`.flow`) and load via AbstractRuntime (see [cli.md](cli.md)).

Convenient loader:

```python
from pathlib import Path
import json
from abstractflow.visual import VisualFlow

def load_flows(dir_path: str) -> dict[str, VisualFlow]:
    flows: dict[str, VisualFlow] = {}
    for p in Path(dir_path).glob("*.json"):
        vf = VisualFlow.model_validate(json.loads(p.read_text(encoding="utf-8")))
        flows[vf.id] = vf
    return flows
```

Evidence:
- VisualFlow execution wiring: [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py)
- Subflow reachability / registry behavior: [../tests/test_visual_subflow_registry_reachability.py](../tests/test_visual_subflow_registry_reachability.py), [../tests/test_visual_subflow_recursion.py](../tests/test_visual_subflow_recursion.py)

## Run the visual editor (local)

The editor is a reference app (FastAPI backend + React frontend). Follow: [web-editor.md](web-editor.md).

Quick start (no repo clone needed):

```bash
pip install "abstractflow[editor]"
abstractflow serve --reload --port 8080
npx @abstractframework/flow
```

Tip (from source): install the backend deps from the repo root with `pip install -e ".[server,agent]"`.

## Workflow bundles (`.flow`)

To package a VisualFlow + subflows into a single file, use the CLI:
- [cli.md](cli.md)

## Waiting runs (durable asks/events/schedules)

Some flows intentionally block waiting for external input (e.g. `ask_user`, `wait_event`, `wait_until`).

- `FlowRunner.run()` returns `{"waiting": True, "state": <RunState>, ...}` when blocked (`abstractflow/runner.py`).
- `execute_visual_flow()` returns a friendly shape including `waiting`, `wait_key`, and optional UX fields (`prompt`, `choices`, `allow_free_text`) (`abstractflow/visual/executor.py`).
  - Note: waiting results are reported as `success: False` with an `error` message (the run is not “failed”; it is blocked on input).

To resume a run you need a host that can call `Runtime.resume(...)` (the web editor does this via WebSocket; see [web-editor.md](web-editor.md)).

---
file: docs/api.md
---

# API reference (high-level)

This page documents the public Python API surface of the `abstractflow` package.

See also: [../README.md](../README.md), [getting-started.md](getting-started.md), [architecture.md](architecture.md), [faq.md](faq.md).

## Version

- `abstractflow.__version__` (string)
- `abstractflow.get_version()` (helper)

Evidence: [../abstractflow/__init__.py](../abstractflow/__init__.py), [../pyproject.toml](../pyproject.toml).

## Programmatic flows

### Flow IR

`Flow`, `FlowNode`, and `FlowEdge` are re-exported from AbstractRuntime so there is a single source of truth for semantics.

```python
from abstractflow import Flow, FlowNode, FlowEdge
```

Evidence: [../abstractflow/core/flow.py](../abstractflow/core/flow.py), [../abstractflow/__init__.py](../abstractflow/__init__.py).

### FlowRunner

`FlowRunner` compiles a `Flow` to a runtime `WorkflowSpec` and executes it using an AbstractRuntime `Runtime`.

```python
from abstractflow import FlowRunner
```

Key behaviors:
- Creates a default in-memory runtime when you don’t provide one.
- Normalizes completion output into `{"success": bool, "result": ...}`.
- Returns `{"waiting": True, ...}` if the flow blocks on durable input.
- Provides `start(...)`, `step(...)`, and `resume(...)` for host-driven execution loops.

Evidence: [../abstractflow/runner.py](../abstractflow/runner.py).

### Compilation

Compilation functions are delegated to AbstractRuntime’s VisualFlow compiler and re-exported:

```python
from abstractflow import compile_flow
```

Advanced compilation helpers are also re-exported (typically used by hosts/tools, not most end users):

```python
from abstractflow.compiler import compile_visualflow, compile_visualflow_tree
```

Evidence: [../abstractflow/compiler.py](../abstractflow/compiler.py), [../abstractflow/__init__.py](../abstractflow/__init__.py).

## Visual flows (VisualFlow JSON)

### Models

Pydantic models for the portable JSON format:

```python
from abstractflow.visual import VisualFlow, VisualNode, VisualEdge, NodeType, PinType
```

Evidence: [../abstractflow/visual/models.py](../abstractflow/visual/models.py), [../abstractflow/visual/__init__.py](../abstractflow/visual/__init__.py).

### Execute a VisualFlow

Use `execute_visual_flow(...)` for a simple “run and return a result” call:

```python
from abstractflow.visual import execute_visual_flow
```

For advanced use cases (custom stores/tool execution, or access to run state/ledger), build a runner:

```python
from abstractflow.visual import create_visual_runner
```

Utilities:

```python
from abstractflow.visual import visual_to_flow
```

Evidence: [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py), [getting-started.md](getting-started.md).

### Interfaces/contracts (optional)

If a host expects a specific IO contract, VisualFlows can declare interface markers in `VisualFlow.interfaces`.

```python
from abstractflow.visual.interfaces import (
    ABSTRACTCODE_AGENT_V1,
    validate_visual_flow_interface,
    apply_visual_flow_interface_scaffold,
)
```

Evidence: [../abstractflow/visual/interfaces.py](../abstractflow/visual/interfaces.py).

## Workflow bundles (`.flow`)

WorkflowBundle helpers are available as a thin wrapper around AbstractRuntime’s bundle implementation:

```python
from abstractflow.workflow_bundle import (
    pack_workflow_bundle,
    inspect_workflow_bundle,
    unpack_workflow_bundle,
)
```

Evidence: [../abstractflow/workflow_bundle.py](../abstractflow/workflow_bundle.py), [cli.md](cli.md).

## Adapters (advanced)

If you build custom hosts or want direct control over node handler construction, adapters are re-exported:

```python
from abstractflow.adapters import (
    create_function_node_handler,
    create_agent_node_handler,
    create_subflow_node_handler,
)
```

Evidence: [../abstractflow/adapters/](../abstractflow/adapters/).

## CLI

The `abstractflow` CLI entry point is declared in `pyproject.toml` (`project.scripts`) and implemented in:
- [../abstractflow/cli.py](../abstractflow/cli.py)

The CLI includes:
- WorkflowBundle tools: `abstractflow bundle ...`
- Visual editor backend runner (optional): `abstractflow serve ...` (requires `abstractflow[server]`)

Evidence: [../pyproject.toml](../pyproject.toml), [../abstractflow/cli.py](../abstractflow/cli.py), [../web/backend/cli.py](../web/backend/cli.py).

---
file: docs/architecture.md
---

# AbstractFlow — Architecture (Current)

> Updated: 2026-02-09  
> Scope: describes **implemented behavior** in this repository (no roadmap claims).

AbstractFlow is a workflow authoring + orchestration layer in the [AbstractFramework ecosystem](https://github.com/lpalbou/AbstractFramework), built on:
- **AbstractRuntime**: durable runs, waits, subworkflows, stores (`RunStore`/`LedgerStore`/`ArtifactStore`)
- **AbstractCore** (via runtime integration): LLM + tool effects
- **AbstractAgent** (optional): Agent node subworkflows (ReAct)
- **AbstractMemory** (optional): memory/KG nodes

See also: [../README.md](../README.md), [getting-started.md](getting-started.md), [api.md](api.md), [faq.md](faq.md), [visualflow.md](visualflow.md), [web-editor.md](web-editor.md), [cli.md](cli.md).

## Repository layout (what ships where)

```
abstractflow/                  # Published Python package
  __init__.py                  # Public API exports
  core/flow.py                 # Flow IR re-export (from AbstractRuntime)
  runner.py                    # FlowRunner (runtime-backed)
  compiler.py                  # Compiler shim (delegates to AbstractRuntime)
  visual/                      # VisualFlow models + portable execution wiring
  adapters/                    # Adapter re-exports (delegates to AbstractRuntime)
  cli.py                       # `abstractflow` CLI
  workflow_bundle.py           # Bundle helpers (delegates to AbstractRuntime)
docs/                          # Human docs (this folder)
web/                           # Reference visual editor app (backend shipped via `abstractflow[server]`)
  backend/                     # FastAPI backend (CRUD + websocket execution)
  frontend/                    # React editor + run UI
  flows/                       # Default flow storage when running backend from `web/`
  runtime/                     # Default runtime persistence in a source checkout (installed: ~/.abstractflow/runtime)
tests/                         # Test suite
```

## High-level data and execution flow

```mermaid
flowchart LR
  subgraph Authoring
    FE[web/frontend<br/>React editor] -->|save/load VisualFlow JSON| BE[web/backend<br/>FastAPI]
    BE -->|persists| FLOWS[(web/flows/*.json)]
  end

  subgraph Execution
    HOST[Host process<br/>(web backend / CLI / 3rd party)] -->|validate| VF[VisualFlow models<br/>abstractflow/visual/models.py]
    HOST -->|create_visual_runner| WIRE[Runtime wiring<br/>abstractflow/visual/executor.py]
    WIRE --> RT[AbstractRuntime Runtime<br/>tick/resume]
    RT --> STORES[(RunStore / LedgerStore / ArtifactStore)]
    RT -->|LLM_CALL, TOOL_CALLS| AC[AbstractCore integration]
    RT -->|START_SUBWORKFLOW| REG[WorkflowRegistry]
  end

  BE -->|WS: run/resume/control| HOST
```

## Portable data model: VisualFlow JSON

The portable authoring format is `VisualFlow` (Pydantic models):
- `VisualFlow`, `VisualNode`, `VisualEdge`, `NodeType`, `PinType`, …

Evidence: [../abstractflow/visual/models.py](../abstractflow/visual/models.py).

Key portability rule (enforced by design): the JSON must contain enough configuration to execute outside the web backend. Hosts may add storage, auth, and UI around it, but execution should remain host-independent.

## Compilation and execution (portable)

### VisualFlow → Flow IR

AbstractFlow delegates “VisualFlow → Flow IR” semantics to AbstractRuntime:
- `abstractflow.visual.executor.visual_to_flow()` calls `abstractruntime.visualflow_compiler.visual_to_flow(...)`.

Evidence: [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py).

### Flow IR → WorkflowSpec

AbstractFlow delegates compilation to AbstractRuntime:
- `abstractflow.compiler.compile_flow` is re-exported from `abstractruntime.visualflow_compiler.compiler`.

Evidence: [../abstractflow/compiler.py](../abstractflow/compiler.py).

### Running (FlowRunner)

`FlowRunner` owns host-friendly execution convenience:
- creates a default in-memory runtime when you don’t provide one
- normalizes outputs to `{"success": bool, "result": ...}` for callers
- can auto-drive nested `SUBWORKFLOW` waits in non-interactive contexts

Evidence: [../abstractflow/runner.py](../abstractflow/runner.py), tests in [../tests/test_runner.py](../tests/test_runner.py).

## VisualFlow execution wiring (host responsibilities)

The key host entrypoint is:
- `abstractflow.visual.executor.create_visual_runner(...)`

It wires the runtime based on **what is present in the flow tree**:
- registers subflows/agent workflows when needed (workflow registry)
- enables artifact storage when memory nodes are present
- wires AbstractCore effect handlers when LLM/tool nodes are present
- optionally installs AbstractMemory KG effect handlers when `memory_kg_*` nodes are present

Evidence: [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py).

## Session-scoped events (VisualSessionRunner)

VisualFlows that include custom events (`on_event` / `emit_event`) are executed with a session-aware runner:
- `VisualSessionRunner` starts derived event-listener workflows as **child runs** in the same session.
- During `run()`, it also ticks those child runs so `EMIT_EVENT` branches make progress without a separate host loop.

Evidence: [../abstractflow/visual/session_runner.py](../abstractflow/visual/session_runner.py), wiring in [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py), tests in [../tests/test_visual_custom_events.py](../tests/test_visual_custom_events.py).

## Web editor host (FastAPI + WebSockets)

The reference host in `web/` provides:
- Flow CRUD (`web/backend/routes/flows.py`) storing `./flows/*.json` relative to its working dir
- Durable stores for runs/ledger/artifacts (`web/backend/services/runtime_stores.py`)
- WebSocket execution (`web/backend/routes/ws.py`) with message types:
  - `{ "type": "run", "input_data": {…} }`
  - `{ "type": "resume", "response": "…" }`
  - `{ "type": "control", "action": "pause|resume|cancel", "run_id": "…" }`

See [web-editor.md](web-editor.md) for run instructions.

## Workflow bundles (`.flow`)

WorkflowBundles package a root VisualFlow JSON plus any referenced subflows into a single `.flow` (zip) file (manifest + flow JSON files).

- CLI: `abstractflow bundle pack|inspect|unpack` (`abstractflow/cli.py`)
- Implementation delegates to AbstractRuntime: `abstractflow/workflow_bundle.py`
- Format/packing semantics are owned by AbstractRuntime; AbstractFlow is a thin wrapper.

Evidence: [../tests/test_workflow_bundle_pack.py](../tests/test_workflow_bundle_pack.py), [../abstractflow/workflow_bundle.py](../abstractflow/workflow_bundle.py).

## What AbstractFlow owns vs delegates

**Owns in this repo**
- VisualFlow schema (`abstractflow/visual/models.py`)
- Host wiring helpers (`abstractflow/visual/executor.py`, `abstractflow/visual/session_runner.py`)
- Public runner conveniences (`abstractflow/runner.py`)
- Reference web editor app (`web/`)
- CLI wrapper (`abstractflow/cli.py`)

**Delegates to AbstractRuntime**
- Compilation semantics and builtins (`abstractflow/compiler.py`, `abstractflow/visual/builtins.py`)
- Adapter implementations (`abstractflow/adapters/*`)
- WorkflowBundle format and IO (`abstractflow/workflow_bundle.py`)

---
file: docs/faq.md
---

# FAQ

See also: [../README.md](../README.md), [getting-started.md](getting-started.md), [api.md](api.md), [architecture.md](architecture.md).

## What is AbstractFlow?

AbstractFlow is a Python library for defining and executing **durable** AI workflows:
- Programmatic graphs (`Flow` + `FlowRunner`)
- Portable visual workflows (`VisualFlow` JSON) that can run outside the editor

Evidence: [../abstractflow/runner.py](../abstractflow/runner.py), [../abstractflow/visual/models.py](../abstractflow/visual/models.py), [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py).

## Is AbstractFlow production-ready?

Not yet. The package is marked **Pre-alpha** and may introduce breaking changes.

Evidence: [../pyproject.toml](../pyproject.toml) (`Development Status :: 2 - Pre-Alpha`).

## What’s the difference between `Flow` and `VisualFlow`?

- `Flow`: programmatic flow IR (re-exported from AbstractRuntime) used by `FlowRunner`.
- `VisualFlow`: portable JSON authoring format (Pydantic models) produced by the web editor and runnable from any host.

Evidence: [../abstractflow/core/flow.py](../abstractflow/core/flow.py), [../abstractflow/visual/models.py](../abstractflow/visual/models.py), [../abstractflow/runner.py](../abstractflow/runner.py).

## Can I execute a VisualFlow JSON without running the web editor?

Yes. Load the JSON into `VisualFlow` and run it with `abstractflow.visual.execute_visual_flow(...)` (or build a runner with `create_visual_runner(...)` if you need access to the runtime/run state).

Evidence: [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py).

## How do subflows work?

Subflows are VisualFlows referenced by id from nodes of type `subflow`:
- `node.data["subflowId"]` (legacy: `flowId`)

When executing, you must provide a mapping of all flows by id: `flows={flow_id: VisualFlow, ...}`.

Evidence: [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py), [visualflow.md](visualflow.md).

## How do “waiting” runs work? How do I resume?

Some nodes intentionally block on external input (e.g. user/event/schedule waits).
- `FlowRunner.run()` returns `{"waiting": True, ...}` when blocked.
- The web editor resumes blocked runs over WebSocket (`type:"resume"`).

Evidence: [../abstractflow/runner.py](../abstractflow/runner.py), [../web/backend/routes/ws.py](../web/backend/routes/ws.py), [web-editor.md](web-editor.md).

## How do custom events work in VisualFlow?

For VisualFlows, `VisualSessionRunner` starts `on_event` listeners as **child runs** in the same session and ticks them so `emit_event` branches progress.

Evidence: [../abstractflow/visual/session_runner.py](../abstractflow/visual/session_runner.py), wiring in [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py).

## Does `pip install abstractflow` include the web editor UI?

Not the UI. The visual editor has two parts:
- Backend (FastAPI): included when you install `abstractflow[editor]` (or `abstractflow[server]`) and runnable via `abstractflow serve`.
- UI (React): published as the npm package `@abstractframework/flow` (run via `npx`).

Evidence: [../pyproject.toml](../pyproject.toml) (`server` extra + `project.scripts`), [../abstractflow/cli.py](../abstractflow/cli.py), [../web/frontend/bin/cli.js](../web/frontend/bin/cli.js).

## Where does the web editor store flows and run data?

Defaults:
- Flows: `./flows/*.json` relative to the backend working directory (override with `ABSTRACTFLOW_FLOWS_DIR`).
- Runtime persistence (runs/ledger/artifacts):
  - source checkout: `web/runtime/`
  - installed package: `~/.abstractflow/runtime`
  - override with `ABSTRACTFLOW_RUNTIME_DIR`.

Evidence: [../web/backend/routes/flows.py](../web/backend/routes/flows.py) (`FLOWS_DIR`, `ABSTRACTFLOW_FLOWS_DIR`), [../web/backend/services/paths.py](../web/backend/services/paths.py).

## How does tool / file access work (security)?

The web backend creates a per-run workspace directory and wraps tool execution with workspace scoping:
- Workspace base: `ABSTRACTFLOW_BASE_EXECUTION` (or `/tmp` / OS temp)
- Workspace root is injected into `input_data` (`workspace_root`) and used to scope tools

Evidence: [../web/backend/services/execution_workspace.py](../web/backend/services/execution_workspace.py), [../abstractflow/visual/workspace_scoped_tools.py](../abstractflow/visual/workspace_scoped_tools.py), [../web/backend/routes/ws.py](../web/backend/routes/ws.py), [../web/backend/routes/flows.py](../web/backend/routes/flows.py).

## How do tools work? How do I add more tools?

The editor backend exposes a **conservative default tool set** derived from AbstractRuntime’s AbstractCore integration.

To add or customize tools, you have a few host-level options:

- **Custom host (Python)**: build your own tool executor and pass it to `create_visual_runner(...)`.
- **Editor backend**: extend the tool discovery route (`GET /api/tools`) and the host tool executor used for runs.
- **Upstream defaults**: depending on your deployment, you may choose to replace/extend AbstractRuntime’s “default tools” selection.

Evidence:
- Tool discovery endpoint: [../web/backend/routes/tools.py](../web/backend/routes/tools.py) (`GET /api/tools`)
- Default tool executor wiring: [../abstractflow/visual/workspace_scoped_tools.py](../abstractflow/visual/workspace_scoped_tools.py)
- Editor backend wiring: [../web/backend/routes/flows.py](../web/backend/routes/flows.py), [../web/backend/routes/ws.py](../web/backend/routes/ws.py)
- Run guide: [web-editor.md](web-editor.md)

## How do I package and share workflows?

Use WorkflowBundle (`.flow`):
- CLI: `abstractflow bundle pack|inspect|unpack`
- The bundle format and packer are owned by AbstractRuntime; AbstractFlow provides a thin wrapper.

Evidence: [../abstractflow/cli.py](../abstractflow/cli.py), [../abstractflow/workflow_bundle.py](../abstractflow/workflow_bundle.py), tests in [../tests/test_workflow_bundle_pack.py](../tests/test_workflow_bundle_pack.py).

## Do I need an AbstractGateway?

Not necessarily. VisualFlow execution is runtime-based and can run locally with AbstractCore integration. The web editor can optionally connect to a gateway (URL/token) for catalogs and bundle upload/reload.

Evidence: [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py) (gateway token resolution), [../web/backend/services/gateway_connection.py](../web/backend/services/gateway_connection.py), [../web/backend/routes/flows.py](../web/backend/routes/flows.py) (publish/upload/reload).

## Why do I see pins in `node.data.inputs/outputs` instead of `node.inputs/outputs`?

Saved flows from the editor store pin metadata under `node.data.inputs` / `node.data.outputs`. The top-level `inputs` / `outputs` fields may exist but are often empty.

Evidence: [../abstractflow/visual/interfaces.py](../abstractflow/visual/interfaces.py) (`_pin_types` reads `node.data.*`), sample flows in [../web/flows/](../web/flows/).

## Where is the “compiler” implemented?

Compilation semantics live in AbstractRuntime’s VisualFlow compiler. This package delegates and re-exports:
- `abstractflow/compiler.py` (compile functions)
- `abstractflow/adapters/*` and `abstractflow/visual/builtins.py` (node adapters/builtins)

Evidence: [../abstractflow/compiler.py](../abstractflow/compiler.py), [../abstractflow/adapters/](../abstractflow/adapters/), [../abstractflow/visual/builtins.py](../abstractflow/visual/builtins.py).

---
file: docs/visualflow.md
---

# VisualFlow (portable JSON workflow format)

`VisualFlow` is the portable workflow document produced by the visual editor in `web/frontend/` and persisted by the backend in `web/backend/`.

The schema lives in [../abstractflow/visual/models.py](../abstractflow/visual/models.py) (Pydantic models). Any host can:
- load/validate the JSON into `VisualFlow`
- execute it using `abstractflow.visual` helpers

See also: [../README.md](../README.md), [getting-started.md](getting-started.md), [faq.md](faq.md), [web-editor.md](web-editor.md), [architecture.md](architecture.md).

## Minimal schema (what to expect)

- `VisualFlow`
  - `id: str`
  - `name: str`, `description: str`
  - `interfaces: list[str]` (optional host contracts)
  - `nodes: list[VisualNode]`, `edges: list[VisualEdge]`
  - `entryNode: str | null` (optional, Blueprint-style execution root)
- `VisualNode`
  - `id: str`, `type: NodeType`, `position: {x,y}`
  - `data: dict` (node config + pin metadata)
- `VisualEdge`
  - `source`, `sourceHandle`, `target`, `targetHandle`

Evidence: [../abstractflow/visual/models.py](../abstractflow/visual/models.py).

## Node types and pins

- The full list of node types is `NodeType` in [../abstractflow/visual/models.py](../abstractflow/visual/models.py).
- Pin types are `PinType` in [../abstractflow/visual/models.py](../abstractflow/visual/models.py) (and mirrored for UI concerns in [../web/frontend/src/types/flow.ts](../web/frontend/src/types/flow.ts)).

Two edge “kinds” are used by convention:
- **Execution edges**: connect to the target handle `exec-in` (Blueprint-style control flow).
- **Data edges**: connect non-exec handles and carry values between pins.

Evidence:
- VisualFlow runner wiring uses execution-graph reachability (`targetHandle == "exec-in"`) in [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py).
- UI colors data edges by pin type in [../web/frontend/src/components/Canvas.tsx](../web/frontend/src/components/Canvas.tsx).

Note on pins in saved JSON:
- The editor persists pin definitions under `node.data.inputs` / `node.data.outputs`.
- The top-level `node.inputs` / `node.outputs` fields may be present but empty.

Evidence: [../abstractflow/visual/interfaces.py](../abstractflow/visual/interfaces.py) (`_pin_types` reads `node.data.*`) and sample flows in [../web/flows/](../web/flows/).

## Subflows

Subflows are regular VisualFlows referenced by id from a node of type `subflow`.

Convention:
- `node.type == "subflow"`
- `node.data["subflowId"]` holds the referenced flow id (legacy key `flowId` is tolerated).

Evidence:
- Runner wiring resolves subflows in [../abstractflow/visual/executor.py](../abstractflow/visual/executor.py) (`subflowId` / legacy `flowId`)
- Bundle packing is delegated to AbstractRuntime via [../abstractflow/workflow_bundle.py](../abstractflow/workflow_bundle.py) (see [../tests/test_workflow_bundle_pack.py](../tests/test_workflow_bundle_pack.py)).
- Tests: [../tests/test_visual_subflow_registry_reachability.py](../tests/test_visual_subflow_registry_reachability.py), [../tests/test_visual_subflow_recursion.py](../tests/test_visual_subflow_recursion.py)

## Interfaces (optional host contracts)

`VisualFlow.interfaces` is a list of interface markers a host can interpret as “this workflow supports a known IO contract”.

AbstractFlow ships:
- `abstractcode.agent.v1` (`ABSTRACTCODE_AGENT_V1`) with validators and scaffolding helpers

Evidence: [../abstractflow/visual/interfaces.py](../abstractflow/visual/interfaces.py).

---
file: abstractflow/visual/models.py
---

"""Pydantic models for the AbstractFlow visual workflow JSON format.

These models are intentionally kept in the `abstractflow` package so workflows
authored in the visual editor can be loaded and executed from any host (CLI,
AbstractCode, servers), not only the web backend.
"""

from __future__ import annotations

from enum import Enum
from datetime import datetime, timezone
import uuid
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field


class PinType(str, Enum):
    """Types of pins with their colors."""

    EXECUTION = "execution"  # White #FFFFFF - Flow control
    STRING = "string"  # Magenta #FF00FF - Text data
    NUMBER = "number"  # Green #00FF00 - Integer/Float
    BOOLEAN = "boolean"  # Red #FF0000 - True/False
    OBJECT = "object"  # Cyan #00FFFF - JSON objects
    MEMORY = "memory"  # Mint - Memory configuration object (KG/span/session controls)
    ASSERTION = "assertion"  # Teal - KG assertion object (subject/predicate/object + metadata)
    ASSERTIONS = "assertions"  # Teal - List of KG assertion objects (assertion[])
    ARRAY = "array"  # Orange #FF8800 - Collections
    TOOLS = "tools"  # Orange - Tool allowlist (string[])
    PROVIDER = "provider"  # Cyan-blue - LLM provider id/name (string-like)
    MODEL = "model"  # Purple - LLM model id/name (string-like)
    AGENT = "agent"  # Blue #4488FF - Agent reference
    ANY = "any"  # Gray #888888 - Accepts any type


class NodeType(str, Enum):
    """Types of nodes in the visual editor."""

    # Event/Trigger nodes (entry points)
    ON_FLOW_START = "on_flow_start"
    ON_USER_REQUEST = "on_user_request"
    ON_AGENT_MESSAGE = "on_agent_message"
    ON_SCHEDULE = "on_schedule"
    ON_EVENT = "on_event"
    # Flow IO nodes
    ON_FLOW_END = "on_flow_end"
    # Core execution nodes
    AGENT = "agent"
    FUNCTION = "function"
    CODE = "code"
    SUBFLOW = "subflow"
    # Math
    ADD = "add"
    SUBTRACT = "subtract"
    MULTIPLY = "multiply"
    DIVIDE = "divide"
    MODULO = "modulo"
    POWER = "power"
    ABS = "abs"
    ROUND = "round"
    RANDOM_INT = "random_int"
    RANDOM_FLOAT = "random_float"
    # String
    CONCAT = "concat"
    SPLIT = "split"
    JOIN = "join"
    FORMAT = "format"
    STRING_TEMPLATE = "string_template"
    UPPERCASE = "uppercase"
    LOWERCASE = "lowercase"
    TRIM = "trim"
    SUBSTRING = "substring"
    LENGTH = "length"
    CONTAINS = "contains"
    REPLACE = "replace"
    # Control
    IF = "if"
    SWITCH = "switch"
    LOOP = "loop"
    WHILE = "while"
    FOR = "for"
    SEQUENCE = "sequence"
    PARALLEL = "parallel"
    COMPARE = "compare"
    NOT = "not"
    AND = "and"
    OR = "or"
    COALESCE = "coalesce"
    # Data
    GET = "get"
    SET = "set"
    MERGE = "merge"
    MAKE_ARRAY = "make_array"
    MAKE_OBJECT = "make_object"
    MAKE_CONTEXT = "make_context"
    MAKE_META = "make_meta"
    MAKE_SCRATCHPAD = "make_scratchpad"
    GET_ELEMENT = "get_element"
    GET_RANDOM_ELEMENT = "get_random_element"
    ARRAY_MAP = "array_map"
    ARRAY_FILTER = "array_filter"
    ARRAY_CONCAT = "array_concat"
    ARRAY_LENGTH = "array_length"
    ARRAY_APPEND = "array_append"
    ARRAY_DEDUP = "array_dedup"
    HAS_TOOLS = "has_tools"
    ADD_MESSAGE = "add_message"
    GET_CONTEXT = "get_context"
    GET_VAR = "get_var"
    SET_VAR = "set_var"
    SET_VARS = "set_vars"
    SET_VAR_PROPERTY = "set_var_property"
    PARSE_JSON = "parse_json"
    STRINGIFY_JSON = "stringify_json"
    FORMAT_TOOL_RESULTS = "format_tool_results"
    AGENT_TRACE_REPORT = "agent_trace_report"
    BREAK_OBJECT = "break_object"
    SYSTEM_DATETIME = "system_datetime"
    MODEL_CATALOG = "model_catalog"
    PROVIDER_CATALOG = "provider_catalog"
    PROVIDER_MODELS = "provider_models"
    # Literals
    LITERAL_STRING = "literal_string"
    LITERAL_NUMBER = "literal_number"
    LITERAL_BOOLEAN = "literal_boolean"
    LITERAL_JSON = "literal_json"
    JSON_SCHEMA = "json_schema"
    LITERAL_ARRAY = "literal_array"
    TOOL_PARAMETERS = "tool_parameters"
    # Effects
    ASK_USER = "ask_user"
    ANSWER_USER = "answer_user"
    LLM_CALL = "llm_call"
    WAIT_UNTIL = "wait_until"
    WAIT_EVENT = "wait_event"
    EMIT_EVENT = "emit_event"
    READ_FILE = "read_file"
    WRITE_FILE = "write_file"
    MEMORY_NOTE = "memory_note"
    MEMORY_QUERY = "memory_query"
    MEMORY_TAG = "memory_tag"
    MEMORY_COMPACT = "memory_compact"
    MEMORY_REHYDRATE = "memory_rehydrate"
    MEMORY_KG_ASSERT = "memory_kg_assert"
    MEMORY_KG_QUERY = "memory_kg_query"
    MEMORY_KG_RESOLVE = "memory_kg_resolve"
    MEMACT_COMPOSE = "memact_compose"
    TOOL_CALLS = "tool_calls"
    CALL_TOOL = "call_tool"
    TOOLS_ALLOWLIST = "tools_allowlist"
    BOOL_VAR = "bool_var"
    VAR_DECL = "var_decl"


class Pin(BaseModel):
    """A connection point on a node."""

    id: str
    label: str
    type: PinType


class Position(BaseModel):
    """2D position on canvas."""

    x: float
    y: float


class VisualNode(BaseModel):
    """A node in the visual flow editor."""

    id: str = Field(default_factory=lambda: str(uuid.uuid4())[:8])
    type: NodeType
    position: Position
    data: Dict[str, Any] = Field(default_factory=dict)
    # Node display properties (from template)
    label: Optional[str] = None
    icon: Optional[str] = None
    headerColor: Optional[str] = None
    inputs: List[Pin] = Field(default_factory=list)
    outputs: List[Pin] = Field(default_factory=list)


class VisualEdge(BaseModel):
    """An edge connecting two nodes."""

    id: str = Field(default_factory=lambda: str(uuid.uuid4())[:8])
    source: str
    sourceHandle: str  # Pin ID on source node
    target: str
    targetHandle: str  # Pin ID on target node
    animated: bool = False


class VisualFlow(BaseModel):
    """A complete visual flow definition."""

    id: str = Field(default_factory=lambda: str(uuid.uuid4())[:8])
    name: str
    description: str = ""
    # Optional interface markers (host contracts).
    # Example: ["abstractcode.agent.v1"] to indicate this workflow can be run as a RunnableFlow in chat-like clients.
    interfaces: List[str] = Field(default_factory=list)
    nodes: List[VisualNode] = Field(default_factory=list)
    edges: List[VisualEdge] = Field(default_factory=list)
    entryNode: Optional[str] = None
    created_at: Optional[str] = None
    updated_at: Optional[str] = None


class FlowCreateRequest(BaseModel):
    """Request to create a new flow."""

    name: str
    description: str = ""
    interfaces: List[str] = Field(default_factory=list)
    nodes: List[VisualNode] = Field(default_factory=list)
    edges: List[VisualEdge] = Field(default_factory=list)
    entryNode: Optional[str] = None


class FlowUpdateRequest(BaseModel):
    """Request to update an existing flow."""

    name: Optional[str] = None
    description: Optional[str] = None
    interfaces: Optional[List[str]] = None
    nodes: Optional[List[VisualNode]] = None
    edges: Optional[List[VisualEdge]] = None
    entryNode: Optional[str] = None


class FlowRunRequest(BaseModel):
    """Request to execute a flow."""

    input_data: Dict[str, Any] = Field(default_factory=dict)


class FlowRunResult(BaseModel):
    """Result of a flow execution."""

    success: bool
    result: Optional[Any] = None
    error: Optional[str] = None
    run_id: Optional[str] = None
    waiting: bool = False
    wait_key: Optional[str] = None
    prompt: Optional[str] = None
    choices: Optional[List[str]] = None
    allow_free_text: Optional[bool] = None


class ExecutionMetrics(BaseModel):
    """Optional per-step (or whole-run) execution metrics.

    These fields are best-effort and may be omitted depending on host/runtime capabilities.
    """

    duration_ms: Optional[float] = None
    input_tokens: Optional[int] = None
    output_tokens: Optional[int] = None
    tokens_per_s: Optional[float] = None


class ExecutionEvent(BaseModel):
    """Real-time execution event for WebSocket."""

    type: str  # "node_start", "node_complete", "flow_complete", "flow_error"
    # ISO 8601 UTC timestamp for event emission (host-side observability).
    ts: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    runId: Optional[str] = None
    nodeId: Optional[str] = None
    result: Optional[Any] = None
    error: Optional[str] = None
    meta: Optional[ExecutionMetrics] = None

---
file: abstractflow/visual/interfaces.py
---

"""VisualFlow interface contracts (portable host validation).

This module defines *declarative* workflow interface markers and best-effort
validators so hosts (e.g. AbstractCode) can safely treat a workflow as a
specialized capability with a known IO contract.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Mapping, Optional, Tuple

from .models import VisualFlow


ABSTRACTCODE_AGENT_V1 = "abstractcode.agent.v1"


@dataclass(frozen=True)
class VisualFlowInterfaceSpec:
    interface_id: str
    label: str
    description: str
    required_start_outputs: Mapping[str, str]
    required_end_inputs: Mapping[str, str]
    recommended_start_outputs: Mapping[str, str] = None  # type: ignore[assignment]
    recommended_end_inputs: Mapping[str, str] = None  # type: ignore[assignment]


def _pin_types(pins: Any) -> Dict[str, str]:
    """Return {pin_id -> type_str} for a pin list.

    VisualFlow stores pins inside the node's `data.inputs/outputs` lists.
    """
    out: Dict[str, str] = {}
    if not isinstance(pins, list):
        return out
    for p in pins:
        if not isinstance(p, dict):
            continue
        pid = p.get("id")
        if not isinstance(pid, str) or not pid:
            continue
        ptype = p.get("type")
        t = ptype.value if hasattr(ptype, "value") else str(ptype or "")
        out[pid] = t
    return out


def _node_type_str(node: Any) -> str:
    t = getattr(node, "type", None)
    return t.value if hasattr(t, "value") else str(t or "")


def _iter_nodes(flow: VisualFlow) -> Iterable[Any]:
    for n in getattr(flow, "nodes", []) or []:
        yield n


def get_interface_specs() -> Dict[str, VisualFlowInterfaceSpec]:
    """Return known interface specs (by id)."""
    return {
        ABSTRACTCODE_AGENT_V1: VisualFlowInterfaceSpec(
            interface_id=ABSTRACTCODE_AGENT_V1,
            label="RunnableFlow (v1)",
            description=(
                "Host-configurable prompt → response contract for running a workflow in chat-like clients (AbstractCode, AbstractObserver, etc)."
            ),
            required_start_outputs={
                "provider": "provider",
                "model": "model",
                "prompt": "string",
            },
            required_end_inputs={
                "response": "string",
                "success": "boolean",
                "meta": "object",
            },
            recommended_start_outputs={
                "use_context": "boolean",
                "memory": "memory",
                "context": "object",
                "system": "string",
                "tools": "tools",
                "max_iterations": "number",
                "max_in_tokens": "number",
                "temperature": "number",
                "seed": "number",
                "resp_schema": "object",
            },
            recommended_end_inputs={
                # Optional but commonly wired for host UX:
                "scratchpad": "object",
            },
        ),
    }


def validate_visual_flow_interface(flow: VisualFlow, interface_id: str) -> List[str]:
    """Validate that a VisualFlow implements a known interface contract.

    Returns a list of human-friendly error strings (empty when valid).
    """
    errors: List[str] = []
    iid = str(interface_id or "").strip()
    if not iid:
        return ["interface_id is required"]

    spec = get_interface_specs().get(iid)
    if spec is None:
        return [f"Unknown interface_id: {iid}"]

    declared = getattr(flow, "interfaces", None)
    declared_list = list(declared) if isinstance(declared, list) else []
    if iid not in declared_list:
        errors.append(f"Flow must declare interfaces: ['{iid}']")

    starts = [n for n in _iter_nodes(flow) if _node_type_str(n) == "on_flow_start"]
    if not starts:
        errors.append("Flow must include an On Flow Start node (type=on_flow_start).")
        return errors
    if len(starts) > 1:
        errors.append("Flow must include exactly one On Flow Start node (found multiple).")
        return errors

    ends = [n for n in _iter_nodes(flow) if _node_type_str(n) == "on_flow_end"]
    if not ends:
        errors.append("Flow must include at least one On Flow End node (type=on_flow_end).")
        return errors

    start = starts[0]
    start_data = getattr(start, "data", None)
    start_out = _pin_types(start_data.get("outputs") if isinstance(start_data, dict) else None)

    for pin_id, expected_type in dict(spec.required_start_outputs).items():
        if pin_id not in start_out:
            errors.append(f"On Flow Start must expose an output pin '{pin_id}' ({expected_type}).")
            continue
        actual = start_out.get(pin_id) or ""
        if expected_type and actual and actual != expected_type:
            errors.append(
                f"On Flow Start pin '{pin_id}' must be type '{expected_type}' (got '{actual}')."
            )

    # Validate all end nodes: whichever executes must satisfy the contract.
    for end in ends:
        end_data = getattr(end, "data", None)
        end_in = _pin_types(end_data.get("inputs") if isinstance(end_data, dict) else None)
        for pin_id, expected_type in dict(spec.required_end_inputs).items():
            if pin_id not in end_in:
                errors.append(
                    f"On Flow End node '{getattr(end, 'id', '')}' must expose an input pin '{pin_id}' ({expected_type})."
                )
                continue
            actual = end_in.get(pin_id) or ""
            if expected_type and actual and actual != expected_type:
                errors.append(
                    f"On Flow End node '{getattr(end, 'id', '')}' pin '{pin_id}' must be type '{expected_type}' (got '{actual}')."
                )

    return errors


def apply_visual_flow_interface_scaffold(
    flow: VisualFlow,
    interface_id: str,
    *,
    include_recommended: bool = True,
) -> bool:
    """Best-effort: apply a known interface's pin scaffolding to a VisualFlow.

    This is intended for authoring UX:
    - When a workflow is marked as implementing an interface, we ensure the
      required pins exist on the expected nodes (On Flow Start / On Flow End).
    - If those nodes are missing, we create them (unconnected) so the author
      has a correct starting point.

    Returns True if the flow was mutated.
    """
    iid = str(interface_id or "").strip()
    spec = get_interface_specs().get(iid)
    if spec is None:
        return False

    def _pin_dict(pin_id: str, type_str: str, *, label: Optional[str] = None) -> Dict[str, Any]:
        return {"id": pin_id, "label": label or pin_id, "type": type_str}

    def _ensure_pin(
        pins: list[Any],
        *,
        pin_id: str,
        type_str: str,
        label: Optional[str] = None,
    ) -> bool:
        for p in pins:
            if isinstance(p, dict) and p.get("id") == pin_id:
                # Ensure type matches the interface contract.
                if p.get("type") != type_str:
                    p["type"] = type_str
                    return True
                return False
        pins.append(_pin_dict(pin_id, type_str, label=label))
        return True

    def _ensure_exec_pin(pins: list[Any], *, pin_id: str, direction: str) -> bool:
        # We keep exec pins present because most authoring UX expects them, even though the
        # interface contract itself only speaks about data pins.
        if not isinstance(direction, str) or direction not in {"in", "out"}:
            direction = "out"
        changed = False
        for p in pins:
            if isinstance(p, dict) and p.get("id") == pin_id:
                if p.get("type") != "execution":
                    p["type"] = "execution"
                    changed = True
                # exec pins typically have empty label; keep existing label if present.
                return changed
        # Prepend exec pins for readability.
        pins.insert(0, {"id": pin_id, "label": "", "type": "execution"})
        return True

    def _reorder_pins(pins: list[Any], *, desired_ids: list[str]) -> bool:
        """Reorder pins in-place so interface pins appear in a stable, readable order."""
        if not isinstance(pins, list) or not desired_ids:
            return False
        ordered: list[Any] = []
        seen: set[str] = set()

        def _first_pin(pid: str) -> Any | None:
            for p in pins:
                if isinstance(p, dict) and p.get("id") == pid:
                    return p
            return None

        for pid in desired_ids:
            if pid in seen:
                continue
            p = _first_pin(pid)
            if p is None:
                continue
            ordered.append(p)
            seen.add(pid)

        for p in pins:
            pid = p.get("id") if isinstance(p, dict) else None
            if isinstance(pid, str) and pid in seen:
                continue
            ordered.append(p)

        if ordered == pins:
            return False
        pins[:] = ordered
        return True

    # Desired pins (required + optional recommended).
    start_pins = dict(spec.required_start_outputs)
    end_pins = dict(spec.required_end_inputs)
    if include_recommended:
        if isinstance(spec.recommended_start_outputs, Mapping):
            for k, v in dict(spec.recommended_start_outputs).items():
                start_pins.setdefault(str(k), str(v))
        if isinstance(spec.recommended_end_inputs, Mapping):
            for k, v in dict(spec.recommended_end_inputs).items():
                end_pins.setdefault(str(k), str(v))

    # Locate nodes.
    nodes = list(getattr(flow, "nodes", []) or [])
    used_ids = {str(getattr(n, "id", "") or "") for n in nodes}

    def _unique_node_id(base: str) -> str:
        b = str(base or "").strip() or "node"
        if b not in used_ids:
            used_ids.add(b)
            return b
        i = 2
        while True:
            cand = f"{b}-{i}"
            if cand not in used_ids:
                used_ids.add(cand)
                return cand
            i += 1

    def _ensure_nodes() -> Tuple[Any, List[Any], bool]:
        changed_local = False
        starts = [n for n in nodes if _node_type_str(n) == "on_flow_start"]
        ends = [n for n in nodes if _node_type_str(n) == "on_flow_end"]

        if not starts:
            try:
                from .models import NodeType, Position, VisualNode
            except Exception:
                # Should not happen in normal installs; bail out gracefully.
                return (None, ends, False)
            start_id = _unique_node_id("start")
            start = VisualNode(
                id=start_id,
                type=NodeType.ON_FLOW_START,
                position=Position(x=-420.0, y=120.0),
                data={
                    "nodeType": "on_flow_start",
                    "label": "On Flow Start",
                    "icon": "&#x1F3C1;",
                    "headerColor": "#C0392B",
                    "inputs": [],
                    "outputs": [{"id": "exec-out", "label": "", "type": "execution"}],
                },
            )
            nodes.insert(0, start)
            changed_local = True
            starts = [start]

        if not ends:
            try:
                from .models import NodeType, Position, VisualNode
            except Exception:
                return (starts[0], [], changed_local)
            end_id = _unique_node_id("end")
            end = VisualNode(
                id=end_id,
                type=NodeType.ON_FLOW_END,
                position=Position(x=260.0, y=120.0),
                data={
                    "nodeType": "on_flow_end",
                    "label": "On Flow End",
                    "icon": "&#x23F9;",
                    "headerColor": "#C0392B",
                    "inputs": [{"id": "exec-in", "label": "", "type": "execution"}],
                    "outputs": [],
                },
            )
            nodes.append(end)
            changed_local = True
            ends = [end]

        return (starts[0], ends, changed_local)

    start_node, end_nodes, changed = _ensure_nodes()
    if start_node is None:
        return False

    # Ensure pins on start.
    start_data = getattr(start_node, "data", None)
    if not isinstance(start_data, dict):
        start_data = {}
        setattr(start_node, "data", start_data)
        changed = True
    outputs = start_data.get("outputs")
    if not isinstance(outputs, list):
        outputs = []
        start_data["outputs"] = outputs
        changed = True
    changed = _ensure_exec_pin(outputs, pin_id="exec-out", direction="out") or changed
    for pid, t in start_pins.items():
        changed = _ensure_pin(outputs, pin_id=str(pid), type_str=str(t), label=str(pid)) or changed

    desired_start_order = [
        "exec-out",
        "use_context",
        "memory",
        "context",
        "provider",
        "model",
        "system",
        "prompt",
        "tools",
        "max_iterations",
        "max_in_tokens",
        "temperature",
        "seed",
        "resp_schema",
    ]
    changed = _reorder_pins(outputs, desired_ids=desired_start_order) or changed

    # Ensure pins on all end nodes.
    for end in end_nodes:
        end_data = getattr(end, "data", None)
        if not isinstance(end_data, dict):
            end_data = {}
            setattr(end, "data", end_data)
            changed = True
        inputs = end_data.get("inputs")
        if not isinstance(inputs, list):
            inputs = []
            end_data["inputs"] = inputs
            changed = True

        # Backward-compat cleanup: remove deprecated interface pins (`result` / `raw_result`)
        # when they are not part of the current desired contract.
        deprecated_end_pins = {"result", "raw_result"}
        if not any(pid in end_pins for pid in deprecated_end_pins):
            removed: set[str] = set()
            kept: list[Any] = []
            for p in inputs:
                pid = p.get("id") if isinstance(p, dict) else None
                if isinstance(pid, str) and pid in deprecated_end_pins:
                    removed.add(pid)
                    changed = True
                    continue
                kept.append(p)
            if removed:
                inputs[:] = kept
                # Remove edges that targeted the deprecated pins (best-effort).
                try:
                    flow_edges = getattr(flow, "edges", None)
                    if isinstance(flow_edges, list):
                        flow.edges = [
                            e
                            for e in flow_edges
                            if not (
                                getattr(e, "target", None) == getattr(end, "id", None)
                                and getattr(e, "targetHandle", None) in removed
                            )
                        ]
                except Exception:
                    pass

        changed = _ensure_exec_pin(inputs, pin_id="exec-in", direction="in") or changed
        for pid, t in end_pins.items():
            changed = _ensure_pin(inputs, pin_id=str(pid), type_str=str(t), label=str(pid)) or changed

        # Keep interface pins in a predictable order for UX.
        desired_end_order = ["exec-in", "response", "success", "meta", "scratchpad"]
        changed = _reorder_pins(inputs, desired_ids=desired_end_order) or changed

    # Write back nodes list if it was reconstructed.
    try:
        flow.nodes = nodes  # type: ignore[assignment]
    except Exception:
        pass

    # Ensure entryNode points at the start when missing/empty.
    try:
        entry = getattr(flow, "entryNode", None)
        if not isinstance(entry, str) or not entry.strip():
            flow.entryNode = str(getattr(start_node, "id", "") or "") or None
            changed = True
    except Exception:
        pass

    return bool(changed)

---
file: abstractflow/__init__.py
---

"""AbstractFlow - Multi-agent orchestration layer for the Abstract Framework.

AbstractFlow enables composition of agents into pipelines and coordinates
their execution via AbstractRuntime. It provides:

- Flow: Declarative flow definition with nodes and edges
- FlowRunner: High-level interface for running flows
- compile_flow: Convert Flow to WorkflowSpec for direct runtime usage

Example:
    >>> from abstractflow import Flow, FlowRunner
    >>>
    >>> # Define a simple flow
    >>> flow = Flow("my_pipeline")
    >>> flow.add_node("step1", lambda x: x * 2, input_key="value", output_key="doubled")
    >>> flow.add_node("step2", lambda x: x + 10, input_key="doubled", output_key="result")
    >>> flow.add_edge("step1", "step2")
    >>> flow.set_entry("step1")
    >>>
    >>> # Run the flow
    >>> runner = FlowRunner(flow)
    >>> result = runner.run({"value": 5})
    >>> print(result)  # {'result': 20, 'success': True}

For agent-based flows:
    >>> from abstractflow import Flow, FlowRunner
    >>> from abstractagent import create_react_agent
    >>>
    >>> planner = create_react_agent(provider="ollama", model="qwen3:4b")
    >>> executor = create_react_agent(provider="ollama", model="qwen3:4b")
    >>>
    >>> flow = Flow("plan_and_execute")
    >>> flow.add_node("plan", planner, output_key="plan")
    >>> flow.add_node("execute", executor, input_key="plan")
    >>> flow.add_edge("plan", "execute")
    >>> flow.set_entry("plan")
    >>>
    >>> runner = FlowRunner(flow)
    >>> result = runner.run({"context": {"task": "Build a REST API"}})
"""

__version__ = "0.3.7"
__author__ = "Laurent-Philippe Albou"
__email__ = "contact@abstractflow.ai"
__license__ = "MIT"

# Core classes
from .core.flow import Flow, FlowNode, FlowEdge

# Compiler
from .compiler import compile_flow

# Runner
from .runner import FlowRunner

# Adapters (for advanced usage)
from .adapters import (
    create_function_node_handler,
    create_agent_node_handler,
    create_subflow_node_handler,
)

__all__ = [
    # Version info
    "__version__",
    "__author__",
    "__email__",
    "__license__",
    # Core classes
    "Flow",
    "FlowNode",
    "FlowEdge",
    # Compiler
    "compile_flow",
    # Runner
    "FlowRunner",
    # Adapters
    "create_function_node_handler",
    "create_agent_node_handler",
    "create_subflow_node_handler",
]


def get_version() -> str:
    """Get the current version of AbstractFlow."""
    return __version__


def is_development_version() -> bool:
    """Check if this is a development version."""
    return False  # Now implemented!

---
file: abstractflow/runner.py
---

"""FlowRunner - executes flows using AbstractRuntime."""

from __future__ import annotations

from typing import Any, Dict, Optional, TYPE_CHECKING

from .core.flow import Flow
from .compiler import compile_flow

if TYPE_CHECKING:
    from abstractruntime.core.models import RunState
    from abstractruntime.core.runtime import Runtime
    from abstractruntime.core.spec import WorkflowSpec


class FlowRunner:
    """Executes flows using AbstractRuntime.

    FlowRunner provides a high-level interface for running flows. It handles:
    - Compiling the flow to a WorkflowSpec
    - Creating a default runtime if not provided
    - Managing run lifecycle (start, step, run, resume)

    Example:
        >>> flow = Flow("my_flow")
        >>> flow.add_node("start", lambda x: x * 2, input_key="value")
        >>> flow.set_entry("start")
        >>>
        >>> runner = FlowRunner(flow)
        >>> result = runner.run({"value": 21})
        >>> print(result)  # {'result': 42, 'success': True}
    """

    def __init__(
        self,
        flow: Flow,
        runtime: Optional["Runtime"] = None,
    ):
        """Initialize a FlowRunner.

        Args:
            flow: The Flow definition to run
            runtime: Optional AbstractRuntime instance. If not provided,
                     a default in-memory runtime will be created.
        """
        self.flow = flow
        self.workflow: "WorkflowSpec" = compile_flow(flow)
        self.runtime = runtime or self._create_default_runtime()
        self._current_run_id: Optional[str] = None

    def _create_default_runtime(self) -> "Runtime":
        """Create a default in-memory runtime."""
        try:
            from abstractruntime import Runtime, InMemoryRunStore, InMemoryLedgerStore  # type: ignore
        except Exception:  # pragma: no cover
            from abstractruntime.core.runtime import Runtime  # type: ignore
            from abstractruntime.storage.in_memory import InMemoryLedgerStore, InMemoryRunStore  # type: ignore

        return Runtime(
            run_store=InMemoryRunStore(),
            ledger_store=InMemoryLedgerStore(),
        )

    @property
    def run_id(self) -> Optional[str]:
        """Get the current run ID."""
        return self._current_run_id

    @staticmethod
    def _normalize_completed_output(raw: Any) -> Dict[str, Any]:
        """Normalize workflow completion output for host callers.

        Runtime-level workflows may complete with various output shapes:
        - VisualFlow On Flow End: {"my_output": ..., "success": True}
        - Terminal node returning scalar: {"response": 123, "success": True}
        - Legacy / explicit: {"result": ..., "success": True}

        AbstractFlow's public contract is: {"success": bool, "result": Any, ...}.
        """
        if not isinstance(raw, dict):
            return {"success": True, "result": raw}

        success = raw.get("success")
        if success is False:
            # Preserve error shape (tests + callers expect top-level "error"/"node", etc).
            return raw

        # Prefer explicit `result` when present (visual flows may also keep
        # top-level keys for UI/WS convenience).
        if "result" in raw:
            return {"success": True, "result": raw.get("result")}

        payload = {k: v for k, v in raw.items() if k != "success"}
        if len(payload) == 1:
            (only_key, only_val) = next(iter(payload.items()))
            if only_key in {"result", "response"}:
                return {"success": True, "result": only_val}

        return {"success": True, "result": payload}

    def start(
        self,
        input_data: Optional[Dict[str, Any]] = None,
        *,
        actor_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> str:
        """Start flow execution.

        Args:
            input_data: Initial variables for the flow

        Returns:
            The run ID for this execution
        """
        vars_dict = input_data or {}
        self._current_run_id = self.runtime.start(
            workflow=self.workflow,
            vars=vars_dict,
            actor_id=actor_id,
            session_id=session_id,
        )
        return self._current_run_id

    def step(self, max_steps: int = 1) -> "RunState":
        """Execute one or more steps.

        Args:
            max_steps: Maximum number of steps to execute

        Returns:
            The current RunState after stepping

        Raises:
            ValueError: If no run has been started
        """
        if not self._current_run_id:
            raise ValueError("No active run. Call start() first.")

        return self.runtime.tick(
            workflow=self.workflow,
            run_id=self._current_run_id,
            max_steps=max_steps,
        )

    def run(
        self,
        input_data: Optional[Dict[str, Any]] = None,
        *,
        actor_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Execute flow to completion.

        This method starts the flow and runs until it completes, fails,
        or enters a waiting state.

        Args:
            input_data: Initial variables for the flow

        Returns:
            The flow's output dictionary. If the flow is waiting,
            returns {"waiting": True, "state": <RunState>}.

        Raises:
            RuntimeError: If the flow fails
        """
        from abstractruntime.core.models import RunStatus, WaitReason

        self.start(input_data, actor_id=actor_id, session_id=session_id)

        while True:
            state = self.runtime.tick(
                workflow=self.workflow,
                run_id=self._current_run_id,
            )

            if state.status == RunStatus.COMPLETED:
                return self._normalize_completed_output(state.output)

            if state.status == RunStatus.FAILED:
                raise RuntimeError(f"Flow failed: {state.error}")

            if state.status == RunStatus.WAITING:
                # Convenience: when waiting on a SUBWORKFLOW, FlowRunner.run() can
                # auto-drive the child to completion and resume the parent.
                #
                # Visual Agent nodes use async+wait START_SUBWORKFLOW so web hosts
                # can stream traces. In non-interactive contexts (unit tests, CLI),
                # we still want a synchronous `run()` to complete when possible.
                wait = getattr(state, "waiting", None)
                if (
                    wait is not None
                    and getattr(wait, "reason", None) == WaitReason.SUBWORKFLOW
                    and getattr(self.runtime, "workflow_registry", None) is not None
                ):
                    registry = getattr(self.runtime, "workflow_registry", None)

                    def _extract_sub_run_id(wait_state: Any) -> Optional[str]:
                        details2 = getattr(wait_state, "details", None)
                        if isinstance(details2, dict):
                            rid2 = details2.get("sub_run_id")
                            if isinstance(rid2, str) and rid2:
                                return rid2
                        wk = getattr(wait_state, "wait_key", None)
                        if isinstance(wk, str) and wk.startswith("subworkflow:"):
                            return wk.split("subworkflow:", 1)[1] or None
                        return None

                    def _spec_for(run_state: Any):
                        wf_id = getattr(run_state, "workflow_id", None)
                        # FlowRunner always has the root workflow spec (self.workflow).
                        # The runtime registry is required only for *child* workflows.
                        #
                        # Without this fallback, synchronous `FlowRunner.run()` can hang on
                        # SUBWORKFLOW waits if callers register only subworkflows (common in
                        # unit tests where the parent spec is not registered).
                        if wf_id == getattr(self.workflow, "workflow_id", None):
                            return self.workflow
                        return registry.get(wf_id) if registry is not None else None

                    top_run_id = self._current_run_id  # type: ignore[assignment]
                    if isinstance(top_run_id, str) and top_run_id:
                        # Find the deepest run in a SUBWORKFLOW wait chain.
                        target_run_id = top_run_id
                        for _ in range(50):
                            cur_state = self.runtime.get_state(target_run_id)
                            if cur_state.status != RunStatus.WAITING or cur_state.waiting is None:
                                break
                            if cur_state.waiting.reason != WaitReason.SUBWORKFLOW:
                                break
                            next_id = _extract_sub_run_id(cur_state.waiting)
                            if not next_id:
                                break
                            target_run_id = next_id

                        # Drive runs bottom-up: tick the deepest runnable run, then bubble completion
                        # payloads to waiting parents until we either block on external input or
                        # the chain unwinds.
                        current_run_id = target_run_id
                        for _ in range(10_000):
                            cur_state = self.runtime.get_state(current_run_id)
                            if cur_state.status == RunStatus.RUNNING:
                                wf = _spec_for(cur_state)
                                if wf is None:
                                    break
                                cur_state = self.runtime.tick(workflow=wf, run_id=current_run_id)

                            if cur_state.status == RunStatus.WAITING:
                                # If this is a subworkflow wait, descend further.
                                if cur_state.waiting is not None and cur_state.waiting.reason == WaitReason.SUBWORKFLOW:
                                    next_id = _extract_sub_run_id(cur_state.waiting)
                                    if next_id:
                                        current_run_id = next_id
                                        continue
                                # Blocked on non-subworkflow input (ASK_USER / EVENT / UNTIL).
                                break

                            if cur_state.status == RunStatus.FAILED:
                                raise RuntimeError(f"Subworkflow failed: {cur_state.error}")
                            if cur_state.status == RunStatus.CANCELLED:
                                raise RuntimeError("Subworkflow cancelled")
                            if cur_state.status != RunStatus.COMPLETED:
                                break

                            parent_id = getattr(cur_state, "parent_run_id", None)
                            if not isinstance(parent_id, str) or not parent_id:
                                break

                            parent_state = self.runtime.get_state(parent_id)
                            if (
                                parent_state.status == RunStatus.WAITING
                                and parent_state.waiting is not None
                                and parent_state.waiting.reason == WaitReason.SUBWORKFLOW
                            ):
                                parent_wf = _spec_for(parent_state)
                                if parent_wf is None:
                                    break

                                node_traces = None
                                try:
                                    node_traces = self.runtime.get_node_traces(cur_state.run_id)
                                except Exception:
                                    node_traces = None

                                self.runtime.resume(
                                    workflow=parent_wf,
                                    run_id=parent_id,
                                    wait_key=None,
                                    payload={
                                        "sub_run_id": cur_state.run_id,
                                        "output": cur_state.output,
                                        "node_traces": node_traces,
                                    },
                                    max_steps=0,
                                )
                                current_run_id = parent_id
                                # Continue bubbling (and ticking resumed parents) until we unwind.
                                continue

                            break

                        # After driving/bubbling, re-enter the main loop and tick the top run again.
                        continue

                # Flow is waiting for external input
                return {
                    "waiting": True,
                    "state": state,
                    "wait_key": state.waiting.wait_key if state.waiting else None,
                }

    def resume(
        self,
        wait_key: Optional[str] = None,
        payload: Optional[Dict[str, Any]] = None,
        *,
        max_steps: int = 100,
    ) -> "RunState":
        """Resume a waiting flow.

        Args:
            wait_key: The wait key to resume (optional, uses current if not specified)
            payload: Data to provide to the waiting node

        Returns:
            The RunState after resuming
        """
        if not self._current_run_id:
            raise ValueError("No active run to resume.")

        state = self.runtime.resume(
            workflow=self.workflow,
            run_id=self._current_run_id,
            wait_key=wait_key,
            payload=payload or {},
            max_steps=max_steps,
        )
        try:
            from abstractruntime.core.models import RunStatus

            if getattr(state, "status", None) == RunStatus.COMPLETED:
                state.output = self._normalize_completed_output(getattr(state, "output", None))  # type: ignore[attr-defined]
        except Exception:
            pass
        return state

    def get_state(self) -> Optional["RunState"]:
        """Get the current run state.

        Returns:
            The current RunState, or None if no run is active
        """
        if not self._current_run_id:
            return None
        return self.runtime.get_state(self._current_run_id)

    def get_ledger(self) -> list:
        """Get the execution ledger for the current run.

        Returns:
            List of step records, or empty list if no run
        """
        if not self._current_run_id:
            return []
        return self.runtime.get_ledger(self._current_run_id)

    def is_running(self) -> bool:
        """Check if the flow is currently running."""
        from abstractruntime.core.models import RunStatus

        state = self.get_state()
        return state is not None and state.status == RunStatus.RUNNING

    def is_waiting(self) -> bool:
        """Check if the flow is waiting for input."""
        from abstractruntime.core.models import RunStatus

        state = self.get_state()
        return state is not None and state.status == RunStatus.WAITING

    def is_complete(self) -> bool:
        """Check if the flow has completed."""
        from abstractruntime.core.models import RunStatus

        state = self.get_state()
        return state is not None and state.status == RunStatus.COMPLETED

    def is_failed(self) -> bool:
        """Check if the flow has failed."""
        from abstractruntime.core.models import RunStatus

        state = self.get_state()
        return state is not None and state.status == RunStatus.FAILED

    def __repr__(self) -> str:
        status = "not started"
        if self._current_run_id:
            state = self.get_state()
            if state:
                status = state.status.value
        return f"FlowRunner(flow={self.flow.flow_id!r}, status={status!r})"

---
file: abstractflow/visual/executor.py
---

"""Portable visual-flow execution utilities.

This module wires the VisualFlow authoring DSL (JSON) to AbstractRuntime for
durable execution. Compilation semantics (VisualFlow → Flow → WorkflowSpec) are
delegated to `abstractruntime.visualflow_compiler` so there is a single
semantics engine across the framework.
"""

from __future__ import annotations

import os
import hashlib
import threading
from typing import Any, Dict, Optional, cast

from ..core.flow import Flow
from ..runner import FlowRunner

from .agent_ids import visual_react_workflow_id
from .models import VisualFlow


_MEMORY_KG_STORE_CACHE_LOCK = threading.Lock()
# Keyed by (store_base_dir, gateway_url, token_fingerprint).
#
# Why include the token fingerprint:
# - The embedder captures the auth token at construction time.
# - The UI can set/update the token at runtime (without restarting the backend).
# - If we didn't key by token, we'd keep using a cached store with a stale token and get 401s.
_MEMORY_KG_STORE_CACHE: dict[tuple[str, str, str], Any] = {}

def _resolve_gateway_auth_token() -> str | None:
    """Resolve the gateway auth token for host-to-gateway calls.

    Canonical env vars:
    - ABSTRACTGATEWAY_AUTH_TOKEN
    - ABSTRACTFLOW_GATEWAY_AUTH_TOKEN (legacy compatibility)

    Additional host fallbacks:
    - ABSTRACTCODE_GATEWAY_TOKEN (AbstractCode CLI convention)
    - ABSTRACTGATEWAY_AUTH_TOKENS / ABSTRACTFLOW_GATEWAY_AUTH_TOKENS (first token)
    """
    candidates = [
        "ABSTRACTGATEWAY_AUTH_TOKEN",
        "ABSTRACTFLOW_GATEWAY_AUTH_TOKEN",
        "ABSTRACTCODE_GATEWAY_TOKEN",
    ]
    for name in candidates:
        raw = os.getenv(name)
        token = str(raw or "").strip()
        if token:
            return token

    token_lists = [
        "ABSTRACTGATEWAY_AUTH_TOKENS",
        "ABSTRACTFLOW_GATEWAY_AUTH_TOKENS",
    ]
    for name in token_lists:
        raw = os.getenv(name)
        if not isinstance(raw, str) or not raw.strip():
            continue
        first = raw.split(",", 1)[0].strip()
        if first:
            return first

    return None


def create_visual_runner(
    visual_flow: VisualFlow,
    *,
    flows: Dict[str, VisualFlow],
    run_store: Optional[Any] = None,
    ledger_store: Optional[Any] = None,
    artifact_store: Optional[Any] = None,
    tool_executor: Optional[Any] = None,
    input_data: Optional[Dict[str, Any]] = None,
) -> FlowRunner:
    """Create a FlowRunner for a visual run with a correctly wired runtime.

    Responsibilities:
    - Build a WorkflowRegistry containing the root flow and any referenced subflows.
    - Create a runtime with an ArtifactStore (required for MEMORY_* effects).
    - If any LLM_CALL / Agent nodes exist in the flow tree, wire AbstractCore-backed
      effect handlers (via AbstractRuntime's integration module).

    Notes:
    - When LLM nodes rely on *connected* provider/model pins (e.g. from ON_FLOW_START),
      this runner still needs a default provider/model to initialize runtime capabilities.
      We use `input_data["provider"]`/`input_data["model"]` when provided, otherwise fall
      back to static pin defaults (best-effort).
    """
    # Be resilient to different AbstractRuntime install layouts: not all exports
    # are guaranteed to be re-exported from `abstractruntime.__init__`.
    try:
        from abstractruntime import Runtime  # type: ignore
    except Exception:  # pragma: no cover
        from abstractruntime.core.runtime import Runtime  # type: ignore

    try:
        from abstractruntime import InMemoryRunStore, InMemoryLedgerStore  # type: ignore
    except Exception:  # pragma: no cover
        from abstractruntime.storage.in_memory import InMemoryRunStore, InMemoryLedgerStore  # type: ignore

    # Workflow registry is used for START_SUBWORKFLOW composition (subflows + Agent nodes).
    #
    # This project supports different AbstractRuntime distributions; some older installs
    # may not expose WorkflowRegistry. In that case, fall back to a tiny in-process
    # dict-based registry with the same `.register()` + `.get()` surface.
    try:
        from abstractruntime import WorkflowRegistry  # type: ignore
    except Exception:  # pragma: no cover
        try:
            from abstractruntime.scheduler.registry import WorkflowRegistry  # type: ignore
        except Exception:  # pragma: no cover
            from abstractruntime.core.spec import WorkflowSpec  # type: ignore

            class WorkflowRegistry(dict):  # type: ignore[no-redef]
                def register(self, workflow: "WorkflowSpec") -> None:
                    self[str(workflow.workflow_id)] = workflow

    from ..compiler import compile_flow
    from .event_ids import visual_event_listener_workflow_id
    from .session_runner import VisualSessionRunner

    def _node_type(node: Any) -> str:
        t = getattr(node, "type", None)
        return t.value if hasattr(t, "value") else str(t)

    def _reachable_exec_node_ids(vf: VisualFlow) -> set[str]:
        """Return execution-reachable node ids (within this VisualFlow only).

        We consider only the *execution graph* (exec edges: targetHandle=exec-in).
        Disconnected/isolated execution nodes are ignored (Blueprint-style).
        """
        EXEC_TYPES: set[str] = {
            # Triggers / core exec
            "on_flow_start",
            "on_user_request",
            "on_agent_message",
            "on_schedule",
            "on_event",
            "on_flow_end",
            "agent",
            "function",
            "code",
            "subflow",
            # Workflow variables (execution setter)
            "set_var",
            "set_vars",
            "set_var_property",
            # Control exec
            "if",
            "switch",
            "loop",
            "while",
            "for",
            "sequence",
            "parallel",
            # Effects
            "ask_user",
            "answer_user",
            "llm_call",
            "tool_calls",
            "wait_until",
            "wait_event",
            "emit_event",
            "read_file",
            "write_file",
            "memory_note",
            "memory_query",
            "memory_tag",
            "memory_compact",
            "memory_rehydrate",
            "memory_kg_assert",
            "memory_kg_query",
            "memact_compose",
        }

        node_types: Dict[str, str] = {n.id: _node_type(n) for n in vf.nodes}
        exec_ids = {nid for nid, t in node_types.items() if t in EXEC_TYPES}
        if not exec_ids:
            return set()

        incoming_exec = {e.target for e in vf.edges if getattr(e, "targetHandle", None) == "exec-in"}

        roots: list[str] = []
        if isinstance(vf.entryNode, str) and vf.entryNode in exec_ids:
            roots.append(vf.entryNode)
        # Custom events are independent entrypoints; include them as roots for "executable" reachability.
        for n in vf.nodes:
            if n.id in exec_ids and node_types.get(n.id) == "on_event":
                roots.append(n.id)

        if not roots:
            # Fallback: infer a single root as "exec node with no incoming edge".
            for n in vf.nodes:
                if n.id in exec_ids and n.id not in incoming_exec:
                    roots.append(n.id)
                    break
        if not roots:
            roots.append(next(iter(exec_ids)))

        adj: Dict[str, list[str]] = {}
        for e in vf.edges:
            if getattr(e, "targetHandle", None) != "exec-in":
                continue
            if e.source not in exec_ids or e.target not in exec_ids:
                continue
            adj.setdefault(e.source, []).append(e.target)

        reachable: set[str] = set()
        stack2 = list(dict.fromkeys([r for r in roots if isinstance(r, str) and r]))
        while stack2:
            cur = stack2.pop()
            if cur in reachable:
                continue
            reachable.add(cur)
            for nxt in adj.get(cur, []):
                if nxt not in reachable:
                    stack2.append(nxt)
        return reachable

    # Collect all reachable flows (root + transitive subflows).
    #
    # Important: subflows are executed via runtime `START_SUBWORKFLOW` by workflow id.
    # This means subflow cycles (including self-recursion) are valid and should not be
    # rejected at runner-wiring time; we only need to register each workflow id once.
    ordered: list[VisualFlow] = []
    visited: set[str] = set()

    def _dfs(vf: VisualFlow) -> None:
        if vf.id in visited:
            return
        visited.add(vf.id)
        ordered.append(vf)

        reachable = _reachable_exec_node_ids(vf)
        for n in vf.nodes:
            node_type = _node_type(n)
            if node_type != "subflow":
                continue
            if reachable and n.id not in reachable:
                continue
            subflow_id = n.data.get("subflowId") or n.data.get("flowId")  # legacy
            if not isinstance(subflow_id, str) or not subflow_id.strip():
                raise ValueError(f"Subflow node '{n.id}' missing subflowId")
            subflow_id = subflow_id.strip()
            child = flows.get(subflow_id)
            # Self-recursion should work even if `flows` does not redundantly include this vf.
            if child is None and subflow_id == vf.id:
                child = vf
            if child is None:
                raise ValueError(f"Referenced subflow '{subflow_id}' not found")
            _dfs(child)

    _dfs(visual_flow)

    # Detect optional runtime features needed by this flow tree.
    # These flags keep `create_visual_runner()` resilient to older AbstractRuntime installs.
    needs_registry = False
    needs_artifacts = False
    needs_memory_kg = False
    for vf in ordered:
        reachable = _reachable_exec_node_ids(vf)
        for n in vf.nodes:
            if reachable and n.id not in reachable:
                continue
            t = _node_type(n)
            if t in {"subflow", "agent"}:
                needs_registry = True
            if t in {"on_event", "emit_event"}:
                needs_registry = True
            if t in {"memory_note", "memory_query", "memory_rehydrate", "memory_compact"}:
                needs_artifacts = True
            if t in {"memory_kg_assert", "memory_kg_query"}:
                needs_memory_kg = True

    # Detect whether this flow tree needs AbstractCore LLM integration.
    # Provider/model can be supplied either via node config *or* via connected input pins.
    has_llm_nodes = False
    llm_configs: set[tuple[str, str]] = set()
    default_llm: tuple[str, str] | None = None
    provider_hints: list[str] = []

    def _pin_connected(vf: VisualFlow, *, node_id: str, pin_id: str) -> bool:
        for e in vf.edges:
            try:
                if e.target == node_id and e.targetHandle == pin_id:
                    return True
            except Exception:
                continue
        return False

    def _infer_connected_pin_default(vf: VisualFlow, *, node_id: str, pin_id: str) -> Optional[str]:
        """Best-effort static inference for a connected pin's default value.

        This is used only to pick a reasonable *default* provider/model for the runtime
        (capabilities, limits, etc). Per-node/provider routing still happens at execution
        time via effect payloads.
        """
        try:
            for e in vf.edges:
                if e.target != node_id or e.targetHandle != pin_id:
                    continue
                source_id = getattr(e, "source", None)
                if not isinstance(source_id, str) or not source_id:
                    continue
                source_handle = getattr(e, "sourceHandle", None)
                if not isinstance(source_handle, str) or not source_handle:
                    source_handle = pin_id

                src = next((n for n in vf.nodes if getattr(n, "id", None) == source_id), None)
                if src is None:
                    return None
                data = getattr(src, "data", None)
                if not isinstance(data, dict):
                    return None

                pin_defaults = data.get("pinDefaults")
                if isinstance(pin_defaults, dict) and source_handle in pin_defaults:
                    v = pin_defaults.get(source_handle)
                    if isinstance(v, str) and v.strip():
                        return v.strip()

                literal_value = data.get("literalValue")
                if isinstance(literal_value, str) and literal_value.strip():
                    return literal_value.strip()
                if isinstance(literal_value, dict):
                    dv = literal_value.get("default")
                    if isinstance(dv, str) and dv.strip():
                        return dv.strip()
                    vv = literal_value.get(source_handle)
                    if isinstance(vv, str) and vv.strip():
                        return vv.strip()
                return None
        except Exception:
            return None

        return None

    def _add_pair(provider_raw: Any, model_raw: Any) -> None:
        nonlocal default_llm
        if not isinstance(provider_raw, str) or not provider_raw.strip():
            return
        if not isinstance(model_raw, str) or not model_raw.strip():
            return
        pair = (provider_raw.strip().lower(), model_raw.strip())
        llm_configs.add(pair)
        if default_llm is None:
            default_llm = pair

    # Prefer run inputs for the runtime default provider/model when available.
    # This avoids expensive provider probing and makes model capability detection match
    # what the user selected in the Run Flow modal.
    if isinstance(input_data, dict):
        _add_pair(input_data.get("provider"), input_data.get("model"))

    for vf in ordered:
        reachable = _reachable_exec_node_ids(vf)
        for n in vf.nodes:
            node_type = _node_type(n)
            if reachable and n.id not in reachable:
                continue
            if node_type in {"llm_call", "agent", "tool_calls", "memory_compact"}:
                has_llm_nodes = True

            if node_type == "llm_call":
                cfg = n.data.get("effectConfig", {}) if isinstance(n.data, dict) else {}
                cfg = cfg if isinstance(cfg, dict) else {}
                provider = cfg.get("provider")
                model = cfg.get("model")

                provider_ok = isinstance(provider, str) and provider.strip()
                model_ok = isinstance(model, str) and model.strip()
                provider_connected = _pin_connected(vf, node_id=n.id, pin_id="provider")
                model_connected = _pin_connected(vf, node_id=n.id, pin_id="model")

                if not provider_ok and not provider_connected:
                    raise ValueError(
                        f"LLM_CALL node '{n.id}' in flow '{vf.id}' missing provider "
                        "(set effectConfig.provider or connect the provider input pin)"
                    )
                if not model_ok and not model_connected:
                    raise ValueError(
                        f"LLM_CALL node '{n.id}' in flow '{vf.id}' missing model "
                        "(set effectConfig.model or connect the model input pin)"
                    )
                provider_default = (
                    provider
                    if provider_ok
                    else _infer_connected_pin_default(vf, node_id=n.id, pin_id="provider")
                    if provider_connected
                    else None
                )
                model_default = (
                    model
                    if model_ok
                    else _infer_connected_pin_default(vf, node_id=n.id, pin_id="model")
                    if model_connected
                    else None
                )
                _add_pair(provider_default, model_default)

            elif node_type == "memory_compact":
                cfg = n.data.get("effectConfig", {}) if isinstance(n.data, dict) else {}
                cfg = cfg if isinstance(cfg, dict) else {}
                provider = cfg.get("provider")
                model = cfg.get("model")

                provider_ok = isinstance(provider, str) and provider.strip()
                model_ok = isinstance(model, str) and model.strip()
                provider_connected = _pin_connected(vf, node_id=n.id, pin_id="provider")
                model_connected = _pin_connected(vf, node_id=n.id, pin_id="model")

                provider_default = (
                    provider
                    if provider_ok
                    else _infer_connected_pin_default(vf, node_id=n.id, pin_id="provider")
                    if provider_connected
                    else None
                )
                model_default = (
                    model
                    if model_ok
                    else _infer_connected_pin_default(vf, node_id=n.id, pin_id="model")
                    if model_connected
                    else None
                )
                _add_pair(provider_default, model_default)

            elif node_type == "agent":
                cfg = n.data.get("agentConfig", {}) if isinstance(n.data, dict) else {}
                cfg = cfg if isinstance(cfg, dict) else {}
                provider = cfg.get("provider")
                model = cfg.get("model")

                provider_ok = isinstance(provider, str) and provider.strip()
                model_ok = isinstance(model, str) and model.strip()
                provider_connected = _pin_connected(vf, node_id=n.id, pin_id="provider")
                model_connected = _pin_connected(vf, node_id=n.id, pin_id="model")

                if not provider_ok and not provider_connected:
                    raise ValueError(
                        f"Agent node '{n.id}' in flow '{vf.id}' missing provider "
                        "(set agentConfig.provider or connect the provider input pin)"
                    )
                if not model_ok and not model_connected:
                    raise ValueError(
                        f"Agent node '{n.id}' in flow '{vf.id}' missing model "
                        "(set agentConfig.model or connect the model input pin)"
                    )
                provider_default = (
                    provider
                    if provider_ok
                    else _infer_connected_pin_default(vf, node_id=n.id, pin_id="provider")
                    if provider_connected
                    else None
                )
                model_default = (
                    model
                    if model_ok
                    else _infer_connected_pin_default(vf, node_id=n.id, pin_id="model")
                    if model_connected
                    else None
                )
                _add_pair(provider_default, model_default)

            elif node_type == "provider_models":
                cfg = n.data.get("providerModelsConfig", {}) if isinstance(n.data, dict) else {}
                cfg = cfg if isinstance(cfg, dict) else {}
                provider = cfg.get("provider")
                if isinstance(provider, str) and provider.strip():
                    provider_hints.append(provider.strip().lower())
                    allowed = cfg.get("allowedModels")
                    if not isinstance(allowed, list):
                        allowed = cfg.get("allowed_models")
                    if isinstance(allowed, list):
                        for m in allowed:
                            _add_pair(provider, m)

    extra_effect_handlers: Dict[Any, Any] = {}
    if needs_memory_kg:
        try:
            # Dev convenience (monorepo):
            #
            # When running from source (without installing each package), `import abstractmemory`
            # can resolve to the *project directory* (namespace package, no exports) instead of
            # the src-layout package at `abstractmemory/src/abstractmemory`.
            #
            # Add the src-layout path when it exists so VisualFlows with `memory_kg_*` nodes
            # work out-of-the-box in local dev environments.
            import sys
            from pathlib import Path

            repo_root = Path(__file__).resolve().parents[3]  # .../abstractframework
            mem_src = repo_root / "abstractmemory" / "src"
            if mem_src.is_dir():
                mem_src_str = str(mem_src)
                try:
                    sys.path.remove(mem_src_str)
                except ValueError:
                    pass
                sys.path.insert(0, mem_src_str)

            from abstractmemory import LanceDBTripleStore
            from abstractruntime.integrations.abstractmemory.effect_handlers import build_memory_kg_effect_handlers
            from abstractruntime.storage.artifacts import utc_now_iso
        except Exception as e:
            raise RuntimeError(
                "This flow uses memory_kg_* nodes, but AbstractMemory integration is not available. "
                "Install `abstractmemory` (and optionally `abstractmemory[lancedb]`)."
            ) from e

        # Ensure stores exist so KG handlers can resolve run-tree scope fallbacks.
        if run_store is None:
            run_store = InMemoryRunStore()
        if ledger_store is None:
            ledger_store = InMemoryLedgerStore()

        base_dir = None
        mem_dir_raw = os.getenv("ABSTRACTMEMORY_DIR") or os.getenv("ABSTRACTFLOW_MEMORY_DIR")
        if isinstance(mem_dir_raw, str) and mem_dir_raw.strip():
            try:
                base_dir = Path(mem_dir_raw).expanduser().resolve()
            except Exception:
                base_dir = None
        if base_dir is None and artifact_store is not None:
            base_attr = getattr(artifact_store, "_base", None)
            if base_attr is not None:
                try:
                    base_dir = Path(base_attr).expanduser().resolve() / "abstractmemory"
                except Exception:
                    base_dir = None

        # Embeddings are a gateway/runtime capability (singleton embedding space per gateway instance).
        try:
            from abstractmemory.embeddings import AbstractGatewayTextEmbedder
        except Exception as e:
            raise RuntimeError(
                "This flow uses memory_kg_* nodes, but AbstractMemory gateway embeddings integration is not available. "
                "Install `abstractmemory` (src layout) and ensure it is importable."
            ) from e

        gateway_url = str(os.getenv("ABSTRACTFLOW_GATEWAY_URL") or os.getenv("ABSTRACTGATEWAY_URL") or "").strip()
        if not gateway_url:
            gateway_url = "http://127.0.0.1:8081"
        auth_token = _resolve_gateway_auth_token()
        # Deterministic/offline mode:
        # - When embeddings are explicitly disabled, allow LanceDB to operate in pattern-only mode.
        # - Vector search (query_text) will raise in the store when no embedder is configured.
        embedder = None
        embed_provider = (
            os.getenv("ABSTRACTFLOW_EMBEDDING_PROVIDER")
            or os.getenv("ABSTRACTMEMORY_EMBEDDING_PROVIDER")
            or os.getenv("ABSTRACTGATEWAY_EMBEDDING_PROVIDER")
        )
        if str(embed_provider or "").strip().lower() not in {"__disabled__", "disabled", "none", "off"}:
            embedder = AbstractGatewayTextEmbedder(base_url=gateway_url, auth_token=auth_token)

        if base_dir is None:
            raise RuntimeError(
                "This flow uses memory_kg_* nodes, but no durable memory directory could be resolved. "
                "Set `ABSTRACTFLOW_MEMORY_DIR` (or `ABSTRACTMEMORY_DIR`), or run with a file-backed ArtifactStore."
            )

        base_dir.mkdir(parents=True, exist_ok=True)
        token_fingerprint = "embeddings_disabled"
        if embedder is not None:
            if auth_token:
                token_fingerprint = hashlib.sha256(auth_token.encode("utf-8")).hexdigest()[:12]
            else:
                token_fingerprint = "missing_token"
        cache_key = (str(base_dir), gateway_url if embedder is not None else "__embeddings_disabled__", token_fingerprint)
        with _MEMORY_KG_STORE_CACHE_LOCK:
            store_obj = _MEMORY_KG_STORE_CACHE.get(cache_key)
        if store_obj is None:
            try:
                store_obj = LanceDBTripleStore(base_dir / "kg", embedder=embedder)
            except Exception as e:
                raise RuntimeError(
                    "This flow uses memory_kg_* nodes, which require a LanceDB-backed store. "
                    "Install `lancedb` and ensure the host runs under the same environment."
                ) from e
            with _MEMORY_KG_STORE_CACHE_LOCK:
                _MEMORY_KG_STORE_CACHE[cache_key] = store_obj

        extra_effect_handlers = build_memory_kg_effect_handlers(store=store_obj, run_store=run_store, now_iso=utc_now_iso)

    if has_llm_nodes:
        provider_model = default_llm

        # Strict behavior: do not probe unrelated providers/models to "guess" a default.
        #
        # A VisualFlow run must provide a deterministic provider+model for the runtime:
        # - via run inputs (e.g. ON_FLOW_START pinDefaults / user-provided input_data), OR
        # - via static node configs (effectConfig/agentConfig), OR
        # - via connected pin defaults (best-effort).
        #
        # If we can't determine that, fail loudly with a clear error message.
        if provider_model is None:
            raise RuntimeError(
                "This flow uses LLM nodes (llm_call/agent/memory_compact), but no default provider/model could be determined. "
                "Set provider+model on a node, or connect provider/model pins to a node with pinDefaults "
                "(e.g. ON_FLOW_START), or pass `input_data={'provider': ..., 'model': ...}` when creating the runner."
            )

        provider, model = provider_model
        try:
            from abstractruntime.integrations.abstractcore.factory import create_local_runtime
            # Older/newer AbstractRuntime distributions expose tool executors differently.
            # Tool execution is not required for plain LLM_CALL-only flows, so we make
            # this optional and fall back to the factory defaults.
            try:
                from abstractruntime.integrations.abstractcore import MappingToolExecutor  # type: ignore
            except Exception:  # pragma: no cover
                try:
                    from abstractruntime.integrations.abstractcore.tool_executor import MappingToolExecutor  # type: ignore
                except Exception:  # pragma: no cover
                    MappingToolExecutor = None  # type: ignore[assignment]
            try:
                from abstractruntime.integrations.abstractcore.default_tools import get_default_tools  # type: ignore
            except Exception:  # pragma: no cover
                get_default_tools = None  # type: ignore[assignment]
        except Exception as e:  # pragma: no cover
            raise RuntimeError(
                "This flow uses LLM nodes (llm_call/agent), but the installed AbstractRuntime "
                "does not provide the AbstractCore integration. Install/enable the integration "
                "or remove LLM nodes from the flow."
            ) from e

        effective_tool_executor = tool_executor
        if effective_tool_executor is None and MappingToolExecutor is not None and callable(get_default_tools):
            try:
                tools = list(get_default_tools())  # type: ignore[misc]
                # Include a couple of safe web helpers that ship with `abstractcore[tools]` but
                # are not part of AbstractRuntime's default tool list yet.
                try:
                    from abstractcore.tools.common_tools import skim_url, skim_websearch

                    def _tool_name(func: Any) -> str:
                        tool_def = getattr(func, "_tool_definition", None)
                        if tool_def is not None:
                            name = getattr(tool_def, "name", None)
                            if isinstance(name, str) and name.strip():
                                return name.strip()
                        name = getattr(func, "__name__", "")
                        return str(name or "").strip()

                    seen = {_tool_name(t) for t in tools if callable(t)}
                    for t in [skim_url, skim_websearch]:
                        if not callable(t):
                            continue
                        name = _tool_name(t)
                        if not name or name in seen:
                            continue
                        seen.add(name)
                        tools.append(t)
                except Exception:
                    pass

                effective_tool_executor = MappingToolExecutor.from_tools(tools)  # type: ignore[attr-defined]
            except Exception:
                effective_tool_executor = None

        # LLM timeout policy (web-hosted workflow execution).
        #
        # Contract:
        # - AbstractRuntime (the orchestrator) is the authority for execution policy such as timeouts.
        # - This host can *override* that policy via env for deployments that want a different SLO.
        #
        # Env overrides:
        # - ABSTRACTFLOW_LLM_TIMEOUT_S (float seconds)
        # - ABSTRACTFLOW_LLM_TIMEOUT (alias)
        #
        # Set to 0 or a negative value to opt into "unlimited".
        llm_kwargs: Dict[str, Any] = {}
        timeout_raw = os.getenv("ABSTRACTFLOW_LLM_TIMEOUT_S") or os.getenv("ABSTRACTFLOW_LLM_TIMEOUT")
        if timeout_raw is None or not str(timeout_raw).strip():
            # No override: let the orchestrator (AbstractRuntime) apply its default.
            pass
        else:
            raw = str(timeout_raw).strip().lower()
            if raw in {"none", "null", "inf", "infinite", "unlimited"}:
                # Explicit override: opt back into unlimited HTTP requests.
                llm_kwargs["timeout"] = None
            else:
                try:
                    timeout_s = float(raw)
                except Exception:
                    timeout_s = None
                # Only override when parsing succeeded; otherwise fall back to AbstractCore config default.
                if timeout_s is None:
                    pass
                elif isinstance(timeout_s, (int, float)) and timeout_s <= 0:
                    # Consistent with the documented behavior: <=0 => unlimited.
                    llm_kwargs["timeout"] = None
                else:
                    llm_kwargs["timeout"] = timeout_s

        # Output token budget for web-hosted runs.
        #
        # Contract: do not impose an arbitrary default cap here. When unset, the runtime/provider
        # uses the model's declared capabilities (`model_capabilities.json`) for its defaults.
        #
        # Operators can still override via env (including disabling by setting <=0 / "unlimited").
        max_out_raw = os.getenv("ABSTRACTFLOW_LLM_MAX_OUTPUT_TOKENS") or os.getenv("ABSTRACTFLOW_MAX_OUTPUT_TOKENS")
        max_out: Optional[int] = None
        if max_out_raw is None or not str(max_out_raw).strip():
            max_out = None
        else:
            try:
                max_out = int(str(max_out_raw).strip())
            except Exception:
                max_out = None
        if isinstance(max_out, int) and max_out <= 0:
            max_out = None

        # Pass runtime config to initialize `_limits.max_output_tokens`.
        try:
            from abstractruntime.core.config import RuntimeConfig
            runtime_config = RuntimeConfig(max_output_tokens=max_out)
        except Exception:  # pragma: no cover
            runtime_config = None

        runtime = create_local_runtime(
            provider=provider,
            model=model,
            llm_kwargs=llm_kwargs,
            tool_executor=effective_tool_executor,
            run_store=run_store,
            ledger_store=ledger_store,
            artifact_store=artifact_store,
            config=runtime_config,
            extra_effect_handlers=extra_effect_handlers,
        )
    else:
        runtime_kwargs: Dict[str, Any] = {
            "run_store": run_store or InMemoryRunStore(),
            "ledger_store": ledger_store or InMemoryLedgerStore(),
        }
        if extra_effect_handlers:
            runtime_kwargs["effect_handlers"] = extra_effect_handlers

        if needs_artifacts:
            # MEMORY_* effects require an ArtifactStore. Only configure it when needed.
            artifact_store_obj: Any = artifact_store
            if artifact_store_obj is None:
                try:
                    from abstractruntime import InMemoryArtifactStore  # type: ignore
                    artifact_store_obj = InMemoryArtifactStore()
                except Exception:  # pragma: no cover
                    try:
                        from abstractruntime.storage.artifacts import InMemoryArtifactStore  # type: ignore
                        artifact_store_obj = InMemoryArtifactStore()
                    except Exception as e:  # pragma: no cover
                        raise RuntimeError(
                            "This flow uses MEMORY_* nodes, but the installed AbstractRuntime "
                            "does not provide an ArtifactStore implementation."
                        ) from e

            # Only pass artifact_store if the runtime supports it (older runtimes may not).
            try:
                from inspect import signature

                if "artifact_store" in signature(Runtime).parameters:
                    runtime_kwargs["artifact_store"] = artifact_store_obj
            except Exception:  # pragma: no cover
                # Best-effort: attempt to set via method if present.
                pass

        runtime = Runtime(**runtime_kwargs)

        # Best-effort: configure artifact store via setter if supported.
        if needs_artifacts and "artifact_store" not in runtime_kwargs and hasattr(runtime, "set_artifact_store"):
            try:
                runtime.set_artifact_store(artifact_store_obj)  # type: ignore[name-defined]
            except Exception:
                pass

    flow = visual_to_flow(visual_flow)
    # Build and register custom event listener workflows (On Event nodes).
    event_listener_specs: list[Any] = []
    if needs_registry:
        try:
            from .agent_ids import visual_react_workflow_id
        except Exception:  # pragma: no cover
            visual_react_workflow_id = None  # type: ignore[assignment]

        for vf in ordered:
            reachable = _reachable_exec_node_ids(vf)
            for n in vf.nodes:
                if _node_type(n) != "on_event":
                    continue
                # On Event nodes are roots by definition (even if disconnected from the main entry).
                if reachable and n.id not in reachable:
                    continue

                workflow_id = visual_event_listener_workflow_id(flow_id=vf.id, node_id=n.id)

                # Create a derived VisualFlow for this listener workflow:
                # - workflow id is unique (so it can be registered)
                # - entryNode is the on_event node
                derived = vf.model_copy(deep=True)
                derived.id = workflow_id
                derived.entryNode = n.id

                # Ensure Agent nodes inside this derived workflow reference the canonical
                # ReAct workflow IDs based on the *source* flow id, not the derived id.
                if callable(visual_react_workflow_id):
                    for dn in derived.nodes:
                        if _node_type(dn) != "agent":
                            continue
                        raw_cfg = dn.data.get("agentConfig", {}) if isinstance(dn.data, dict) else {}
                        cfg = dict(raw_cfg) if isinstance(raw_cfg, dict) else {}
                        cfg.setdefault(
                            "_react_workflow_id",
                            visual_react_workflow_id(flow_id=vf.id, node_id=dn.id),
                        )
                        dn.data["agentConfig"] = cfg

                listener_flow = visual_to_flow(derived)
                listener_spec = compile_flow(listener_flow)
                event_listener_specs.append(listener_spec)
    runner: FlowRunner
    if event_listener_specs:
        runner = VisualSessionRunner(flow, runtime=runtime, event_listener_specs=event_listener_specs)
    else:
        runner = FlowRunner(flow, runtime=runtime)

    if needs_registry:
        registry = WorkflowRegistry()
        registry.register(runner.workflow)
        for vf in ordered[1:]:
            child_flow = visual_to_flow(vf)
            child_spec = compile_flow(child_flow)
            registry.register(child_spec)
        for spec in event_listener_specs:
            registry.register(spec)

        # Register per-Agent-node subworkflows (canonical AbstractAgent ReAct).
        #
        # Visual Agent nodes compile into START_SUBWORKFLOW effects that reference a
        # deterministic workflow_id. The registry must contain those WorkflowSpecs.
        #
        # This keeps VisualFlow JSON portable across hosts: any host can run a
        # VisualFlow document by registering these derived specs alongside the flow.
        agent_nodes: list[tuple[str, Dict[str, Any]]] = []
        for vf in ordered:
            for n in vf.nodes:
                node_type = _node_type(n)
                if node_type != "agent":
                    continue
                cfg = n.data.get("agentConfig", {})
                agent_nodes.append((visual_react_workflow_id(flow_id=vf.id, node_id=n.id), cfg if isinstance(cfg, dict) else {}))

        if agent_nodes:
            try:
                from abstractagent.adapters.react_runtime import create_react_workflow
                from abstractagent.logic.react import ReActLogic
            except Exception as e:  # pragma: no cover
                raise RuntimeError(
                    "Visual Agent nodes require AbstractAgent to be installed/importable."
                ) from e

            from abstractcore.tools import ToolDefinition
            from abstractruntime.integrations.abstractcore.default_tools import list_default_tool_specs

            def _tool_defs_from_specs(specs: list[dict[str, Any]]) -> list[ToolDefinition]:
                out: list[ToolDefinition] = []
                for s in specs:
                    if not isinstance(s, dict):
                        continue
                    name = s.get("name")
                    if not isinstance(name, str) or not name.strip():
                        continue
                    desc = s.get("description")
                    params = s.get("parameters")
                    out.append(
                        ToolDefinition(
                            name=name.strip(),
                            description=str(desc or ""),
                            parameters=dict(params) if isinstance(params, dict) else {},
                        )
                    )
                return out

            def _normalize_tool_names(raw: Any) -> list[str]:
                if not isinstance(raw, list):
                    return []
                out: list[str] = []
                for t in raw:
                    if isinstance(t, str) and t.strip():
                        out.append(t.strip())
                return out

            all_tool_defs = _tool_defs_from_specs(list_default_tool_specs())
            # Add schema-only runtime tools (executed as runtime effects by AbstractAgent adapters).
            try:
                from abstractagent.logic.builtins import (  # type: ignore
                    ASK_USER_TOOL,
                    COMPACT_MEMORY_TOOL,
                    DELEGATE_AGENT_TOOL,
                    INSPECT_VARS_TOOL,
                    RECALL_MEMORY_TOOL,
                    REMEMBER_TOOL,
                )

                builtin_defs = [
                    ASK_USER_TOOL,
                    RECALL_MEMORY_TOOL,
                    INSPECT_VARS_TOOL,
                    REMEMBER_TOOL,
                    COMPACT_MEMORY_TOOL,
                    DELEGATE_AGENT_TOOL,
                ]
                seen_names = {t.name for t in all_tool_defs if getattr(t, "name", None)}
                for t in builtin_defs:
                    if getattr(t, "name", None) and t.name not in seen_names:
                        all_tool_defs.append(t)
                        seen_names.add(t.name)
            except Exception:
                pass

            for workflow_id, cfg in agent_nodes:
                provider_raw = cfg.get("provider")
                model_raw = cfg.get("model")
                # NOTE: Provider/model are injected durably through the Agent node's
                # START_SUBWORKFLOW vars (see compiler `_build_sub_vars`). We keep the
                # registered workflow spec provider/model-agnostic so Agent pins can
                # override without breaking persistence/resume.
                provider = None
                model = None

                tools_selected = _normalize_tool_names(cfg.get("tools"))
                logic = ReActLogic(tools=all_tool_defs)
                registry.register(
                    create_react_workflow(
                        logic=logic,
                        workflow_id=workflow_id,
                        provider=provider,
                        model=model,
                        allowed_tools=tools_selected,
                    )
                )

        if hasattr(runtime, "set_workflow_registry"):
            runtime.set_workflow_registry(registry)  # type: ignore[name-defined]
        else:  # pragma: no cover
            raise RuntimeError(
                "This flow requires subworkflows (agent/subflow nodes), but the installed "
                "AbstractRuntime does not support workflow registries."
            )

    return runner


def visual_to_flow(visual: VisualFlow) -> Flow:
    """Convert a VisualFlow definition to a runtime Flow IR."""
    from abstractruntime.visualflow_compiler import load_visualflow_json as _load_visualflow_json
    from abstractruntime.visualflow_compiler import visual_to_flow as _runtime_visual_to_flow

    vf = _load_visualflow_json(visual)
    return cast(Flow, _runtime_visual_to_flow(vf))


def execute_visual_flow(visual_flow: VisualFlow, input_data: Dict[str, Any], *, flows: Dict[str, VisualFlow]) -> Dict[str, Any]:
    """Execute a visual flow with a correctly wired runtime (LLM/MEMORY/SUBFLOW)."""
    runner = create_visual_runner(visual_flow, flows=flows)
    result = runner.run(input_data)

    if isinstance(result, dict) and result.get("waiting"):
        state = runner.get_state()
        wait = state.waiting if state else None
        return {
            "success": False,
            "waiting": True,
            "error": "Flow is waiting for input. Use a host resume mechanism to continue.",
            "run_id": runner.run_id,
            "wait_key": wait.wait_key if wait else None,
            "prompt": wait.prompt if wait else None,
            "choices": list(wait.choices) if wait and isinstance(wait.choices, list) else [],
            "allow_free_text": bool(wait.allow_free_text) if wait else None,
        }

    if isinstance(result, dict):
        return {
            "success": bool(result.get("success", True)),
            "waiting": False,
            "result": result.get("result"),
            "error": result.get("error"),
            "run_id": runner.run_id,
        }

    return {"success": True, "waiting": False, "result": result, "run_id": runner.run_id}

---
file: abstractflow/visual/session_runner.py
---

"""Session-aware runner for VisualFlow executions.

This runner extends FlowRunner with:
- A durable session_id (defaults to root run_id)
- Auto-started custom event listener workflows ("On Event" nodes)

This keeps VisualFlow JSON portable: any host can execute a visual flow and its
event listeners using AbstractRuntime's durable semantics.
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional, TYPE_CHECKING

from ..runner import FlowRunner

if TYPE_CHECKING:
    from abstractruntime.core.runtime import Runtime
    from abstractruntime.core.spec import WorkflowSpec


class VisualSessionRunner(FlowRunner):
    """FlowRunner that starts event listener workflows within the same session."""

    def __init__(
        self,
        flow: Any,
        *,
        runtime: Optional["Runtime"] = None,
        event_listener_specs: Optional[List["WorkflowSpec"]] = None,
    ) -> None:
        super().__init__(flow, runtime=runtime)
        self._event_listener_specs: List["WorkflowSpec"] = list(event_listener_specs or [])
        self._event_listener_run_ids: List[str] = []

    @property
    def event_listener_run_ids(self) -> List[str]:
        return list(self._event_listener_run_ids)

    def start(
        self,
        input_data: Optional[Dict[str, Any]] = None,
        *,
        actor_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> str:
        run_id = super().start(input_data, actor_id=actor_id, session_id=session_id)

        # Default session_id to the root run_id for session-scoped events.
        # If a session_id is explicitly provided, preserve it.
        effective_session_id = session_id
        try:
            state = self.runtime.get_state(run_id)
            if not getattr(state, "session_id", None):
                state.session_id = run_id  # type: ignore[attr-defined]
                self.runtime.run_store.save(state)
            effective_session_id = str(getattr(state, "session_id", None) or run_id).strip() or run_id
        except Exception:
            # Best-effort; session-scoped keys will fall back to run_id if missing.
            effective_session_id = str(session_id).strip() if isinstance(session_id, str) and session_id.strip() else run_id

        if not self._event_listener_specs:
            return run_id

        # Start listeners as child runs in the same session.
        for spec in self._event_listener_specs:
            try:
                child_run_id = self.runtime.start(
                    workflow=spec,
                    vars={},
                    session_id=effective_session_id,
                    actor_id=actor_id,
                    parent_run_id=run_id,
                )
                # Advance the listener to its first WAIT_EVENT (On Event node).
                self.runtime.tick(workflow=spec, run_id=child_run_id, max_steps=10)
                self._event_listener_run_ids.append(child_run_id)
            except Exception:
                # Listener start failures should surface during execution, but
                # we don't want to hide root runs from starting.
                continue

        return run_id

    def run(
        self,
        input_data: Optional[Dict[str, Any]] = None,
        *,
        actor_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Execute the root run and drive session-level listener runs.

        Rationale:
        - `EMIT_EVENT` resumes listener runs but does not (by default) execute them.
        - Hosts that only "run the main workflow" should still see event handler branches run.
        """
        from abstractruntime.core.models import RunStatus, WaitReason

        run_id = self.start(input_data, actor_id=actor_id, session_id=session_id)
        runtime = self.runtime

        def _list_session_runs() -> List[str]:
            out: List[str] = [run_id]
            try:
                rs = getattr(runtime, "run_store", None)
                if rs is not None and hasattr(rs, "list_children"):
                    children = rs.list_children(parent_run_id=run_id)  # type: ignore[attr-defined]
                    for c in children:
                        cid = getattr(c, "run_id", None)
                        if isinstance(cid, str) and cid and cid not in out:
                            out.append(cid)
            except Exception:
                pass
            for cid in self._event_listener_run_ids:
                if cid not in out:
                    out.append(cid)
            return out

        def _tick_child(child_run_id: str, *, max_steps: int = 100) -> None:
            st = runtime.get_state(child_run_id)
            if st.status != RunStatus.RUNNING:
                # WAIT_UNTIL can auto-unblock, so still call tick() to allow progress.
                if st.status == RunStatus.WAITING and st.waiting and st.waiting.reason == WaitReason.UNTIL:
                    pass
                else:
                    return
            reg = getattr(runtime, "workflow_registry", None)
            if reg is None:
                return
            wf = reg.get(st.workflow_id)
            if wf is None:
                return
            runtime.tick(workflow=wf, run_id=child_run_id, max_steps=max_steps)

        while True:
            state = runtime.tick(workflow=self.workflow, run_id=run_id, max_steps=100)

            # Drive children that became RUNNING due to EMIT_EVENT (or subflows).
            for cid in _list_session_runs():
                if cid == run_id:
                    continue
                _tick_child(cid, max_steps=100)

            # Root completion/termination conditions.
            if state.status == RunStatus.COMPLETED:
                # If all children are idle listeners (WAITING EVENT) or terminal, close the session.
                all_idle_or_done = True
                for cid in _list_session_runs():
                    if cid == run_id:
                        continue
                    st = runtime.get_state(cid)
                    if st.status in (RunStatus.COMPLETED, RunStatus.FAILED, RunStatus.CANCELLED):
                        continue
                    if st.status == RunStatus.WAITING and st.waiting and st.waiting.reason == WaitReason.EVENT:
                        continue
                    all_idle_or_done = False
                if all_idle_or_done:
                    # Cancel idle listeners to keep the session tidy.
                    for cid in _list_session_runs():
                        if cid == run_id:
                            continue
                        st = runtime.get_state(cid)
                        if st.status == RunStatus.WAITING and st.waiting and st.waiting.reason == WaitReason.EVENT:
                            try:
                                runtime.cancel_run(cid, reason="Session completed")
                            except Exception:
                                pass
                    return self._normalize_completed_output(state.output)
                # Otherwise, keep driving until children settle into waits/terminal.
                continue

            if state.status == RunStatus.FAILED:
                raise RuntimeError(f"Flow failed: {state.error}")

            if state.status == RunStatus.WAITING:
                # Preserve FlowRunner shape.
                return {
                    "waiting": True,
                    "state": state,
                    "wait_key": state.waiting.wait_key if state.waiting else None,
                }

---
file: abstractflow/visual/workspace_scoped_tools.py
---

"""Workspace-scoped tool execution helpers (AbstractFlow re-export).

The core implementation lives in AbstractRuntime's AbstractCore integration so it can
be shared across hosts/clients. AbstractFlow re-exports the types to preserve existing
imports (`abstractflow.visual.workspace_scoped_tools`).
"""

from __future__ import annotations

from typing import Any, Callable

from abstractruntime.integrations.abstractcore.workspace_scoped_tools import (  # noqa: F401
    WorkspaceScope,
    WorkspaceScopedToolExecutor,
    resolve_workspace_base_dir,
    resolve_user_path,
)

ToolCallable = Callable[..., Any]


def _tool_name(func: ToolCallable) -> str:
    tool_def = getattr(func, "_tool_definition", None)
    if tool_def is not None:
        name = getattr(tool_def, "name", None)
        if isinstance(name, str) and name.strip():
            return name.strip()
    name = getattr(func, "__name__", "")
    return str(name or "").strip()


def _extend_default_tools(tools: list[ToolCallable]) -> list[ToolCallable]:
    """Extend AbstractRuntime defaults with additional AbstractCore tools.

    Rationale:
    - The editor UX expects the "common tools" set to be available.
    - AbstractRuntime intentionally keeps its default list small; this host opts into a couple
      of additional safe web helpers that are already shipped in `abstractcore[tools]`.
    """
    out = list(tools or [])
    seen = {_tool_name(t) for t in out if callable(t)}

    try:
        from abstractcore.tools.common_tools import skim_url, skim_websearch
    except Exception:
        return out

    for t in [skim_url, skim_websearch]:
        if not callable(t):
            continue
        name = _tool_name(t)
        if not name or name in seen:
            continue
        seen.add(name)
        out.append(t)

    return out


def build_scoped_tool_executor(*, scope: WorkspaceScope) -> Any:
    """Create a local tool executor wrapped with workspace scoping."""
    from abstractruntime.integrations.abstractcore.default_tools import get_default_tools
    from abstractruntime.integrations.abstractcore.tool_executor import MappingToolExecutor

    delegate = MappingToolExecutor.from_tools(_extend_default_tools(list(get_default_tools())))
    return WorkspaceScopedToolExecutor(scope=scope, delegate=delegate)

---
file: abstractflow/compiler.py
---

"""AbstractFlow compiler shim.

AbstractFlow no longer owns workflow compilation semantics. The single source of
truth for VisualFlow/Flow compilation lives in `abstractruntime.visualflow_compiler`.
"""

from __future__ import annotations

from abstractruntime.visualflow_compiler.compiler import (
    _create_visual_agent_effect_handler,
    _sync_effect_results_to_node_outputs,
    compile_flow,
    compile_visualflow,
    compile_visualflow_tree,
)

__all__ = [
    "compile_flow",
    "compile_visualflow",
    "compile_visualflow_tree",
    "_create_visual_agent_effect_handler",
    "_sync_effect_results_to_node_outputs",
]

---
file: abstractflow/workflow_bundle.py
---

"""WorkflowBundle (.flow) tooling (authoring-side).

This module remains for backwards compatibility, but bundling logic lives in
`abstractruntime.workflow_bundle` so hosts and clients share the same semantics.
"""

from __future__ import annotations

import json
import zipfile
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

from abstractruntime.workflow_bundle import (
    WORKFLOW_BUNDLE_FORMAT_VERSION_V1,
    WorkflowBundleEntrypoint,
    WorkflowBundleManifest,
    WorkflowBundleError,
    open_workflow_bundle,
    workflow_bundle_manifest_to_dict,
)

from .visual.models import NodeType, VisualFlow


@dataclass(frozen=True)
class PackedWorkflowBundle:
    path: Path
    manifest: WorkflowBundleManifest


def _now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def _read_json_bytes(path: Path) -> bytes:
    return path.read_bytes()


def _load_visualflow_from_bytes(raw: bytes) -> VisualFlow:
    data = json.loads(raw.decode("utf-8"))
    return VisualFlow.model_validate(data)


def _node_type_str(node: Any) -> str:
    t = getattr(node, "type", None)
    return t.value if hasattr(t, "value") else str(t or "")


def _reachable_exec_node_ids(flow: VisualFlow) -> set[str]:
    """Return exec-reachable node ids (Blueprint-style; ignores disconnected exec nodes)."""
    exec_ids: set[str] = set()
    for n in flow.nodes:
        data = dict(n.data) if isinstance(n.data, dict) else {}
        pins = data.get("inputs") if isinstance(data.get("inputs"), list) else []
        pins2 = data.get("outputs") if isinstance(data.get("outputs"), list) else []
        for p in list(pins) + list(pins2):
            if isinstance(p, dict) and p.get("type") == "execution":
                exec_ids.add(str(n.id))
                break

    if not exec_ids:
        return set()

    incoming_exec = {e.target for e in flow.edges if getattr(e, "targetHandle", None) == "exec-in"}

    roots: list[str] = []
    if isinstance(flow.entryNode, str) and flow.entryNode in exec_ids:
        roots.append(flow.entryNode)
    for n in flow.nodes:
        if _node_type_str(n) == str(NodeType.ON_EVENT.value) and n.id in exec_ids:
            roots.append(n.id)
    if not roots:
        for n in flow.nodes:
            if n.id in exec_ids and n.id not in incoming_exec:
                roots.append(n.id)
                break
    if not roots:
        roots.append(next(iter(exec_ids)))

    adj: Dict[str, list[str]] = {}
    for e in flow.edges:
        if getattr(e, "targetHandle", None) != "exec-in":
            continue
        if e.source not in exec_ids or e.target not in exec_ids:
            continue
        adj.setdefault(e.source, []).append(e.target)

    reachable: set[str] = set()
    stack = list(dict.fromkeys([r for r in roots if isinstance(r, str) and r]))
    while stack:
        cur = stack.pop()
        if cur in reachable:
            continue
        reachable.add(cur)
        for nxt in adj.get(cur, []):
            if nxt not in reachable:
                stack.append(nxt)
    return reachable


def _collect_reachable_flows(
    *, root_flow: VisualFlow, root_bytes: bytes, flows_dir: Path
) -> Tuple[List[Tuple[str, VisualFlow, bytes]], List[str]]:
    """Return [(flow_id, flow, raw_bytes)] in discovery order + list of missing subflow ids."""
    ordered: list[Tuple[str, VisualFlow, bytes]] = []
    visited: set[str] = set()
    missing: list[str] = []

    # Memoize loaded files by id for reuse.
    cache: Dict[str, Tuple[VisualFlow, bytes]] = {str(root_flow.id): (root_flow, root_bytes)}

    def _load_by_id(flow_id: str) -> Optional[Tuple[VisualFlow, bytes]]:
        fid = str(flow_id or "").strip()
        if not fid:
            return None
        if fid in cache:
            return cache[fid]
        p = (flows_dir / f"{fid}.json").resolve()
        if not p.exists():
            return None
        raw = _read_json_bytes(p)
        vf = _load_visualflow_from_bytes(raw)
        cache[fid] = (vf, raw)
        return cache[fid]

    def _dfs(vf: VisualFlow, raw: bytes) -> None:
        fid = str(vf.id)
        if fid in visited:
            return
        visited.add(fid)
        ordered.append((fid, vf, raw))

        reachable = _reachable_exec_node_ids(vf)
        for n in vf.nodes:
            if _node_type_str(n) != str(NodeType.SUBFLOW.value):
                continue
            if reachable and n.id not in reachable:
                continue
            data = n.data if isinstance(n.data, dict) else {}
            sub_id = data.get("subflowId") or data.get("flowId")
            if not isinstance(sub_id, str) or not sub_id.strip():
                # Match VisualFlow executor behavior: fail if a reachable subflow is malformed.
                missing.append(f"<missing-subflow-id:{fid}:{n.id}>")
                continue
            sub_id = sub_id.strip()
            child = _load_by_id(sub_id)
            if child is None:
                # Self-recursion is valid even if the file isn't duplicated on disk.
                if sub_id == fid:
                    _dfs(vf, raw)
                    continue
                missing.append(sub_id)
                continue
            _dfs(child[0], child[1])

    _dfs(root_flow, root_bytes)
    return ordered, missing


def pack_workflow_bundle(
    *,
    root_flow_json: str | Path,
    out_path: str | Path,
    bundle_id: Optional[str] = None,
    bundle_version: str = "0.0.0",
    flows_dir: Optional[str | Path] = None,
    entrypoints: Optional[List[str]] = None,
    default_entrypoint: Optional[str] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> PackedWorkflowBundle:
    """Pack a `.flow` bundle from a root VisualFlow JSON file."""
    root_path = Path(root_flow_json).expanduser().resolve()
    if not root_path.exists():
        raise FileNotFoundError(f"root flow not found: {root_path}")
    root_bytes = _read_json_bytes(root_path)
    root_flow = _load_visualflow_from_bytes(root_bytes)

    flows_base = Path(flows_dir).expanduser().resolve() if flows_dir is not None else root_path.parent
    if not flows_base.exists() or not flows_base.is_dir():
        raise FileNotFoundError(f"flows_dir does not exist: {flows_base}")

    ordered, missing = _collect_reachable_flows(root_flow=root_flow, root_bytes=root_bytes, flows_dir=flows_base)
    if missing:
        uniq = sorted(set(missing))
        raise WorkflowBundleError(f"Missing referenced subflows in flows_dir: {uniq}")

    # Entry points: default to root flow id.
    entry_ids = list(entrypoints) if isinstance(entrypoints, list) and entrypoints else [str(root_flow.id)]
    entry_ids = [str(x).strip() for x in entry_ids if isinstance(x, str) and str(x).strip()]
    if not entry_ids:
        raise WorkflowBundleError("No valid entrypoints specified")

    de_param = str(default_entrypoint).strip() if isinstance(default_entrypoint, str) and str(default_entrypoint).strip() else ""
    if de_param and de_param not in entry_ids:
        raise WorkflowBundleError(f"default_entrypoint '{de_param}' must be one of: {entry_ids}")
    default_ep = de_param or (str(root_flow.id).strip() if str(root_flow.id).strip() in entry_ids else entry_ids[0])

    # Compile artifacts for all included flows.
    flows_json: Dict[str, bytes] = {}
    interfaces_by_flow: Dict[str, list[str]] = {}
    name_by_flow: Dict[str, str] = {}
    desc_by_flow: Dict[str, str] = {}

    for fid, vf, raw in ordered:
        flows_json[fid] = raw
        name_by_flow[fid] = str(getattr(vf, "name", "") or "")
        desc_by_flow[fid] = str(getattr(vf, "description", "") or "")
        interfaces_by_flow[fid] = list(getattr(vf, "interfaces", []) or [])

    bid = str(bundle_id or "").strip() or str(root_flow.id)
    created_at = _now_iso()

    eps: list[WorkflowBundleEntrypoint] = []
    for fid in entry_ids:
        fid2 = str(fid or "").strip()
        if not fid2:
            continue
        eps.append(
            WorkflowBundleEntrypoint(
                flow_id=fid2,
                name=name_by_flow.get(fid2) or fid2,
                description=desc_by_flow.get(fid2, ""),
                interfaces=list(interfaces_by_flow.get(fid2, [])),
            )
        )
    if not eps:
        raise WorkflowBundleError("No valid entrypoints specified")

    manifest = WorkflowBundleManifest(
        bundle_format_version=WORKFLOW_BUNDLE_FORMAT_VERSION_V1,
        bundle_id=bid,
        bundle_version=str(bundle_version or "0.0.0"),
        created_at=created_at,
        entrypoints=eps,
        default_entrypoint=default_ep,
        flows={fid: f"flows/{fid}.json" for fid in sorted(flows_json.keys())},
        artifacts={},
        assets={},
        metadata=dict(metadata) if isinstance(metadata, dict) else {},
    )
    manifest.validate()

    out = Path(out_path).expanduser().resolve()
    out.parent.mkdir(parents=True, exist_ok=True)

    # Deterministic write order for reproducibility.
    with zipfile.ZipFile(out, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.writestr("manifest.json", json.dumps(workflow_bundle_manifest_to_dict(manifest), indent=2, ensure_ascii=False))
        for fid in sorted(flows_json.keys()):
            zf.writestr(f"flows/{fid}.json", flows_json[fid])

    return PackedWorkflowBundle(path=out, manifest=manifest)


def inspect_workflow_bundle(*, bundle_path: str | Path) -> WorkflowBundleManifest:
    b = open_workflow_bundle(bundle_path)
    return b.manifest


def unpack_workflow_bundle(*, bundle_path: str | Path, out_dir: str | Path) -> Path:
    src = Path(bundle_path).expanduser().resolve()
    out = Path(out_dir).expanduser().resolve()
    out.mkdir(parents=True, exist_ok=True)

    if src.is_dir():
        # Directory bundle: copy files (best-effort).
        for p in src.rglob("*"):
            if p.is_dir():
                continue
            rel = p.relative_to(src)
            dst = out / rel
            dst.parent.mkdir(parents=True, exist_ok=True)
            dst.write_bytes(p.read_bytes())
        return out

    with zipfile.ZipFile(src, "r") as zf:
        zf.extractall(out)
    return out


# Backwards-compat: prefer the shared stdlib-only implementation in AbstractRuntime.
from abstractruntime.workflow_bundle import (  # noqa: E402
    PackedWorkflowBundle as PackedWorkflowBundle,
    inspect_workflow_bundle as inspect_workflow_bundle,
    pack_workflow_bundle as pack_workflow_bundle,
    unpack_workflow_bundle as unpack_workflow_bundle,
)

---
file: docs/web-editor.md
---

# Visual editor (reference app in `web/`)

This repository includes a reference visual editor:
- FastAPI backend: `web/backend/`
- React frontend: `web/frontend/`

It is intended for development and as a reference host for executing VisualFlows. For convenience, the backend is also shipped as an **optional** part of the Python package (`abstractflow[editor]` / `abstractflow[server]`).

See also: [../README.md](../README.md), [getting-started.md](getting-started.md), [faq.md](faq.md), [visualflow.md](visualflow.md), [architecture.md](architecture.md).

## Run (recommended: PyPI + npx)

Terminal 1 (backend):

```bash
python -m venv .venv
source .venv/bin/activate
pip install "abstractflow[editor]"

abstractflow serve --reload --port 8080
```

Alternative entrypoint (equivalent):

```bash
abstractflow-backend --reload --port 8080
```

Terminal 2 (UI):

```bash
npx @abstractframework/flow
```

Notes:
- The UI expects `/api/*` on the same origin; the `npx` server proxies `/api/*` (HTTP + WebSocket) to the backend (default: `http://127.0.0.1:8080`).
- Override the backend target with `--backend-url ...` (or env `ABSTRACTFLOW_BACKEND_URL`).
- `abstractflow[editor]` is equivalent to installing `abstractflow[server]` + `abstractflow[agent]` together.

Open:
- UI: http://localhost:3003
- Backend health: http://localhost:8080/api/health

## Run (from source / dev mode)

Terminal 1 (backend):

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e ".[server,agent]"

cd web
python -m backend --reload --port 8080
```

Terminal 2 (frontend):

```bash
cd web/frontend
npm install
npm run dev
```

Open:
- Frontend: http://localhost:3003 (Vite dev server; see `web/frontend/package.json`)
- Backend: http://localhost:8080/api/health (FastAPI; see `web/backend/main.py`)

## Run (single-process “production” mode)

Build the frontend and let the backend serve it:

```bash
cd web/frontend
npm install
npm run build

cd ../
python -m backend --port 8080
```

Evidence: [../web/backend/main.py](../web/backend/main.py) serves `web/frontend/dist` when it exists.

## Where data is stored

- Flows: `./flows/*.json` relative to the backend working directory.
  - Override with `ABSTRACTFLOW_FLOWS_DIR` (recommended when using the packaged backend).
  - Evidence: `FLOWS_DIR` in [../web/backend/routes/flows.py](../web/backend/routes/flows.py).
- Runtime persistence (runs/ledger/artifacts): defaults to:
  - source checkout: `web/runtime/`
  - installed package: `~/.abstractflow/runtime`
  - Override with `ABSTRACTFLOW_RUNTIME_DIR`.
  - Evidence: [../web/backend/services/paths.py](../web/backend/services/paths.py), [../web/backend/services/runtime_stores.py](../web/backend/services/runtime_stores.py).

## Gateway connectivity (optional)

The backend can talk to an AbstractGateway for embeddings-backed memory KG operations and bundle publishing/reload.

Common env vars / flags:
- `ABSTRACTFLOW_GATEWAY_URL` (or `ABSTRACTGATEWAY_URL`)
- `ABSTRACTGATEWAY_AUTH_TOKEN` (legacy: `ABSTRACTFLOW_GATEWAY_AUTH_TOKEN`)
- CLI flags: `abstractflow serve --gateway-url ... --gateway-token ...` (or `python -m backend ...`) (see `web/backend/cli.py`)

Evidence:
- UI modal: [../web/frontend/src/components/GatewayConnectionModal.tsx](../web/frontend/src/components/GatewayConnectionModal.tsx)
- Backend persistence + env bootstrap: [../web/backend/services/gateway_connection.py](../web/backend/services/gateway_connection.py)
- Embeddings config check + KG embedder wiring: [../web/backend/routes/connection.py](../web/backend/routes/connection.py), [../web/backend/routes/memory_kg.py](../web/backend/routes/memory_kg.py)
- Bundle upload/reload on publish: [../web/backend/routes/flows.py](../web/backend/routes/flows.py)

## Tools (AbstractCore)

Tool lists shown in the editor come from the backend:
- HTTP endpoint: `GET /api/tools` (`web/backend/routes/tools.py`)
- Execution: tool calls are run by the host tool executor (`abstractflow/visual/workspace_scoped_tools.py`)

By default, the backend exposes a conservative tool set derived from AbstractRuntime’s “default tools” list, plus a small number of extra safe web helpers.

To add or customize tools, update the host:
- Tool discovery (`GET /api/tools`): `web/backend/routes/tools.py`
- Tool execution (workspace scoping + mapping executor): `abstractflow/visual/workspace_scoped_tools.py`

Evidence: [../web/backend/routes/tools.py](../web/backend/routes/tools.py), [../abstractflow/visual/workspace_scoped_tools.py](../abstractflow/visual/workspace_scoped_tools.py).

## WebSocket execution

The Run UI uses WebSocket messages:
- `{ "type": "run", "input_data": {…} }`
- `{ "type": "resume", "response": "…" }`
- `{ "type": "control", "action": "pause|resume|cancel", "run_id": "…" }`

Evidence: [../web/backend/routes/ws.py](../web/backend/routes/ws.py).

---
file: web/backend/routes/ws.py
---

"""WebSocket routes for real-time execution updates."""

from __future__ import annotations

import asyncio
import json
import os
import threading
import time
from datetime import datetime, timezone
from typing import Any, Dict, Optional, Tuple

from fastapi import APIRouter, WebSocket, WebSocketDisconnect

from ..models import ExecutionEvent, VisualFlow
from ..services.executor import create_visual_runner
from ..services.execution_workspace import ensure_default_workspace_root, ensure_run_id_workspace_alias
from ..services.runtime_stores import get_runtime_stores
from abstractflow.visual.workspace_scoped_tools import WorkspaceScope, build_scoped_tool_executor

router = APIRouter(tags=["websocket"])

class _SafeSendWebSocket:
    """Send-only WebSocket wrapper that never raises on send.

    Why:
    - In dev (Vite proxy) and in real networks, WebSockets can drop unexpectedly.
    - We do NOT want a transient UI disconnect to abort or corrupt a long-running run.
    - The durable source-of-truth is the RunStore/LedgerStore, not the socket.

    This wrapper makes `send_json` a best-effort no-op once the connection is gone.
    """

    def __init__(self, websocket: WebSocket):
        self._ws = websocket

    async def send_json(self, data: Any) -> None:  # type: ignore[override]
        try:
            await self._ws.send_json(data)
        except Exception:
            # Swallow errors: the execution loop should keep progressing even if the UI disconnects.
            return None


def _ws_utc_now_iso() -> str:
    """UTC ISO timestamp for WS event payloads (JSON-serializable)."""
    return datetime.now(timezone.utc).isoformat()

# Active WebSocket connections
_connections: Dict[str, WebSocket] = {}

# Active execution tasks per connection (run/resume segments).
_active_tasks: Dict[str, asyncio.Task] = {}

# Active runner per connection (includes runtime + run_id).
_active_runners: Dict[str, Any] = {}

# Root run_id per connection (when started).
_active_run_ids: Dict[str, str] = {}

# Pause gate per connection: cleared when paused, set when running.
_control_gates: Dict[str, asyncio.Event] = {}

# Per-connection lock to serialize runtime mutations (tick/resume/pause/cancel).
_connection_locks: Dict[str, asyncio.Lock] = {}

# Active FlowRunners for waiting flows (keyed by connection_id)
_waiting_runners: Dict[str, Any] = {}

# Waiting step context so we can correctly close the waiting node on resume.
# Without this, flows that resume to a different node (ASK_USER, START_SUBWORKFLOW)
# would leave the original node permanently in WAITING in the UI.
_waiting_steps: Dict[str, Dict[str, Any]] = {}

# Flow storage reference (shared with flows.py)
from .flows import _flows

# IMPORTANT: UI trace/scratchpad rendering depends on nested fields like
# `scratchpad.steps[].effect.type/payload` and `scratchpad.steps[].result`.
# A too-small depth limit collapses these into "…", making traces unreadable.
#
# Keep depth bounded to avoid pathological recursion. We do not truncate
# normal user-visible outputs (no string/list/dict caps).
_MAX_JSON_DEPTH = 64


def _json_safe(value: Any, *, depth: int = 0, seen: Optional[set[int]] = None) -> Any:
    """Best-effort JSON-safe conversion (no truncation)."""
    if depth > _MAX_JSON_DEPTH:
        return str(value)

    if value is None or isinstance(value, (bool, int, float, str)):
        return value

    if seen is None:
        seen = set()

    # Track the current recursion path (not a global "seen") so repeated references
    # are preserved, while actual cycles are replaced with "<cycle>".
    vid: Optional[int] = None
    try:
        vid = id(value)
    except Exception:
        vid = None

    if vid is not None:
        if vid in seen:
            return "<cycle>"
        seen.add(vid)

    try:
        if isinstance(value, dict):
            out: Dict[str, Any] = {}
            for k, v in value.items():
                out[str(k)] = _json_safe(v, depth=depth + 1, seen=seen)
            return out

        if isinstance(value, (list, tuple)):
            return [_json_safe(v, depth=depth + 1, seen=seen) for v in list(value)]

        # Pydantic models / dataclasses (best-effort).
        try:
            md = getattr(value, "model_dump", None)
            if callable(md):
                return _json_safe(md(), depth=depth + 1, seen=seen)  # type: ignore[no-any-return]
        except Exception:
            pass

        return str(value)
    finally:
        if vid is not None:
            try:
                seen.discard(vid)
            except Exception:
                pass


def _is_pause_wait(wait: Any, *, run_id: str) -> bool:
    """Return True if this WaitState represents a synthetic manual pause."""
    if wait is None:
        return False
    try:
        reason = getattr(wait, "reason", None)
        reason_value = reason.value if hasattr(reason, "value") else str(reason) if reason else None
    except Exception:
        reason_value = None
    if reason_value != "user":
        return False
    try:
        wait_key = getattr(wait, "wait_key", None)
        if isinstance(wait_key, str) and wait_key == f"pause:{run_id}":
            return True
    except Exception:
        pass
    try:
        details = getattr(wait, "details", None)
        if isinstance(details, dict) and details.get("kind") == "pause":
            return True
    except Exception:
        pass
    return False


def _list_descendant_run_ids(runtime: Any, root_run_id: str) -> list[str]:
    """Best-effort BFS of the run tree rooted at root_run_id (includes root)."""
    out: list[str] = []
    queue: list[str] = [root_run_id]
    seen: set[str] = set()

    run_store = getattr(runtime, "run_store", None)
    list_children = getattr(run_store, "list_children", None)

    while queue:
        rid = queue.pop(0)
        if rid in seen:
            continue
        seen.add(rid)
        out.append(rid)

        if callable(list_children):
            try:
                children = list_children(parent_run_id=rid)
                for c in children:
                    cid = getattr(c, "run_id", None)
                    if isinstance(cid, str) and cid and cid not in seen:
                        queue.append(cid)
            except Exception:
                continue

    return out


@router.websocket("/ws/{flow_id}")
async def websocket_execution(websocket: WebSocket, flow_id: str):
    """WebSocket endpoint for real-time flow execution updates."""
    await websocket.accept()
    connection_id = f"{flow_id}:{id(websocket)}"
    _connections[connection_id] = websocket
    _connection_locks[connection_id] = asyncio.Lock()
    gate = asyncio.Event()
    gate.set()
    _control_gates[connection_id] = gate

    try:
        while True:
            # Receive message from client
            data = await websocket.receive_text()
            message = json.loads(data)

            if message.get("type") == "run":
                existing = _active_tasks.get(connection_id)
                if existing is not None and not existing.done():
                    await websocket.send_json(
                        ExecutionEvent(type="flow_error", error="A run is already in progress on this connection").model_dump()
                    )
                    continue
                safe_ws = _SafeSendWebSocket(websocket)
                task = asyncio.create_task(
                    execute_with_updates(
                        websocket=safe_ws,  # type: ignore[arg-type]
                        flow_id=flow_id,
                        input_data=message.get("input_data", {}),
                        connection_id=connection_id,
                    )
                )
                _active_tasks[connection_id] = task
            elif message.get("type") == "resume":
                existing = _active_tasks.get(connection_id)
                if existing is not None and not existing.done():
                    await websocket.send_json(
                        ExecutionEvent(type="flow_error", error="Cannot resume while a run is in progress").model_dump()
                    )
                    continue
                safe_ws = _SafeSendWebSocket(websocket)
                task = asyncio.create_task(
                    resume_waiting_flow(
                        websocket=safe_ws,  # type: ignore[arg-type]
                        connection_id=connection_id,
                        response=message.get("response", ""),
                    )
                )
                _active_tasks[connection_id] = task
            elif message.get("type") == "control":
                action = str(message.get("action") or "").strip().lower()
                # Optional: allow the client to target a specific persisted run id.
                # This makes controls resilient to transient WS disconnects / UI reloads
                # (a new connection can still pause/cancel a previously started run).
                run_id = message.get("run_id") or message.get("runId")
                await control_run(
                    websocket=websocket,
                    connection_id=connection_id,
                    action=action,
                    run_id=str(run_id) if isinstance(run_id, str) and run_id.strip() else None,
                )
            elif message.get("type") == "ping":
                # Keepalive pong (ignored by the UI) - still includes a timestamp for observability.
                await websocket.send_json(ExecutionEvent(type="pong").model_dump())

    except WebSocketDisconnect:
        pass
    finally:
        # IMPORTANT:
        # Do NOT cancel in-flight run tasks when the UI disconnects.
        # The UI socket is an observability/control channel; execution is durable and should
        # continue (or remain waiting) independently of that socket.
        _active_tasks.pop(connection_id, None)
        if connection_id in _connections:
            del _connections[connection_id]
        _waiting_runners.pop(connection_id, None)
        _waiting_steps.pop(connection_id, None)
        _active_runners.pop(connection_id, None)
        _active_run_ids.pop(connection_id, None)
        _control_gates.pop(connection_id, None)
        _connection_locks.pop(connection_id, None)


async def execute_with_updates(
    websocket: WebSocket,
    flow_id: str,
    input_data: Dict[str, Any],
    connection_id: str,
) -> None:
    """Execute a flow and send real-time updates via WebSocket."""
    if flow_id not in _flows:
        await websocket.send_json(
            ExecutionEvent(
                type="flow_error",
                error=f"Flow '{flow_id}' not found",
            ).model_dump()
        )
        return

    visual_flow = _flows[flow_id]

    try:
        # Session semantics:
        # - Runtime scope `session` depends on `RunState.session_id`.
        # - WebSocket executions should default to a stable session_id so multiple
        #   flow runs in the same UI session can share session-scoped memory.
        #
        # Priority:
        # - explicit session id from client input_data (session_id / sessionId)
        # - otherwise: connection_id (stable for the lifetime of this socket)
        session_id = None
        try:
            raw = input_data.get("session_id") or input_data.get("sessionId")
            if isinstance(raw, str) and raw.strip():
                session_id = raw.strip()
        except Exception:
            session_id = None
        if session_id is None:
            session_id = str(connection_id or "").strip() or None

        gate = _control_gates.get(connection_id)
        if gate is not None:
            gate.set()

        # Default to an isolated per-run workspace so file/shell tools don't pollute the repo.
        # The stable alias `<ABSTRACTFLOW_BASE_EXECUTION>/<run_id>` is created after start.
        workspace_dir = ensure_default_workspace_root(input_data)

        run_store, ledger_store, artifact_store = get_runtime_stores()
        scope = WorkspaceScope.from_input_data(input_data)
        tool_executor = build_scoped_tool_executor(scope=scope) if scope is not None else None
        runner = create_visual_runner(
            visual_flow,
            flows=_flows,
            run_store=run_store,
            ledger_store=ledger_store,
            artifact_store=artifact_store,
            tool_executor=tool_executor,
            input_data=input_data,
        )
        _active_runners[connection_id] = runner

        # Start execution
        run_id = runner.start(input_data, session_id=session_id)
        if isinstance(run_id, str) and run_id:
            _active_run_ids[connection_id] = run_id
            if workspace_dir is not None:
                ensure_run_id_workspace_alias(run_id=run_id, workspace_dir=workspace_dir)
        await websocket.send_json(ExecutionEvent(type="flow_start", runId=run_id).model_dump())

        # Execute and handle waiting
        if gate is not None:
            await gate.wait()
        await _execute_runner_loop(websocket, runner, connection_id)

    except asyncio.CancelledError:
        # Cancellation is an expected control-plane operation (Cancel Run / disconnect).
        # Do not emit `flow_error` for it; the UI will receive `flow_cancelled`.
        return
    except Exception as e:
        import traceback
        traceback.print_exc()
        await websocket.send_json(
            ExecutionEvent(
                type="flow_error",
                error=str(e),
            ).model_dump()
        )
    finally:
        # Allow subsequent control/run messages to spawn new tasks.
        _active_tasks.pop(connection_id, None)


async def control_run(
    *,
    websocket: WebSocket,
    connection_id: str,
    action: str,
    run_id: Optional[str] = None,
) -> None:
    """Handle control operations (pause/resume/cancel) for the current run."""
    # Prefer an explicit run id from the client, fall back to the active connection run.
    runner = _active_runners.get(connection_id) or _waiting_runners.get(connection_id)
    run_id_target = run_id
    if not isinstance(run_id_target, str) or not run_id_target.strip():
        run_id_target = _active_run_ids.get(connection_id) or getattr(runner, "run_id", None)

    if not isinstance(run_id_target, str) or not run_id_target.strip():
        await websocket.send_json(ExecutionEvent(type="flow_error", error="No run_id provided to control").model_dump())
        return
    run_id_target = run_id_target.strip()

    # Use the runner's runtime when available, otherwise create a minimal runtime over the same stores.
    runtime = getattr(runner, "runtime", None) if runner is not None else None
    if runtime is None:
        try:
            run_store, ledger_store, artifact_store = get_runtime_stores()
            try:
                from abstractruntime import Runtime  # type: ignore
            except Exception:  # pragma: no cover
                from abstractruntime.core.runtime import Runtime  # type: ignore
            runtime = Runtime(run_store=run_store, ledger_store=ledger_store, artifact_store=artifact_store)
        except Exception as e:
            await websocket.send_json(
                ExecutionEvent(type="flow_error", error=f"Failed to create runtime for control: {e}").model_dump()
            )
            return

    gate = _control_gates.get(connection_id)
    # IMPORTANT:
    # Do NOT block on the per-connection lock here. The execution loop may hold it
    # across a long-running `runner.step()` (e.g. a local LLM HTTP call). If we wait
    # for the lock, pause/cancel/resume become unresponsive in exactly the scenarios
    # users need them most.
    #
    # Runtime durability is protected by:
    # - atomic RunStore writes (JsonFileRunStore.save)
    # - Runtime.tick() honoring externally persisted pause/cancel before saving

    action2 = str(action or "").strip().lower()
    if action2 not in {"pause", "resume", "cancel"}:
        await websocket.send_json(
            ExecutionEvent(type="flow_error", error=f"Unknown control action '{action2}'").model_dump()
        )
        return

    run_ids = _list_descendant_run_ids(runtime, run_id_target)

    def _apply(fn_name: str, *, reason: Optional[str] = None) -> None:
        fn = getattr(runtime, fn_name, None)
        if not callable(fn):
            raise RuntimeError(f"Runtime missing '{fn_name}()'")
        for rid in run_ids:
            if reason is None:
                fn(rid)
            else:
                fn(rid, reason=reason)

    try:
        if action2 == "pause":
            if gate is not None:
                gate.clear()
            # Apply synchronously: pause/cancel must remain responsive even if the threadpool
            # is saturated by long-running `runner.step()` calls.
            _apply("pause_run", reason="Paused via AbstractFlow UI")
            await websocket.send_json(ExecutionEvent(type="flow_paused", runId=run_id_target).model_dump())
            return

        if action2 == "resume":
            _apply("resume_run")
            if gate is not None:
                gate.set()
            await websocket.send_json(ExecutionEvent(type="flow_resumed", runId=run_id_target).model_dump())
            return

        # cancel
        if gate is not None:
            gate.set()
        _apply("cancel_run", reason="Cancelled via AbstractFlow UI")

        # Stop any in-flight execution loop promptly.
        task = _active_tasks.get(connection_id)
        if task is not None and not task.done():
            task.cancel()

        _waiting_runners.pop(connection_id, None)
        _waiting_steps.pop(connection_id, None)

        await websocket.send_json(ExecutionEvent(type="flow_cancelled", runId=run_id_target).model_dump())
    except Exception as e:
        await websocket.send_json(
            ExecutionEvent(type="flow_error", error=f"Control action failed: {e}").model_dump()
        )


async def _execute_runner_loop(
    websocket: WebSocket,
    runner: Any,
    connection_id: str,
) -> None:
    """Execute the runner loop with waiting support."""
    flow_started_at = time.perf_counter()
    gate = _control_gates.get(connection_id)
    lock = _connection_locks.get(connection_id)

    def _node_effect_type(node_id: str) -> Optional[str]:
        try:
            node = runner.flow.nodes.get(node_id) if hasattr(runner, "flow") and hasattr(runner.flow, "nodes") else None
        except Exception:
            node = None
        if node is None:
            return None
        t = getattr(node, "effect_type", None)
        return str(t) if isinstance(t, str) else None

    def _node_output(node_id: str) -> Any:
        if hasattr(runner, "flow") and hasattr(runner.flow, "_node_outputs"):
            outputs = getattr(runner.flow, "_node_outputs")
            if isinstance(outputs, dict) and node_id in outputs:
                raw = outputs.get(node_id)
                return _json_safe(raw)
        # Fallback for non-visual flows
        state = runner.get_state() if hasattr(runner, "get_state") else None
        if state and hasattr(state, "vars") and isinstance(state.vars, dict):
            return _json_safe(state.vars.get("_last_output"))
        return None

    def _extract_sub_run_id(wait: Any) -> Optional[str]:
        details = getattr(wait, "details", None)
        if isinstance(details, dict):
            sub_run_id = details.get("sub_run_id")
            if isinstance(sub_run_id, str) and sub_run_id:
                return sub_run_id
        wait_key = getattr(wait, "wait_key", None)
        if isinstance(wait_key, str) and wait_key.startswith("subworkflow:"):
            return wait_key.split("subworkflow:", 1)[1] or None
        return None

    def _utc_now_iso() -> str:
        from datetime import datetime, timezone

        return datetime.now(timezone.utc).isoformat()

    def _resolve_waiting_info(state: Any) -> Tuple[str, list, bool, Optional[str], Optional[str]]:
        """Return (prompt, choices, allow_free_text, wait_key, reason)."""
        wait = getattr(state, "waiting", None)
        if wait is None:
            return ("Please respond:", [], True, None, None)

        reason = getattr(wait, "reason", None)
        reason_value = reason.value if hasattr(reason, "value") else str(reason) if reason else None

        if reason_value != "subworkflow":
            prompt = getattr(wait, "prompt", None) or "Please respond:"
            choices = list(getattr(wait, "choices", []) or [])
            allow_free_text = bool(getattr(wait, "allow_free_text", True))
            wait_key = getattr(wait, "wait_key", None)
            return (prompt, choices, allow_free_text, wait_key, reason_value)

        # Bubble up the deepest waiting child so the UI can render ASK_USER prompts.
        runtime = getattr(runner, "runtime", None)
        if runtime is None:
            return ("Waiting for subworkflow…", [], True, getattr(wait, "wait_key", None), reason_value)

        sub_run_id = _extract_sub_run_id(wait)
        if not sub_run_id:
            return ("Waiting for subworkflow…", [], True, getattr(wait, "wait_key", None), reason_value)

        current_run_id = sub_run_id
        for _ in range(25):
            sub_state = runtime.get_state(current_run_id)
            sub_wait = getattr(sub_state, "waiting", None)
            if sub_wait is None:
                break
            sub_reason = getattr(sub_wait, "reason", None)
            sub_reason_value = (
                sub_reason.value if hasattr(sub_reason, "value") else str(sub_reason) if sub_reason else None
            )
            if sub_reason_value == "subworkflow":
                next_id = _extract_sub_run_id(sub_wait)
                if not next_id:
                    break
                current_run_id = next_id
                continue

            prompt = getattr(sub_wait, "prompt", None) or "Please respond:"
            choices = list(getattr(sub_wait, "choices", []) or [])
            allow_free_text = bool(getattr(sub_wait, "allow_free_text", True))
            wait_key = getattr(sub_wait, "wait_key", None)
            return (prompt, choices, allow_free_text, wait_key, sub_reason_value)

        return ("Waiting for subworkflow…", [], True, getattr(wait, "wait_key", None), reason_value)

    def _is_agent_node(node_id: str) -> bool:
        try:
            node = runner.flow.nodes.get(node_id) if hasattr(runner, "flow") and hasattr(runner.flow, "nodes") else None
        except Exception:
            node = None
        return bool(node is not None and getattr(node, "effect_type", None) == "agent")

    def _agent_phase(state_vars: Any, node_id: str) -> str:
        if not isinstance(state_vars, dict):
            return ""
        temp = state_vars.get("_temp")
        if not isinstance(temp, dict):
            return ""
        agent_ns = temp.get("agent")
        if not isinstance(agent_ns, dict):
            return ""
        bucket = agent_ns.get(node_id)
        if not isinstance(bucket, dict):
            return ""
        phase = bucket.get("phase")
        return str(phase) if phase is not None else ""

    runtime = getattr(runner, "runtime", None)

    # Runtime-owned node traces (stored in RunState.vars["_runtime"]["node_traces"]) are the
    # most reliable source of per-effect observability (LLM_CALL / TOOL_CALLS loops), especially
    # for Agent nodes that may stay on the same runtime node across many effects.
    #
    # We stream *deltas* (new entries) over the WS so the UI can render live traces without
    # waiting for the outer visual node to complete.
    trace_cursors: Dict[str, Dict[str, int]] = {}

    # Child subworkflow ticks can be configured to trade off granularity vs overhead.
    # More steps per tick reduces store IO and WS chatter for fast tool loops.
    try:
        child_tick_max_steps = int(os.environ.get("ABSTRACTFLOW_CHILD_TICK_MAX_STEPS", "5") or "5")
    except Exception:
        child_tick_max_steps = 5
    if child_tick_max_steps < 1:
        child_tick_max_steps = 1

    async def _emit_trace_deltas(run_id: str, state: Any) -> None:
        """Emit trace_update events for newly appended runtime node trace entries."""
        if runtime is None:
            return
        traces: Any = None
        try:
            vars_obj = getattr(state, "vars", None)
            if isinstance(vars_obj, dict):
                runtime_ns = vars_obj.get("_runtime")
                traces = runtime_ns.get("node_traces") if isinstance(runtime_ns, dict) else None
        except Exception:
            traces = None

        # Fallback (slower): load persisted run state from the runtime store.
        if traces is None:
            try:
                traces = runtime.get_node_traces(run_id)
            except Exception:
                return

        if not isinstance(traces, dict) or not traces:
            return

        cursor = trace_cursors.get(run_id)
        if not isinstance(cursor, dict):
            cursor = {}
            trace_cursors[run_id] = cursor

        for node_id, trace_obj in traces.items():
            if not isinstance(node_id, str) or not node_id:
                continue
            if not isinstance(trace_obj, dict):
                continue
            steps = trace_obj.get("steps")
            if not isinstance(steps, list) or not steps:
                continue

            prev = cursor.get(node_id, 0)
            if not isinstance(prev, int) or prev < 0:
                prev = 0
            if prev >= len(steps):
                cursor[node_id] = len(steps)
                continue

            new_steps = steps[prev:]
            cursor[node_id] = len(steps)
            # NOTE: The UI expects JSON-safe objects; runtime traces are already JSON-safe,
            # but we still run through _json_safe to guard against handler bugs.
            await websocket.send_json(
                {
                    "type": "trace_update",
                    "ts": _utc_now_iso(),
                    "runId": run_id,
                    "nodeId": node_id,
                    "steps": _json_safe(new_steps),
                }
            )

    # Backward-compat / test support:
    # Some tests patch `create_visual_runner()` with a minimal fake runner that does not
    # expose a Runtime. In that case, fall back to the legacy single-run loop.
    if runtime is None:
        active_node_id: Optional[str] = None
        root_run_id = getattr(runner, "run_id", None)
        while True:
            if gate is not None:
                await gate.wait()
            before = runner.get_state() if hasattr(runner, "get_state") else None
            node_before = getattr(before, "current_node", None) if before is not None else None

            if node_before and node_before != active_node_id:
                await websocket.send_json(
                    ExecutionEvent(type="node_start", runId=root_run_id, nodeId=node_before).model_dump()
                )
                active_node_id = node_before
                await asyncio.sleep(0)

            t0 = time.perf_counter()
            # NOTE: This branch exists primarily for tests that patch `create_visual_runner()`
            # with a minimal fake runner (no Runtime). Running `runner.step()` directly keeps
            # the control flow deterministic and avoids relying on thread executors.
            state = runner.step()
            duration_ms = (time.perf_counter() - t0) * 1000.0

            if hasattr(runner, "is_waiting") and runner.is_waiting():
                _waiting_runners[connection_id] = runner
                _waiting_steps[connection_id] = {
                    "node_id": node_before or getattr(state, "current_node", None),
                    "started_at": time.perf_counter(),
                }
                prompt, choices, allow_free_text, wait_key, reason = _resolve_waiting_info(state)
                await websocket.send_json(
                    {
                        "type": "flow_waiting",
                        "ts": _utc_now_iso(),
                        "runId": root_run_id,
                        "nodeId": node_before or getattr(state, "current_node", None),
                        "wait_key": wait_key,
                        "reason": reason,
                        "prompt": prompt,
                        "choices": choices,
                        "allow_free_text": allow_free_text,
                    }
                )
                break

            if active_node_id:
                should_close = True
                if _is_agent_node(active_node_id):
                    phase = _agent_phase(getattr(state, "vars", None), active_node_id)
                    if phase != "done" and not getattr(runner, "is_complete", lambda: False)() and not getattr(
                        runner, "is_failed", lambda: False
                    )():
                        should_close = False

                if should_close:
                    await websocket.send_json(
                        ExecutionEvent(
                            type="node_complete",
                            runId=root_run_id,
                            nodeId=active_node_id,
                            result=_node_output(active_node_id),
                            meta={"duration_ms": round(float(duration_ms), 2)},
                        ).model_dump()
                    )
                    active_node_id = None

            if hasattr(runner, "is_complete") and runner.is_complete():
                flow_duration_ms = (time.perf_counter() - flow_started_at) * 1000.0
                await websocket.send_json(
                    ExecutionEvent(
                        type="flow_complete",
                        runId=root_run_id,
                        result=getattr(state, "output", None),
                        meta={"duration_ms": round(float(flow_duration_ms), 2)},
                    ).model_dump()
                )
                break

            if hasattr(runner, "is_failed") and runner.is_failed():
                error_node = None
                if hasattr(state, "vars") and isinstance(state.vars, dict):
                    error_node = state.vars.get("_flow_error_node")
                await websocket.send_json(
                    ExecutionEvent(
                        type="flow_error",
                        runId=root_run_id,
                        nodeId=error_node,
                        error=getattr(state, "error", None),
                    ).model_dump()
                )
                break

            await asyncio.sleep(0.01)

        return

    # Track "open" node steps per run_id so we can support session-level execution
    # (root run + event listener runs) without losing UX semantics (Delay/event waits).
    run_tracks: Dict[str, Dict[str, Any]] = {}

    def _track(run_id: str) -> Dict[str, Any]:
        t = run_tracks.get(run_id)
        if not isinstance(t, dict):
            t = {
                "active_node_id": None,
                "active_duration_ms": 0.0,
                "wait_until_started_at": None,
                "wait_until_node_id": None,
                "wait_event_started_at": None,
                "wait_event_node_id": None,
                # START_SUBWORKFLOW waits (Agent nodes / Subflow nodes):
                # When a run is WAITING(reason=SUBWORKFLOW) and later gets resumed to a different node,
                # we must emit `node_complete` for the waiting node. Otherwise the UI keeps it stuck
                # as RUNNING forever (and subsequent nodes appear to run "in parallel").
                "subworkflow_started_at": None,
                "subworkflow_node_id": None,
                # For UX: allow the UI to expand and render child run steps while the parent
                # is still waiting on a subworkflow (so long-running subflows aren't "opaque").
                "subworkflow_child_run_id": None,
            }
            run_tracks[run_id] = t
        return t

    # Best-effort aggregate metrics (active time + token usage).
    total_duration_ms: float = 0.0
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    _token_lock = threading.Lock()
    _seen_token_step_ids: set[str] = set()

    def _extract_usage_tokens(usage_raw: Any) -> Tuple[Optional[int], Optional[int]]:
        """Return (input_tokens, output_tokens) from a usage object (best-effort)."""
        if not isinstance(usage_raw, dict):
            return (None, None)
        usage = usage_raw

        def _as_int(v: Any) -> Optional[int]:
            try:
                n = int(v)
                return n if n >= 0 else None
            except Exception:
                return None

        input_tokens = _as_int(usage.get("prompt_tokens"))
        if input_tokens is None:
            input_tokens = _as_int(usage.get("input_tokens"))

        output_tokens = _as_int(usage.get("completion_tokens"))
        if output_tokens is None:
            output_tokens = _as_int(usage.get("output_tokens"))

        return (input_tokens, output_tokens)

    def _node_metrics(node_id: str, *, duration_ms: float, output: Any) -> Dict[str, Any]:
        """Compute per-node metrics for UI badges (best-effort, JSON-safe)."""
        metrics: Dict[str, Any] = {"duration_ms": round(float(duration_ms), 2)}

        effect_type = _node_effect_type(node_id)
        if not effect_type:
            return metrics

        input_tokens: Optional[int] = None
        output_tokens: Optional[int] = None

        if effect_type == "llm_call":
            usage = None
            if isinstance(output, dict):
                raw = output.get("raw")
                if isinstance(raw, dict):
                    usage = raw.get("usage")
                if usage is None:
                    usage = output.get("usage")
            input_tokens, output_tokens = _extract_usage_tokens(usage if isinstance(usage, dict) else None)

        elif effect_type == "agent":
            scratchpad = output.get("scratchpad") if isinstance(output, dict) else None
            steps = scratchpad.get("steps") if isinstance(scratchpad, dict) else None
            if isinstance(steps, list):
                in_sum = 0
                out_sum = 0
                has_any = False
                for s in steps:
                    if not isinstance(s, dict):
                        continue
                    effect = s.get("effect")
                    if not isinstance(effect, dict):
                        continue
                    if effect.get("type") != "llm_call":
                        continue
                    result = s.get("result")
                    usage = result.get("usage") if isinstance(result, dict) else None
                    i, o = _extract_usage_tokens(usage if isinstance(usage, dict) else None)
                    if i is not None:
                        in_sum += i
                        has_any = True
                    if o is not None:
                        out_sum += o
                        has_any = True
                if has_any:
                    input_tokens = in_sum
                    output_tokens = out_sum

        if input_tokens is not None:
            metrics["input_tokens"] = int(input_tokens)
        if output_tokens is not None:
            metrics["output_tokens"] = int(output_tokens)
        if output_tokens is not None and duration_ms > 0:
            metrics["tokens_per_s"] = round(float(output_tokens) / (float(duration_ms) / 1000.0), 2)

        return metrics

    def _count_tokens_from_usage(usage: Any, *, key: Optional[str] = None) -> None:
        """Accumulate session token totals from an LLM `usage` object (best-effort)."""
        nonlocal total_input_tokens, total_output_tokens
        i, o = _extract_usage_tokens(usage)
        if i is None and o is None:
            return
        with _token_lock:
            if key and key in _seen_token_step_ids:
                return
            if key:
                _seen_token_step_ids.add(key)
            if i is not None:
                total_input_tokens += int(i)
            if o is not None:
                total_output_tokens += int(o)

    def _on_ledger_record(rec: Any) -> None:
        """Ledger subscriber callback: count tokens for completed LLM_CALL effects."""
        if not isinstance(rec, dict):
            return
        if rec.get("status") != "completed":
            return
        eff = rec.get("effect")
        if not isinstance(eff, dict) or eff.get("type") != "llm_call":
            return
        step_id = rec.get("step_id")
        step_key = step_id.strip() if isinstance(step_id, str) and step_id.strip() else None
        result = rec.get("result")
        usage = None
        if isinstance(result, dict):
            usage = result.get("usage")
            if usage is None:
                raw = result.get("raw")
                if isinstance(raw, dict):
                    usage = raw.get("usage")
        _count_tokens_from_usage(usage, key=step_key)

    root_run_id = getattr(runner, "run_id", None)
    if not isinstance(root_run_id, str) or not root_run_id:
        return

    # Aggregate token totals from durable ledger records (LLM_CALL usage).
    #
    # Why ledger:
    # - It captures *every* completed LLM call (including loops, agents, subflows).
    # - `_temp.effects` only stores the *latest* result per node id and would undercount loops.
    unsubscribe_ledger: Optional[Any] = None
    ledger_trace_task: Optional[asyncio.Task] = None
    try:
        ledger_store = runtime.ledger_store if runtime is not None else None  # type: ignore[union-attr]
    except Exception:
        ledger_store = None

    if ledger_store is not None and hasattr(ledger_store, "subscribe"):
        # Stream ledger records as trace_update events (STARTED/COMPLETED/FAILED/WAITING).
        #
        # Why ledger:
        # - STARTED records are appended *before* long-running effects execute (LLM/tool),
        #   so UIs can show truthful "something is happening" signals on slow hardware.
        # - COMPLETED/FAILED/WAITING updates reuse the same `step_id` and are appended as
        #   separate records, allowing the frontend to upsert (started -> completed).
        ledger_q: "asyncio.Queue[dict]" = asyncio.Queue()
        loop = asyncio.get_running_loop()

        descendant_cache: set[str] = {root_run_id}

        def _is_descendant_run_id(candidate: str) -> bool:
            if candidate in descendant_cache:
                return True
            if runtime is None:
                return False
            cur = candidate
            seen: set[str] = set()
            for _ in range(50):
                if cur in seen:
                    return False
                seen.add(cur)
                try:
                    st = runtime.get_state(cur)
                except Exception:
                    return False
                parent = getattr(st, "parent_run_id", None)
                if not isinstance(parent, str) or not parent.strip():
                    return False
                pid = parent.strip()
                if pid == root_run_id:
                    descendant_cache.add(candidate)
                    return True
                cur = pid
            return False

        def _trace_step_from_ledger(rec: dict) -> dict:
            # Normalize StepRecord (ledger) -> TraceStep (UI).
            status = rec.get("status")
            eff = rec.get("effect")
            step: dict = {
                "ts": rec.get("ended_at") or rec.get("started_at") or _utc_now_iso(),
                "status": status,
                "step_id": rec.get("step_id"),
                "attempt": rec.get("attempt"),
                "idempotency_key": rec.get("idempotency_key"),
                "effect": eff,
            }
            if status == "completed":
                step["result"] = rec.get("result")
            elif status == "failed":
                step["error"] = rec.get("error")
            elif status == "waiting":
                # Legacy trace entries store wait details under `wait`.
                res = rec.get("result")
                if isinstance(res, dict):
                    w = res.get("wait")
                    if isinstance(w, dict):
                        step["wait"] = w
                    else:
                        step["result"] = res
                else:
                    step["result"] = res
            return step

        async def _pump_ledger_traces() -> None:
            while True:
                rec = await ledger_q.get()
                if rec is None:
                    return
                try:
                    run_id = rec.get("run_id")
                    node_id = rec.get("node_id")
                    if not isinstance(run_id, str) or not run_id.strip():
                        continue
                    if not isinstance(node_id, str) or not node_id.strip():
                        continue
                    rid = run_id.strip()
                    if rid == root_run_id:
                        # Root run traces are ignored by the UI, so skip.
                        continue
                    if not _is_descendant_run_id(rid):
                        continue
                    await websocket.send_json(
                        {
                            "type": "trace_update",
                            "ts": _utc_now_iso(),
                            "runId": rid,
                            "nodeId": node_id,
                            "steps": _json_safe([_trace_step_from_ledger(rec)]),
                        }
                    )
                except Exception:
                    # Observability must not crash the WS loop.
                    continue

        ledger_trace_task = asyncio.create_task(_pump_ledger_traces())

        def _ledger_cb(payload: Any) -> None:
            if not isinstance(payload, dict):
                return
            rid = payload.get("run_id")
            if isinstance(rid, str) and rid.strip():
                # Fast-path filter for unrelated runs.
                if rid.strip() != root_run_id and not _is_descendant_run_id(rid.strip()):
                    return
            _on_ledger_record(payload)
            try:
                loop.call_soon_threadsafe(ledger_q.put_nowait, payload)
            except Exception:
                return

        try:
            unsubscribe_ledger = ledger_store.subscribe(_ledger_cb)  # type: ignore[attr-defined]
        except Exception:
            unsubscribe_ledger = None

        # Prime totals from already persisted records (handles resume/reconnect).
        try:
            if hasattr(ledger_store, "list") and runtime is not None:
                for rid in _list_descendant_run_ids(runtime, root_run_id):
                    for rec in ledger_store.list(rid):  # type: ignore[attr-defined]
                        _on_ledger_record(rec)
        except Exception:
            pass

    # Helper: get a durable output for a node from a given RunState (works for listener runs).
    def _run_node_output(state: Any, node_id: str) -> Any:
        if state is None or not hasattr(state, "vars") or not isinstance(state.vars, dict):
            return None
        temp = state.vars.get("_temp")
        if not isinstance(temp, dict):
            temp = {}

        def _agent_scratchpad() -> Any:
            agent_ns = temp.get("agent")
            if not isinstance(agent_ns, dict):
                return None
            bucket = agent_ns.get(node_id)
            if not isinstance(bucket, dict):
                return None
            return bucket.get("scratchpad")

        effects = temp.get("effects")
        if isinstance(effects, dict) and node_id in effects:
            raw = effects.get(node_id)
            effect_type = _node_effect_type(node_id)

            # Normalize child-run outputs to match root visual node outputs where possible.
            if effect_type == "llm_call":
                if isinstance(raw, dict):
                    return _json_safe({"response": raw.get("content"), "raw": raw})
                return _json_safe({"response": raw, "raw": raw})

            if effect_type == "agent":
                scratchpad = _agent_scratchpad()
                if scratchpad is not None:
                    return _json_safe({"result": raw, "scratchpad": scratchpad})
                return _json_safe(raw)

            return _json_safe(raw)
        node_outputs = temp.get("node_outputs")
        if isinstance(node_outputs, dict) and node_id in node_outputs:
            return _json_safe(node_outputs.get(node_id))
        last = state.vars.get("_last_output")
        return _json_safe(last)

    def _workflow_for_run_id(run_id: str) -> Any:
        if runtime is None:
            raise RuntimeError("Runtime missing on runner")
        st = runtime.get_state(run_id)
        if getattr(st, "workflow_id", None) == getattr(runner, "workflow", None).workflow_id:  # type: ignore[attr-defined]
            return runner.workflow
        reg = getattr(runtime, "workflow_registry", None)
        if reg is None:
            raise RuntimeError("workflow_registry missing on runtime (required for event listeners)")
        spec = reg.get(getattr(st, "workflow_id", None))
        if spec is None:
            raise RuntimeError(f"Workflow '{getattr(st, 'workflow_id', None)}' not found in registry")
        return spec

    async def _tick_run(run_id: str, *, is_root: bool) -> Optional[Any]:
        """Tick a run by at most one step and emit node_start/node_complete for it.

        Returns the updated state (or None if unchanged).
        """
        nonlocal total_duration_ms
        if runtime is None:
            return None
        if gate is not None:
            await gate.wait()
        track = _track(run_id)

        if lock is not None:
            async with lock:
                before = runtime.get_state(run_id)
        else:
            before = runtime.get_state(run_id)
        node_before = getattr(before, "current_node", None)

        # Close SUBWORKFLOW waits when the run has been resumed past the waiting node.
        #
        # Why: Runtime.resume(...) transitions WAITING(SUBWORKFLOW) → RUNNING(next_node) without
        # producing a new runtime step record for the waiting node. The WS layer must therefore
        # synthesize a `node_complete` so the UI timeline + canvas highlighting stay consistent.
        sw_node = track.get("subworkflow_node_id")
        if isinstance(sw_node, str) and sw_node:
            status_str = str(getattr(before, "status", None))
            still_waiting_sub = False
            if status_str == "RunStatus.WAITING":
                w = getattr(before, "waiting", None)
                r = getattr(w, "reason", None) if w is not None else None
                rv = r.value if hasattr(r, "value") else str(r) if r else None
                still_waiting_sub = rv == "subworkflow"
            if not still_waiting_sub and node_before != sw_node:
                started_at = track.get("subworkflow_started_at")
                waited_ms = 0.0
                if isinstance(started_at, (int, float)):
                    waited_ms = (time.perf_counter() - float(started_at)) * 1000.0

                # Keep root flow outputs in sync (so `_node_output` can see the resumed effect result).
                if is_root:
                    try:
                        from abstractflow.compiler import _sync_effect_results_to_node_outputs as _sync_effect_results  # type: ignore
                        if hasattr(runner, "flow"):
                            _sync_effect_results(before, runner.flow)
                    except Exception:
                        pass

                out_sw = _node_output(sw_node) if is_root else _run_node_output(before, sw_node)
                await websocket.send_json(
                    ExecutionEvent(
                        type="node_complete",
                        runId=run_id,
                        nodeId=sw_node,
                        result=out_sw,
                        meta=_node_metrics(sw_node, duration_ms=waited_ms, output=out_sw),
                    ).model_dump()
                )

                # Clear wait markers; allow the next node_start to become active.
                track["subworkflow_node_id"] = None
                track["subworkflow_started_at"] = None
                track["subworkflow_child_run_id"] = None
                active0 = track.get("active_node_id")
                if isinstance(active0, str) and active0 == sw_node:
                    track["active_node_id"] = None
                    track["active_duration_ms"] = 0.0

        # Blueprint-style UX: event listeners should not appear as a "running step"
        # while they are *waiting* for an EVENT. Keep them silent until they are
        # actually triggered (resumed by EMIT_EVENT).
        if not is_root and str(getattr(before, "status", None)) == "RunStatus.WAITING":
            w0 = getattr(before, "waiting", None)
            r0 = getattr(w0, "reason", None) if w0 is not None else None
            rv0 = r0.value if hasattr(r0, "value") else str(r0) if r0 else None
            if rv0 == "event":
                if isinstance(node_before, str) and node_before:
                    if track.get("wait_event_started_at") is None or track.get("wait_event_node_id") != node_before:
                        track["wait_event_started_at"] = time.perf_counter()
                        track["wait_event_node_id"] = node_before
                track["active_node_id"] = None
                track["active_duration_ms"] = 0.0
                return None

        # Close EVENT waits for non-root runs when they get resumed (wait is over).
        if (
            isinstance(track.get("wait_event_node_id"), str)
            and track.get("wait_event_node_id")
            and str(getattr(before, "status", None)) != "RunStatus.WAITING"
        ):
            wnode = str(track.get("wait_event_node_id"))
            started_at = track.get("wait_event_started_at")
            waited_ms = 0.0
            if isinstance(started_at, (int, float)):
                waited_ms = (time.perf_counter() - float(started_at)) * 1000.0
            # Emit start+complete at the moment the event is delivered (Blueprint-style).
            await websocket.send_json(
                ExecutionEvent(
                    type="node_start",
                    runId=run_id,
                    nodeId=wnode,
                ).model_dump()
            )
            out0 = _run_node_output(before, wnode)
            await websocket.send_json(
                ExecutionEvent(
                    type="node_complete",
                    runId=run_id,
                    nodeId=wnode,
                    result=out0,
                    meta=_node_metrics(wnode, duration_ms=waited_ms, output=out0),
                ).model_dump()
            )
            track["wait_event_node_id"] = None
            track["wait_event_started_at"] = None
            track["active_node_id"] = None
            track["active_duration_ms"] = 0.0

        # Terminal runs: avoid re-emitting node_start/node_complete on every loop
        # iteration while we keep the WS open to drive child runs.
        if str(getattr(before, "status", None)) in {"RunStatus.COMPLETED", "RunStatus.FAILED", "RunStatus.CANCELLED"}:
            track["active_node_id"] = None
            track["active_duration_ms"] = 0.0
            return None

        # Special-case WAIT_UNTIL (Delay): handle time-based waits without user prompts.
        wait0 = getattr(before, "waiting", None)
        if str(getattr(before, "status", None)) == "RunStatus.WAITING":
            if _is_pause_wait(wait0, run_id=run_id):
                # Manual pause: never surface as flow_waiting.
                track["active_node_id"] = None
                track["active_duration_ms"] = 0.0
                return None
            reason0 = getattr(wait0, "reason", None) if wait0 is not None else None
            reason_value0 = reason0.value if hasattr(reason0, "value") else str(reason0) if reason0 else None
            if reason_value0 == "until":
                if track.get("wait_until_started_at") is None or track.get("wait_until_node_id") != node_before:
                    track["wait_until_started_at"] = time.perf_counter()
                    track["wait_until_node_id"] = node_before

                until_raw = getattr(wait0, "until", None) if wait0 is not None else None
                remaining_s = 0.2
                try:
                    from datetime import datetime, timezone

                    until = datetime.fromisoformat(str(until_raw))
                    now = datetime.now(timezone.utc)
                    remaining_s = (until - now).total_seconds()
                except Exception:
                    remaining_s = 0.2

                if remaining_s > 0:
                    if gate is not None:
                        await gate.wait()
                    await asyncio.sleep(float(min(max(remaining_s, 0.05), 0.25)))
                    return None

                # Time elapsed: resume to next node, but do not execute it yet.
                wf = _workflow_for_run_id(run_id)
                resume_payload: Dict[str, Any] = {}
                try:
                    from datetime import datetime, timezone

                    if _node_effect_type(str(node_before)) == "on_schedule":
                        resume_payload = {"timestamp": datetime.now(timezone.utc).isoformat()}
                        if until_raw is not None:
                            resume_payload["scheduled_for"] = str(until_raw)
                except Exception:
                    resume_payload = {}

                if lock is not None:
                    async with lock:
                        await asyncio.to_thread(
                            runtime.resume,
                            workflow=wf,
                            run_id=run_id,
                            wait_key=None,
                            payload=resume_payload,
                            max_steps=0,
                        )
                        after_resume = runtime.get_state(run_id)
                else:
                    await asyncio.to_thread(
                        runtime.resume,
                        workflow=wf,
                        run_id=run_id,
                        wait_key=None,
                        payload=resume_payload,
                        max_steps=0,
                    )
                    after_resume = runtime.get_state(run_id)

                # Close Delay node step now (the wait is finished).
                active = track.get("active_node_id")
                if isinstance(active, str) and active:
                    if is_root:
                        # Ensure root flow outputs reflect the resumed effect payload.
                        try:
                            from abstractflow.compiler import _sync_effect_results_to_node_outputs as _sync_effect_results  # type: ignore

                            if hasattr(runner, "flow"):
                                _sync_effect_results(after_resume, runner.flow)
                        except Exception:
                            pass

                    waited_ms = 0.0
                    started = track.get("wait_until_started_at")
                    if isinstance(started, (int, float)):
                        waited_ms = (time.perf_counter() - float(started)) * 1000.0
                    out0 = _run_node_output(after_resume, active) if not is_root else _node_output(active)
                    await websocket.send_json(
                        ExecutionEvent(
                            type="node_complete",
                            runId=run_id,
                            nodeId=active,
                            result=out0,
                            meta=_node_metrics(active, duration_ms=waited_ms, output=out0),
                        ).model_dump()
                    )
                    track["active_node_id"] = None
                    track["active_duration_ms"] = 0.0
                track["wait_until_started_at"] = None
                track["wait_until_node_id"] = None
                return after_resume

        # Listener workflow entrypoints (On Event): transitioning into WAITING EVENT
        # must be silent, otherwise the UI shows the listener as "RUNNING" while it
        # is merely subscribed.
        if (
            not is_root
            and str(getattr(before, "status", None)) == "RunStatus.RUNNING"
            and isinstance(getattr(before, "workflow_id", None), str)
            and str(getattr(before, "workflow_id", None)).startswith("visual_event_listener_")
            and isinstance(node_before, str)
            and node_before
        ):
            try:
                wf = _workflow_for_run_id(run_id)
                entry = getattr(wf, "entry_node", None)
            except Exception:
                wf = None
                entry = None

            if wf is not None and isinstance(entry, str) and entry == node_before:
                state = await asyncio.to_thread(runtime.tick, workflow=wf, run_id=run_id, max_steps=1)

                if str(getattr(state, "status", None)) == "RunStatus.WAITING":
                    w = getattr(state, "waiting", None)
                    reason = getattr(w, "reason", None) if w is not None else None
                    reason_value = reason.value if hasattr(reason, "value") else str(reason) if reason else None
                    if reason_value == "event":
                        track["wait_event_started_at"] = time.perf_counter()
                        track["wait_event_node_id"] = node_before
                        track["active_node_id"] = None
                        track["active_duration_ms"] = 0.0
                        return state

                # Fall back to normal processing if the listener entrypoint didn't wait.
                track["active_node_id"] = None
                track["active_duration_ms"] = 0.0

        # Emit node_start when we enter a new node (per-run).
        if isinstance(node_before, str) and node_before and node_before != track.get("active_node_id"):
            await websocket.send_json(
                ExecutionEvent(
                    type="node_start",
                    runId=run_id,
                    nodeId=node_before,
                ).model_dump()
            )
            track["active_node_id"] = node_before
            track["active_duration_ms"] = 0.0
            await asyncio.sleep(0)

        # Tick one step
        t0 = time.perf_counter()
        if is_root:
            if lock is not None:
                async with lock:
                    state = await asyncio.to_thread(runner.step)
            else:
                state = await asyncio.to_thread(runner.step)
        else:
            wf = _workflow_for_run_id(run_id)
            if lock is not None:
                async with lock:
                    state = await asyncio.to_thread(
                        runtime.tick, workflow=wf, run_id=run_id, max_steps=child_tick_max_steps
                    )
            else:
                state = await asyncio.to_thread(
                    runtime.tick, workflow=wf, run_id=run_id, max_steps=child_tick_max_steps
                )
        duration_ms = (time.perf_counter() - t0) * 1000.0

        # Emit per-effect trace deltas for this run (including child agent runs).
        #
        # Primary source: ledger stream (STARTED/COMPLETED/FAILED) via subscription.
        # Fallback: runtime node_traces (older runtimes / tests).
        if not is_root and ledger_trace_task is None:
            await _emit_trace_deltas(run_id, state)

        if is_root:
            # Keep root flow outputs in sync for rich previews and metrics.
            try:
                from abstractflow.compiler import _sync_effect_results_to_node_outputs as _sync_effect_results  # type: ignore
                if hasattr(runner, "flow"):
                    _sync_effect_results(state, runner.flow)
            except Exception:
                pass

        # Accumulate wall time per node while it stays "open" (multi-tick nodes like Agent).
        active = track.get("active_node_id")
        if isinstance(active, str) and active:
            try:
                track["active_duration_ms"] = float(track.get("active_duration_ms") or 0.0) + float(duration_ms)
            except Exception:
                track["active_duration_ms"] = float(duration_ms)

        # Waiting
        if str(getattr(state, "status", None)) == "RunStatus.WAITING":
            wait = getattr(state, "waiting", None)
            if is_root and _is_pause_wait(wait, run_id=root_run_id):
                # Manual pause: do not surface as flow_waiting.
                track["active_node_id"] = None
                track["active_duration_ms"] = 0.0
                return state
            reason = getattr(wait, "reason", None) if wait is not None else None
            reason_value = reason.value if hasattr(reason, "value") else str(reason) if reason else None

            if reason_value == "subworkflow":
                active0 = track.get("active_node_id")
                sw_id = active0 if isinstance(active0, str) and active0 else (node_before if isinstance(node_before, str) else None)
                if isinstance(sw_id, str) and sw_id:
                    if track.get("subworkflow_started_at") is None or track.get("subworkflow_node_id") != sw_id:
                        track["subworkflow_started_at"] = time.perf_counter()
                        track["subworkflow_node_id"] = sw_id

                    sub_run_id = _extract_sub_run_id(wait)
                    if isinstance(sub_run_id, str) and sub_run_id and track.get("subworkflow_child_run_id") != sub_run_id:
                        track["subworkflow_child_run_id"] = sub_run_id
                        # Inform the UI of the child run id *while* the parent node is still running.
                        await websocket.send_json(
                            {
                                "type": "subworkflow_update",
                                "ts": _utc_now_iso(),
                                "runId": run_id,
                                "nodeId": sw_id,
                                "sub_run_id": sub_run_id,
                            }
                        )

            if reason_value == "until":
                if track.get("wait_until_started_at") is None or track.get("wait_until_node_id") != track.get("active_node_id"):
                    track["wait_until_started_at"] = time.perf_counter()
                    track["wait_until_node_id"] = track.get("active_node_id")
                return state

            if reason_value == "event" and not is_root:
                # Listener waits are silent; we close the node we just executed (if any),
                # then track the WAIT_EVENT node so we can emit it only when resumed.
                #
                # Without this, a listener that runs a normal node (e.g. ANSWER_USER)
                # and then returns to WAITING(event) would drop the final `node_complete`.
                wait_node = getattr(state, "current_node", None)
                wait_node_id = str(wait_node) if isinstance(wait_node, str) else (str(node_before) if isinstance(node_before, str) else "")

                active0 = track.get("active_node_id")
                if isinstance(active0, str) and active0 and active0 != wait_node_id:
                    out0 = _run_node_output(state, active0)
                    total_node_ms = float(track.get("active_duration_ms") or duration_ms)
                    metrics = _node_metrics(active0, duration_ms=total_node_ms, output=out0)
                    await websocket.send_json(
                        ExecutionEvent(
                            type="node_complete",
                            runId=run_id,
                            nodeId=active0,
                            result=out0,
                            meta=metrics,
                        ).model_dump()
                    )

                if wait_node_id:
                    if track.get("wait_event_started_at") is None or track.get("wait_event_node_id") != wait_node_id:
                        track["wait_event_started_at"] = time.perf_counter()
                        track["wait_event_node_id"] = wait_node_id
                track["active_node_id"] = None
                track["active_duration_ms"] = 0.0
                return state

            # Root waits (or non-event waits) are surfaced to the UI *only* when the deepest
            # waiting reason requires user input. A root waiting on SUBWORKFLOW completion
            # must keep running so we can tick the child and stream live trace updates.
            if is_root:
                prompt, choices, allow_free_text, wait_key, reason = _resolve_waiting_info(state)
                if reason != "subworkflow":
                    _waiting_runners[connection_id] = runner
                    _waiting_steps[connection_id] = {
                        "node_id": node_before or getattr(state, "current_node", None),
                        "started_at": time.perf_counter(),
                    }
                    await websocket.send_json(
                        {
                            "type": "flow_waiting",
                            "ts": _utc_now_iso(),
                            "runId": run_id,
                            "nodeId": node_before or state.current_node,
                            "wait_key": wait_key,
                            "reason": reason,
                            "prompt": prompt,
                            "choices": choices,
                            "allow_free_text": allow_free_text,
                        }
                    )
            return state

        # Completed step: emit node_complete for the node we just executed.
        active = track.get("active_node_id")
        if isinstance(active, str) and active:
            should_close = True
            if _is_agent_node(active):
                phase = _agent_phase(getattr(state, "vars", None), active)
                status_raw = getattr(state, "status", None)
                status_val = getattr(status_raw, "value", None) if status_raw is not None else None
                status_str = status_val if isinstance(status_val, str) else (status_raw if isinstance(status_raw, str) else str(status_raw))
                is_running = status_str in {"running", "RunStatus.RUNNING"}
                if phase != "done" and is_running:
                    should_close = False
            if should_close:
                out0 = _node_output(active) if is_root else _run_node_output(state, active)
                total_node_ms = float(track.get("active_duration_ms") or duration_ms)
                metrics = _node_metrics(active, duration_ms=total_node_ms, output=out0)
                # Best-effort wall time (kept for future aggregation/diagnostics).
                total_duration_ms += float(total_node_ms)
                await websocket.send_json(
                    ExecutionEvent(
                        type="node_complete",
                        runId=run_id,
                        nodeId=active,
                        result=out0,
                        meta=metrics,
                    ).model_dump()
                )
                track["active_node_id"] = None
                track["active_duration_ms"] = 0.0

        return state

    while True:
        if gate is not None:
            await gate.wait()
        # Discover all descendant runs (root + children + grandchildren...).
        #
        # This is critical for nested subworkflow composition:
        # - Root run may wait on a Subflow child
        # - That child may itself wait on an Agent sub-run (grandchild)
        # If we only tick direct children of the root, the grandchild never progresses
        # and the session deadlocks (UI shows "running" but nothing is computing).
        run_ids: list[str] = _list_descendant_run_ids(runtime, root_run_id)
        if not run_ids:
            run_ids = [root_run_id]

        # Tick root first, then children.
        root_state = await _tick_run(root_run_id, is_root=True)
        # If root sent flow_waiting, stop the loop.
        if connection_id in _waiting_runners:
            break

        # Tick all children once per cycle (enough for UX; delays/events are handled in _tick_run).
        for rid in run_ids:
            if rid == root_run_id:
                continue
            try:
                if runtime is None:
                    st = None
                elif lock is not None:
                    async with lock:
                        st = runtime.get_state(rid)
                else:
                    st = runtime.get_state(rid)
                if st is None:
                    continue
                # Only drive active children; idle event listeners will stay waiting.
                if str(getattr(st, "status", None)) == "RunStatus.RUNNING" or (
                    str(getattr(st, "status", None)) == "RunStatus.WAITING"
                    and getattr(getattr(st, "waiting", None), "reason", None) is not None
                ):
                    await _tick_run(rid, is_root=False)
            except Exception:
                continue

        # Bubble async subworkflow completions back into any waiting parents (root or children).
        #
        # This is required for async+wait START_SUBWORKFLOW (Agent nodes): the parent stays in
        # WAITING(reason=SUBWORKFLOW) while the host drives the child run incrementally.
        try:
            if runtime is not None and getattr(runtime, "workflow_registry", None) is not None:
                from abstractruntime.core.models import RunStatus, WaitReason

                registry = runtime.workflow_registry

                def _spec_for(run_state: Any):
                    spec = registry.get(run_state.workflow_id)
                    if spec is None:
                        raise RuntimeError(f"Workflow '{run_state.workflow_id}' not found in registry")
                    return spec

                for parent_id in list(run_ids):
                    try:
                        parent_state = runtime.get_state(parent_id)
                    except Exception:
                        continue
                    if parent_state is None:
                        continue
                    if parent_state.status != RunStatus.WAITING or parent_state.waiting is None:
                        continue
                    if parent_state.waiting.reason != WaitReason.SUBWORKFLOW:
                        continue
                    sub_run_id = _extract_sub_run_id(parent_state.waiting)
                    if not sub_run_id:
                        continue

                    try:
                        sub_state = runtime.get_state(sub_run_id)
                    except Exception:
                        continue

                    if sub_state.status == RunStatus.COMPLETED:
                        runtime.resume(
                            workflow=_spec_for(parent_state),
                            run_id=parent_state.run_id,
                            wait_key=None,
                            payload={
                                "sub_run_id": sub_state.run_id,
                                "output": sub_state.output,
                                "node_traces": runtime.get_node_traces(sub_state.run_id),
                            },
                            max_steps=0,
                        )
                        continue

                    if sub_state.status == RunStatus.FAILED:
                        # Preserve the durable failure semantics of sync START_SUBWORKFLOW.
                        parent_state.status = RunStatus.FAILED
                        parent_state.waiting = None
                        parent_state.error = f"Subworkflow '{parent_state.workflow_id}' failed: {sub_state.error}"
                        parent_state.updated_at = _utc_now_iso()
                        runtime.run_store.save(parent_state)
                        continue

                    if sub_state.status == RunStatus.CANCELLED:
                        parent_state.status = RunStatus.CANCELLED
                        parent_state.waiting = None
                        parent_state.error = f"Subworkflow '{parent_state.workflow_id}' cancelled"
                        parent_state.updated_at = _utc_now_iso()
                        runtime.run_store.save(parent_state)
                        continue
        except Exception:
            # Best-effort; never let bubbling break the execution loop.
            pass

        # Check session completion:
        # - If root failed: error immediately (best-effort: include node id if available).
        try:
            if runtime is None:
                root_now = None
            elif lock is not None:
                async with lock:
                    root_now = runtime.get_state(root_run_id)
            else:
                root_now = runtime.get_state(root_run_id)
        except Exception:
            root_now = None

        if root_now is not None and str(getattr(root_now, "status", None)) == "RunStatus.FAILED":
            if connection_id in _waiting_runners:
                del _waiting_runners[connection_id]
            error_node = None
            if hasattr(root_now, "vars") and isinstance(root_now.vars, dict):
                error_node = root_now.vars.get("_flow_error_node")
            await websocket.send_json(
                ExecutionEvent(
                    type="flow_error",
                    runId=root_run_id,
                    nodeId=error_node,
                    error=getattr(root_now, "error", None),
                ).model_dump()
            )
            break

        if root_now is not None and str(getattr(root_now, "status", None)) == "RunStatus.CANCELLED":
            if connection_id in _waiting_runners:
                del _waiting_runners[connection_id]
            await websocket.send_json(
                ExecutionEvent(
                    type="flow_cancelled",
                    runId=root_run_id,
                    error=getattr(root_now, "error", None),
                ).model_dump()
            )
            break

        # If root completed, keep the websocket open until all children are either:
        # - completed/failed/cancelled, or
        # - waiting for EVENT (idle listeners).
        if root_now is not None and str(getattr(root_now, "status", None)) == "RunStatus.COMPLETED":
            all_children_idle_or_done = True
            try:
                for rid in run_ids:
                    if rid == root_run_id:
                        continue
                    st = runtime.get_state(rid) if runtime is not None else None
                    if st is None:
                        continue
                    s = str(getattr(st, "status", None))
                    if s in {"RunStatus.COMPLETED", "RunStatus.FAILED", "RunStatus.CANCELLED"}:
                        continue
                    if s == "RunStatus.WAITING":
                        w = getattr(st, "waiting", None)
                        reason = getattr(w, "reason", None) if w is not None else None
                        reason_value = reason.value if hasattr(reason, "value") else str(reason) if reason else None
                        if reason_value == "event":
                            continue
                    all_children_idle_or_done = False
            except Exception:
                all_children_idle_or_done = True

            if all_children_idle_or_done:
                # Cancel idle listeners waiting on EVENT so the session can end cleanly.
                try:
                    for rid in run_ids:
                        if rid == root_run_id:
                            continue
                        st = runtime.get_state(rid) if runtime is not None else None
                        if st is None:
                            continue
                        if str(getattr(st, "status", None)) != "RunStatus.WAITING":
                            continue
                        w = getattr(st, "waiting", None)
                        reason = getattr(w, "reason", None) if w is not None else None
                        reason_value = reason.value if hasattr(reason, "value") else str(reason) if reason else None
                        if reason_value != "event":
                            continue
                        try:
                            runtime.cancel_run(rid, reason="Session completed")  # type: ignore[union-attr]
                        except Exception:
                            pass
                except Exception:
                    pass

                flow_duration_ms = (time.perf_counter() - flow_started_at) * 1000.0
                flow_meta: Dict[str, Any] = {"duration_ms": round(float(flow_duration_ms), 2)}
                # Include aggregate token totals when available (best-effort).
                #
                # NOTE: We count tokens from durable ledger records (LLM_CALL completions), so this
                # remains correct across loops, subflows, and agent subruns.
                with _token_lock:
                    in_total = int(total_input_tokens)
                    out_total = int(total_output_tokens)
                if in_total > 0 or out_total > 0:
                    flow_meta["input_tokens"] = in_total
                    flow_meta["output_tokens"] = out_total
                    # Throughput (overall) is best-effort; use total wall time.
                    if flow_duration_ms > 0 and out_total > 0:
                        flow_meta["tokens_per_s"] = round(float(out_total) / (float(flow_duration_ms) / 1000.0), 2)
                await websocket.send_json(
                    ExecutionEvent(
                        type="flow_complete",
                        runId=root_run_id,
                        result=getattr(root_now, "output", None),
                        meta=flow_meta,
                    ).model_dump()
                )
                break

        # Avoid overwhelming the WebSocket / event loop.
        await asyncio.sleep(0.01)

    # Clean up ledger subscription (best-effort).
    if callable(unsubscribe_ledger):
        try:
            unsubscribe_ledger()
        except Exception:
            pass

    if ledger_trace_task is not None:
        ledger_trace_task.cancel()
        try:
            await ledger_trace_task
        except Exception:
            pass


async def resume_waiting_flow(
    websocket: WebSocket,
    connection_id: str,
    response: str,
) -> None:
    """Resume a waiting flow with the user's response."""
    if connection_id not in _waiting_runners:
        await websocket.send_json(
            ExecutionEvent(
                type="flow_error",
                error="No waiting flow to resume",
            ).model_dump()
        )
        return

    # Clear the waiting marker before resuming; `_execute_runner_loop()` uses the
    # presence of `connection_id` in `_waiting_runners` as a signal to stop the loop.
    runner = _waiting_runners.pop(connection_id)
    _active_runners[connection_id] = runner
    rid0 = getattr(runner, "run_id", None)
    if isinstance(rid0, str) and rid0:
        _active_run_ids[connection_id] = rid0

    gate = _control_gates.get(connection_id)
    if gate is not None:
        gate.set()
    lock = _connection_locks.get(connection_id)

    try:
        state = runner.get_state()
        wait = state.waiting if state else None
        reason = wait.reason.value if wait and hasattr(wait, "reason") else None
        waiting_ctx = _waiting_steps.get(connection_id) if isinstance(_waiting_steps.get(connection_id), dict) else {}
        waiting_node_id = (
            waiting_ctx.get("node_id")
            if isinstance(waiting_ctx, dict) and isinstance(waiting_ctx.get("node_id"), str)
            else getattr(state, "current_node", None)
        )
        started_at = waiting_ctx.get("started_at") if isinstance(waiting_ctx, dict) else None
        waited_ms = 0.0
        if isinstance(started_at, (int, float)):
            waited_ms = (time.perf_counter() - float(started_at)) * 1000.0

        async def _maybe_close_waiting_step() -> None:
            """Close the node that was WAITING if the run has resumed past it."""
            if not isinstance(waiting_node_id, str) or not waiting_node_id:
                return
            after = runner.get_state()
            after_node = getattr(after, "current_node", None) if after is not None else None
            if after_node == waiting_node_id:
                # Still on the same node (e.g. multi-tick Agent); keep it open.
                return

            try:
                from abstractflow.compiler import _sync_effect_results_to_node_outputs as _sync_effect_results  # type: ignore
                if after is not None and hasattr(runner, "flow"):
                    _sync_effect_results(after, runner.flow)
            except Exception:
                pass

            out = None
            try:
                if hasattr(runner, "flow") and hasattr(runner.flow, "_node_outputs"):
                    outputs = getattr(runner.flow, "_node_outputs")
                    if isinstance(outputs, dict):
                        out = outputs.get(waiting_node_id)
            except Exception:
                out = None

            # Emit completion so the UI doesn't keep this node permanently "WAITING".
            # Duration reflects time spent waiting for user input.
            await websocket.send_json(
                ExecutionEvent(
                    type="node_complete",
                    runId=getattr(runner, "run_id", None),
                    nodeId=waiting_node_id,
                    result=_json_safe(out),
                    meta={"duration_ms": round(float(waited_ms), 2)},
                ).model_dump()
            )
            _waiting_steps.pop(connection_id, None)

        if reason != "subworkflow":
            # Resume with the user's response
            if lock is not None:
                async with lock:
                    runner.resume(payload={"response": response}, max_steps=0)
            else:
                runner.resume(payload={"response": response}, max_steps=0)
            await _maybe_close_waiting_step()
            await _execute_runner_loop(websocket, runner, connection_id)
            return

        runtime = getattr(runner, "runtime", None)
        registry = getattr(runtime, "workflow_registry", None) if runtime else None
        if runtime is None or registry is None:
            await websocket.send_json(
                ExecutionEvent(
                    type="flow_error",
                    error="Subworkflow resume requires runner runtime + workflow registry",
                ).model_dump()
            )
            return

        # Resume the deepest waiting child, then bubble completion back up to the parent.
        from abstractruntime.core.models import RunStatus, WaitReason

        def _spec_for(run_state: Any):
            spec = registry.get(run_state.workflow_id)
            if spec is None:
                raise RuntimeError(f"Workflow '{run_state.workflow_id}' not found in registry")
            return spec

        def _extract_sub_run_id_from_wait(wait_state: Any) -> Optional[str]:
            details = getattr(wait_state, "details", None)
            if isinstance(details, dict):
                sub_run_id = details.get("sub_run_id")
                if isinstance(sub_run_id, str) and sub_run_id:
                    return sub_run_id
            wait_key = getattr(wait_state, "wait_key", None)
            if isinstance(wait_key, str) and wait_key.startswith("subworkflow:"):
                return wait_key.split("subworkflow:", 1)[1] or None
            return None

        top_run_id = runner.run_id
        if not top_run_id:
            raise RuntimeError("No active run_id for runner")

        # Find deepest waiting run in subworkflow chain.
        target_run_id = top_run_id
        for _ in range(25):
            current = runtime.get_state(target_run_id)
            if current.status != RunStatus.WAITING or current.waiting is None:
                break
            if current.waiting.reason != WaitReason.SUBWORKFLOW:
                break
            next_id = _extract_sub_run_id_from_wait(current.waiting)
            if not next_id:
                break
            target_run_id = next_id

        target_state = runtime.get_state(target_run_id)
        runtime.resume(
            workflow=_spec_for(target_state),
            run_id=target_run_id,
            wait_key=None,
            payload={"response": response},
            max_steps=0,
        )

        # Drive child runs until they wait again or complete, bubbling completion up.
        current_run_id = target_run_id
        for _ in range(50):
            current_state = runtime.get_state(current_run_id)
            if current_state.status == RunStatus.RUNNING:
                current_state = runtime.tick(workflow=_spec_for(current_state), run_id=current_run_id, max_steps=100)

            if current_state.status == RunStatus.WAITING:
                break

            if current_state.status == RunStatus.FAILED:
                raise RuntimeError(current_state.error or "Subworkflow failed")

            if current_state.status != RunStatus.COMPLETED:
                raise RuntimeError(f"Unexpected subworkflow status: {current_state.status.value}")

            parent_id = current_state.parent_run_id
            if not parent_id:
                break

            parent_state = runtime.get_state(parent_id)
            if parent_state.status != RunStatus.WAITING or parent_state.waiting is None:
                break
            if parent_state.waiting.reason != WaitReason.SUBWORKFLOW:
                break

            runtime.resume(
                workflow=_spec_for(parent_state),
                run_id=parent_id,
                wait_key=None,
                payload={
                    "sub_run_id": current_state.run_id,
                    "output": current_state.output,
                    "node_traces": runtime.get_node_traces(current_state.run_id),
                },
                max_steps=0,
            )

            if parent_id == top_run_id:
                break
            current_run_id = parent_id

        # Top-level may still be waiting (child asked again) or ready to run.
        if runner.is_waiting():
            top_state = runner.get_state()
            top_wait = top_state.waiting if top_state else None

            prompt = "Please respond:"
            choices: list = []
            allow_free_text = True
            wait_key = top_wait.wait_key if top_wait else None
            reason = top_wait.reason.value if top_wait and hasattr(top_wait, "reason") else None

            if top_wait and top_wait.reason == WaitReason.SUBWORKFLOW:
                sub_run_id = _extract_sub_run_id_from_wait(top_wait)
                current_run_id = sub_run_id
                for _ in range(25):
                    if not current_run_id:
                        break
                    sub_state = runtime.get_state(current_run_id)
                    sub_wait = sub_state.waiting
                    if sub_wait is None:
                        break
                    if sub_wait.reason == WaitReason.SUBWORKFLOW:
                        current_run_id = _extract_sub_run_id_from_wait(sub_wait)
                        continue
                    prompt = sub_wait.prompt or "Please respond:"
                    choices = list(sub_wait.choices) if isinstance(sub_wait.choices, list) else []
                    allow_free_text = bool(sub_wait.allow_free_text)
                    wait_key = sub_wait.wait_key
                    reason = sub_wait.reason.value
                    break
            elif top_wait is not None:
                prompt = top_wait.prompt or "Please respond:"
                choices = list(top_wait.choices) if isinstance(top_wait.choices, list) else []
                allow_free_text = bool(top_wait.allow_free_text)

            await websocket.send_json(
                {
                    "type": "flow_waiting",
                    "ts": _ws_utc_now_iso(),
                    "nodeId": top_state.current_node if top_state else None,
                    "wait_key": wait_key,
                    "reason": reason,
                    "prompt": prompt,
                    "choices": choices,
                    "allow_free_text": allow_free_text,
                }
            )
            # Keep runner resumable + record waiting-step context for a follow-up resume.
            _waiting_runners[connection_id] = runner
            _waiting_steps[connection_id] = {
                "node_id": top_state.current_node if top_state else None,
                "started_at": time.perf_counter(),
            }
            return

        await _maybe_close_waiting_step()
        await _execute_runner_loop(websocket, runner, connection_id)

    except Exception as e:
        import traceback
        traceback.print_exc()
        await websocket.send_json(
            ExecutionEvent(
                type="flow_error",
                error=str(e),
            ).model_dump()
        )

---
file: web/backend/routes/flows.py
---

"""Flow CRUD and execution routes."""

from __future__ import annotations

import json
import logging
import os
import re
from urllib.error import HTTPError
import urllib.request
import urllib.parse
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
import uuid

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

from ..models import (
    VisualFlow,
    FlowCreateRequest,
    FlowUpdateRequest,
    FlowRunRequest,
    FlowRunResult,
)
from ..services.executor import create_visual_runner, visual_to_flow
from ..services.execution_workspace import ensure_default_workspace_root, ensure_run_id_workspace_alias
from ..services.runtime_stores import get_runtime_stores
from abstractflow.visual.workspace_scoped_tools import WorkspaceScope, build_scoped_tool_executor
from abstractflow.visual.interfaces import apply_visual_flow_interface_scaffold
from abstractflow.workflow_bundle import pack_workflow_bundle
from abstractruntime.workflow_bundle import open_workflow_bundle

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/flows", tags=["flows"])

# File-based persistence
_FLOWS_DIR_ENV = os.getenv("ABSTRACTFLOW_FLOWS_DIR") or ""
FLOWS_DIR = (
    Path(_FLOWS_DIR_ENV).expanduser().resolve()
    if isinstance(_FLOWS_DIR_ENV, str) and _FLOWS_DIR_ENV.strip()
    else Path("./flows")
)
FLOWS_DIR.mkdir(parents=True, exist_ok=True)

def _default_publish_dir() -> Path:
    # Publish output is authored on the AbstractFlow machine. In gateway-first deployments,
    # the bundle should then be uploaded to the remote gateway (no shared filesystem required).
    raw = os.getenv("ABSTRACTFLOW_PUBLISH_DIR") or os.getenv("ABSTRACTFLOW_FLOWS_DIR") or ""
    if raw and str(raw).strip():
        return Path(raw).expanduser().resolve()
    # Default next to the local flow files so `save` + `publish` stay in one place.
    return (FLOWS_DIR / "bundles").resolve()


_BUNDLE_ID_SAFE_RE = re.compile(r"[^a-zA-Z0-9_-]+")


def _sanitize_bundle_id(raw: str) -> str:
    s = str(raw or "").strip()
    if not s:
        return ""
    s = _BUNDLE_ID_SAFE_RE.sub("-", s)
    s = re.sub(r"-{2,}", "-", s).strip("-")
    return s


def _try_parse_semver(v: str) -> Optional[tuple[int, int, int]]:
    s = str(v or "").strip()
    if not s:
        return None
    parts = [p.strip() for p in s.split(".")]
    if not parts or any(not p for p in parts):
        return None
    nums: list[int] = []
    for p in parts:
        if not p.isdigit():
            return None
        nums.append(int(p))
    while len(nums) < 3:
        nums.append(0)
    return (nums[0], nums[1], nums[2])


def _bump_patch(v: str) -> str:
    sem = _try_parse_semver(v)
    if sem is not None:
        return f"{sem[0]}.{sem[1]}.{sem[2] + 1}"
    s = str(v or "").strip()
    return f"{s}.1" if s else "0.0.1"


def _scan_published_bundle_versions(*, publish_dir: Path, bundle_id: str) -> list[dict[str, str]]:
    out: list[dict[str, str]] = []
    if not publish_dir.exists() or not publish_dir.is_dir():
        return out
    for p in sorted(publish_dir.glob("*.flow")):
        try:
            b = open_workflow_bundle(p)
            man = getattr(b, "manifest", None)
            bid = str(getattr(man, "bundle_id", "") or "").strip()
            if bid != bundle_id:
                continue
            out.append(
                {
                    "bundle_version": str(getattr(man, "bundle_version", "0.0.0") or "0.0.0").strip() or "0.0.0",
                    "created_at": str(getattr(man, "created_at", "") or ""),
                    "path": str(p),
                }
            )
        except Exception:
            continue
    return out


def _latest_version(versions: list[dict[str, str]]) -> Optional[str]:
    if not versions:
        return None
    vers = [str(v.get("bundle_version") or "").strip() for v in versions if str(v.get("bundle_version") or "").strip()]
    if not vers:
        return None
    if all(_try_parse_semver(v) is not None for v in vers):
        return max(vers, key=lambda x: _try_parse_semver(x) or (0, 0, 0))
    # fallback: created_at lexicographic (ISO), then version string
    return max(versions, key=lambda x: (str(x.get("created_at") or ""), str(x.get("bundle_version") or ""))).get("bundle_version")


def _origin_version(versions: list[dict[str, str]]) -> Optional[str]:
    if not versions:
        return None
    return min(versions, key=lambda x: (str(x.get("created_at") or ""), str(x.get("bundle_version") or ""))).get("bundle_version")


class PublishFlowRequest(BaseModel):
    bundle_id: Optional[str] = Field(default=None, description="Stable bundle identity (defaults to sanitized flow.name).")
    bundle_version: Optional[str] = Field(default=None, description="Explicit bundle_version. If omitted, auto-bump from existing published versions.")
    publish_dir: Optional[str] = Field(default=None, description="Override publish directory (defaults to repo flows/bundles or ABSTRACTFLOW_PUBLISH_DIR).")
    reload_gateway: bool = Field(default=True, description="If true, POST /api/gateway/bundles/reload after publishing (best-effort).")


class PublishFlowResponse(BaseModel):
    ok: bool
    bundle_id: str
    bundle_version: str
    bundle_ref: str
    bundle_path: str
    gateway_reloaded: bool = False
    gateway_reload_error: Optional[str] = None


class DeprecateBundleRequest(BaseModel):
    bundle_id: Optional[str] = Field(default=None, description="Bundle id (defaults to sanitized flow.name).")
    flow_id: Optional[str] = Field(default=None, description="Optional entrypoint flow_id (default: all entrypoints for the bundle).")
    reason: Optional[str] = Field(default=None, description="Optional reason to record.")


class DeprecateBundleResponse(BaseModel):
    ok: bool
    bundle_id: str
    flow_id: str
    deprecated_at: Optional[str] = None
    reason: Optional[str] = None
    removed: Optional[bool] = None


def _load_flows_from_disk() -> Dict[str, VisualFlow]:
    """Load all flows from disk on startup."""
    flows: Dict[str, VisualFlow] = {}
    for path in FLOWS_DIR.glob("*.json"):
        try:
            data = json.loads(path.read_text())
            flow = VisualFlow(**data)
            # Best-effort: keep interface-marked workflows scaffolded so the editor
            # always shows the expected pins (even for older files).
            try:
                for iid in list(getattr(flow, "interfaces", []) or []):
                    apply_visual_flow_interface_scaffold(flow, str(iid), include_recommended=True)
            except Exception:
                pass
            flows[flow.id] = flow
            logger.info(f"Loaded flow '{flow.name}' ({flow.id}) from {path}")
        except Exception as e:
            logger.warning(f"Failed to load flow from {path}: {e}")
    return flows


def _save_flow_to_disk(flow: VisualFlow) -> None:
    """Persist a single flow to disk."""
    path = FLOWS_DIR / f"{flow.id}.json"
    path.write_text(flow.model_dump_json(indent=2))
    logger.info(f"Saved flow '{flow.name}' ({flow.id}) to {path}")


def _delete_flow_from_disk(flow_id: str) -> None:
    """Remove a flow file from disk."""
    path = FLOWS_DIR / f"{flow_id}.json"
    if path.exists():
        path.unlink()
        logger.info(f"Deleted flow file {path}")


# Load existing flows from disk on module import
_flows: Dict[str, VisualFlow] = _load_flows_from_disk()


@router.get("", response_model=List[VisualFlow])
async def list_flows():
    """List all saved flows."""
    return list(_flows.values())


@router.post("", response_model=VisualFlow)
async def create_flow(request: FlowCreateRequest):
    """Create a new flow with nodes and edges."""
    now = datetime.utcnow().isoformat()
    flow = VisualFlow(
        id=str(uuid.uuid4())[:8],
        name=request.name,
        description=request.description,
        interfaces=list(request.interfaces or []),
        nodes=request.nodes,
        edges=request.edges,
        entryNode=request.entryNode,
        created_at=now,
        updated_at=now,
    )
    # If the flow declares interfaces, ensure required pins exist.
    try:
        for iid in list(getattr(flow, "interfaces", []) or []):
            apply_visual_flow_interface_scaffold(flow, str(iid), include_recommended=True)
    except Exception:
        pass
    _flows[flow.id] = flow
    _save_flow_to_disk(flow)  # Persist to disk
    return flow


@router.get("/{flow_id}", response_model=VisualFlow)
async def get_flow(flow_id: str):
    """Get a specific flow by ID."""
    if flow_id not in _flows:
        raise HTTPException(status_code=404, detail=f"Flow '{flow_id}' not found")
    return _flows[flow_id]


@router.put("/{flow_id}", response_model=VisualFlow)
async def update_flow(flow_id: str, request: FlowUpdateRequest):
    """Update an existing flow."""
    if flow_id not in _flows:
        raise HTTPException(status_code=404, detail=f"Flow '{flow_id}' not found")

    flow = _flows[flow_id]

    # Update fields if provided
    if request.name is not None:
        flow.name = request.name
    if request.description is not None:
        flow.description = request.description
    if request.interfaces is not None:
        flow.interfaces = list(request.interfaces or [])
    if request.nodes is not None:
        flow.nodes = request.nodes
    if request.edges is not None:
        flow.edges = request.edges
    if request.entryNode is not None:
        flow.entryNode = request.entryNode

    # Keep interface-marked flows scaffolded even if only nodes/edges changed.
    try:
        for iid in list(getattr(flow, "interfaces", []) or []):
            apply_visual_flow_interface_scaffold(flow, str(iid), include_recommended=True)
    except Exception:
        pass

    flow.updated_at = datetime.utcnow().isoformat()
    _flows[flow_id] = flow
    _save_flow_to_disk(flow)  # Persist to disk
    return flow


def _gateway_reload_url() -> str:
    api = _gateway_api_base()
    return f"{api}/gateway/bundles/reload"


def _gateway_upload_url() -> str:
    api = _gateway_api_base()
    return f"{api}/gateway/bundles/upload"


def _gateway_api_base() -> str:
    raw = str(os.getenv("ABSTRACTFLOW_GATEWAY_URL") or os.getenv("ABSTRACTGATEWAY_URL") or "http://127.0.0.1:8081").strip()
    raw = raw.rstrip("/")
    if raw.endswith("/api"):
        return raw
    if raw.endswith("/api/"):
        return raw.rstrip("/")
    return f"{raw}/api"


def _gateway_deprecate_url(bundle_id: str) -> str:
    api = _gateway_api_base()
    bid = urllib.parse.quote(str(bundle_id or "").strip(), safe="")
    return f"{api}/gateway/bundles/{bid}/deprecate"


def _gateway_undeprecate_url(bundle_id: str) -> str:
    api = _gateway_api_base()
    bid = urllib.parse.quote(str(bundle_id or "").strip(), safe="")
    return f"{api}/gateway/bundles/{bid}/undeprecate"


def _gateway_auth_token() -> str:
    # Prefer canonical gateway env var names (with legacy fallbacks).
    raw = os.getenv("ABSTRACTGATEWAY_AUTH_TOKEN") or os.getenv("ABSTRACTFLOW_GATEWAY_AUTH_TOKEN") or ""
    return str(raw or "").strip()


def _try_reload_gateway() -> Optional[str]:
    url = _gateway_reload_url()
    req = urllib.request.Request(url=url, method="POST")
    token = _gateway_auth_token()
    if token:
        req.add_header("Authorization", f"Bearer {token}")
    try:
        with urllib.request.urlopen(req, timeout=5) as resp:
            _ = resp.read()
        return None
    except Exception as e:
        return str(e)


def _encode_multipart_formdata(
    *,
    fields: Dict[str, str],
    file_field: str,
    filename: str,
    content: bytes,
    content_type: str,
) -> tuple[bytes, str]:
    boundary = uuid.uuid4().hex
    crlf = b"\r\n"
    body = bytearray()

    for k, v in (fields or {}).items():
        body.extend(b"--" + boundary.encode("ascii") + crlf)
        body.extend(f'Content-Disposition: form-data; name="{k}"'.encode("utf-8"))
        body.extend(crlf + crlf)
        body.extend(str(v).encode("utf-8"))
        body.extend(crlf)

    body.extend(b"--" + boundary.encode("ascii") + crlf)
    body.extend(f'Content-Disposition: form-data; name="{file_field}"; filename="{filename}"'.encode("utf-8"))
    body.extend(crlf)
    body.extend(f"Content-Type: {content_type}".encode("utf-8"))
    body.extend(crlf + crlf)
    body.extend(bytes(content or b""))
    body.extend(crlf)

    body.extend(b"--" + boundary.encode("ascii") + b"--" + crlf)
    return (bytes(body), f"multipart/form-data; boundary={boundary}")


def _try_upload_bundle_to_gateway(*, bundle_path: Path, overwrite: bool, reload: bool) -> tuple[Optional[dict[str, Any]], Optional[str]]:
    url = _gateway_upload_url()
    token = _gateway_auth_token()

    try:
        content = bundle_path.read_bytes()
    except Exception as e:
        return None, f"Failed to read bundle: {e}"

    body, content_type = _encode_multipart_formdata(
        fields={"overwrite": "true" if overwrite else "false", "reload": "true" if reload else "false"},
        file_field="file",
        filename=bundle_path.name,
        content=content,
        content_type="application/octet-stream",
    )

    req = urllib.request.Request(url=url, data=body, method="POST")
    req.add_header("Content-Type", content_type)
    req.add_header("Accept", "application/json")
    if token:
        req.add_header("Authorization", f"Bearer {token}")
    try:
        with urllib.request.urlopen(req, timeout=10) as resp:
            raw = resp.read().decode("utf-8")
        return (json.loads(raw) if raw else {}, None)
    except HTTPError as e:
        try:
            detail = e.read().decode("utf-8").strip()
        except Exception:
            detail = ""
        return None, detail or str(e)
    except Exception as e:
        return None, str(e)


def _try_update_gateway_deprecation(
    *,
    bundle_id: str,
    action: str,
    flow_id: Optional[str],
    reason: Optional[str],
) -> tuple[Optional[dict[str, Any]], Optional[str]]:
    bid = str(bundle_id or "").strip()
    if not bid:
        return None, "bundle_id is required"
    act = str(action or "").strip().lower()
    if act not in {"deprecate", "undeprecate"}:
        return None, "action must be deprecate|undeprecate"
    url = _gateway_deprecate_url(bid) if act == "deprecate" else _gateway_undeprecate_url(bid)
    token = _gateway_auth_token()

    payload: Dict[str, Any] = {}
    fid = str(flow_id or "").strip()
    if fid:
        payload["flow_id"] = fid
    rs = str(reason or "").strip()
    if rs and act == "deprecate":
        payload["reason"] = rs

    req = urllib.request.Request(url=url, data=json.dumps(payload).encode("utf-8"), method="POST")
    req.add_header("Content-Type", "application/json")
    req.add_header("Accept", "application/json")
    if token:
        req.add_header("Authorization", f"Bearer {token}")
    try:
        with urllib.request.urlopen(req, timeout=10) as resp:
            raw = resp.read().decode("utf-8")
        return (json.loads(raw) if raw else {}, None)
    except HTTPError as e:
        try:
            detail = e.read().decode("utf-8").strip()
        except Exception:
            detail = ""
        return None, detail or str(e)
    except Exception as e:
        return None, str(e)


@router.post("/{flow_id}/publish", response_model=PublishFlowResponse)
async def publish_flow(flow_id: str, request: PublishFlowRequest) -> PublishFlowResponse:
    """Pack and publish a `.flow` bundle for the specified VisualFlow.

    This mirrors `abstractflow bundle pack ...` but:
    - writes to a shared bundles directory (default: `./flows/bundles/`)
    - auto-bumps bundle_version to preserve older published bundles
    - adds lineage metadata into the bundle manifest
    """
    if flow_id not in _flows:
        raise HTTPException(status_code=404, detail=f"Flow '{flow_id}' not found")

    flow = _flows[flow_id]
    bundle_id = _sanitize_bundle_id(str(request.bundle_id or "").strip()) or _sanitize_bundle_id(str(flow.name or "").strip()) or str(flow.id)

    publish_dir = (
        Path(str(request.publish_dir)).expanduser().resolve()
        if isinstance(request.publish_dir, str) and str(request.publish_dir).strip()
        else _default_publish_dir()
    )
    try:
        publish_dir.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to create publish_dir '{publish_dir}': {e}")

    existing = _scan_published_bundle_versions(publish_dir=publish_dir, bundle_id=bundle_id)
    prev = _latest_version(existing)
    origin = _origin_version(existing) or prev

    requested_ver = str(request.bundle_version or "").strip() if isinstance(request.bundle_version, str) and str(request.bundle_version).strip() else ""
    if requested_ver:
        if any(str(x.get("bundle_version") or "").strip() == requested_ver for x in existing):
            raise HTTPException(status_code=400, detail=f"bundle_version '{requested_ver}' already exists for bundle '{bundle_id}'")
        new_ver = requested_ver
    else:
        new_ver = "0.0.0" if not prev else _bump_patch(prev)

    out_path = (publish_dir / f"{bundle_id}@{new_ver}.flow").resolve()
    if out_path.exists():
        raise HTTPException(status_code=400, detail=f"Output bundle already exists: {out_path}")

    root_path = (FLOWS_DIR / f"{flow.id}.json").resolve()
    if not root_path.exists():
        raise HTTPException(status_code=500, detail=f"Flow file not found on disk: {root_path}")

    published_at = datetime.utcnow().isoformat() + "Z"
    metadata: Dict[str, Any] = {
        "publisher": {"host": "abstractflow.web", "published_at": published_at},
        "source": {"root_flow_id": str(flow.id), "root_flow_name": str(flow.name or ""), "root_flow_updated_at": str(flow.updated_at or "")},
        "lineage": {
            "bundle_id": bundle_id,
            "bundle_version": new_ver,
            "origin_bundle_version": str(origin or new_ver),
            **({"previous_bundle_version": str(prev)} if prev else {}),
        },
    }

    try:
        pack_workflow_bundle(
            root_flow_json=root_path,
            out_path=out_path,
            bundle_id=bundle_id,
            bundle_version=new_ver,
            flows_dir=FLOWS_DIR,
            metadata=metadata,
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to publish bundle: {e}")

    gateway_reloaded = False
    gateway_reload_error = None
    if bool(request.reload_gateway):
        # Prefer the gateway upload endpoint (works for remote hosts). Fallback to reload for local dev.
        _resp, upload_err = _try_upload_bundle_to_gateway(bundle_path=out_path, overwrite=False, reload=True)
        if upload_err is None:
            gateway_reloaded = True
            gateway_reload_error = None
        else:
            reload_err = _try_reload_gateway()
            if reload_err is None:
                gateway_reloaded = True
                gateway_reload_error = None
            else:
                gateway_reloaded = False
                gateway_reload_error = f"upload: {upload_err}; reload: {reload_err}"

    return PublishFlowResponse(
        ok=True,
        bundle_id=bundle_id,
        bundle_version=new_ver,
        bundle_ref=f"{bundle_id}@{new_ver}",
        bundle_path=str(out_path),
        gateway_reloaded=bool(gateway_reloaded),
        gateway_reload_error=gateway_reload_error,
    )


@router.post("/{flow_id}/deprecate", response_model=DeprecateBundleResponse)
async def deprecate_flow_bundle(flow_id: str, request: DeprecateBundleRequest) -> DeprecateBundleResponse:
    if flow_id not in _flows:
        raise HTTPException(status_code=404, detail=f"Flow '{flow_id}' not found")
    flow = _flows[flow_id]
    bundle_id = _sanitize_bundle_id(str(request.bundle_id or "").strip()) or _sanitize_bundle_id(str(flow.name or "").strip()) or str(flow.id)

    resp, err = _try_update_gateway_deprecation(
        bundle_id=bundle_id,
        action="deprecate",
        flow_id=request.flow_id,
        reason=request.reason,
    )
    if err is not None:
        raise HTTPException(status_code=400, detail=f"Failed to deprecate on gateway: {err}")
    return DeprecateBundleResponse(
        ok=True,
        bundle_id=str(resp.get("bundle_id") or bundle_id),
        flow_id=str(resp.get("flow_id") or request.flow_id or "*"),
        deprecated_at=str(resp.get("deprecated_at") or "") or None,
        reason=str(resp.get("reason") or request.reason or "") or None,
    )


@router.post("/{flow_id}/undeprecate", response_model=DeprecateBundleResponse)
async def undeprecate_flow_bundle(flow_id: str, request: DeprecateBundleRequest) -> DeprecateBundleResponse:
    if flow_id not in _flows:
        raise HTTPException(status_code=404, detail=f"Flow '{flow_id}' not found")
    flow = _flows[flow_id]
    bundle_id = _sanitize_bundle_id(str(request.bundle_id or "").strip()) or _sanitize_bundle_id(str(flow.name or "").strip()) or str(flow.id)

    resp, err = _try_update_gateway_deprecation(
        bundle_id=bundle_id,
        action="undeprecate",
        flow_id=request.flow_id,
        reason=None,
    )
    if err is not None:
        raise HTTPException(status_code=400, detail=f"Failed to undeprecate on gateway: {err}")
    return DeprecateBundleResponse(
        ok=True,
        bundle_id=str(resp.get("bundle_id") or bundle_id),
        flow_id=str(resp.get("flow_id") or request.flow_id or "*"),
        removed=bool(resp.get("removed") is True),
    )


@router.delete("/{flow_id}")
async def delete_flow(flow_id: str):
    """Delete a flow."""
    if flow_id not in _flows:
        raise HTTPException(status_code=404, detail=f"Flow '{flow_id}' not found")
    del _flows[flow_id]
    _delete_flow_from_disk(flow_id)  # Remove from disk
    return {"status": "deleted", "id": flow_id}


@router.post("/{flow_id}/run", response_model=FlowRunResult)
async def run_flow(flow_id: str, request: FlowRunRequest):
    """Execute a flow and return the result."""
    if flow_id not in _flows:
        raise HTTPException(status_code=404, detail=f"Flow '{flow_id}' not found")

    visual_flow = _flows[flow_id]

    try:
        input_data = dict(request.input_data or {})
        session_id = None
        try:
            raw = input_data.get("session_id") or input_data.get("sessionId")
            if isinstance(raw, str) and raw.strip():
                session_id = raw.strip()
        except Exception:
            session_id = None
        workspace_dir = ensure_default_workspace_root(input_data)
        scope = WorkspaceScope.from_input_data(input_data)
        tool_executor = build_scoped_tool_executor(scope=scope) if scope is not None else None

        run_store, ledger_store, artifact_store = get_runtime_stores()
        runner = create_visual_runner(
            visual_flow,
            flows=_flows,
            run_store=run_store,
            ledger_store=ledger_store,
            artifact_store=artifact_store,
            tool_executor=tool_executor,
            input_data=input_data,
        )
        result = runner.run(input_data, session_id=session_id)
        if workspace_dir is not None and isinstance(runner.run_id, str) and runner.run_id.strip():
            ensure_run_id_workspace_alias(run_id=runner.run_id.strip(), workspace_dir=workspace_dir)

        if isinstance(result, dict) and result.get("waiting"):
            state = runner.get_state()
            wait = state.waiting if state else None
            payload = {
                "success": False,
                "waiting": True,
                "error": "Flow is waiting for input. Use WebSocket (/api/ws/{flow_id}) to resume.",
                "run_id": runner.run_id,
                "wait_key": wait.wait_key if wait else None,
                "prompt": wait.prompt if wait else None,
                "choices": list(wait.choices) if wait and isinstance(wait.choices, list) else [],
                "allow_free_text": bool(wait.allow_free_text) if wait else None,
            }
        elif isinstance(result, dict):
            payload = {
                "success": bool(result.get("success", True)),
                "waiting": False,
                "result": result.get("result"),
                "error": result.get("error"),
                "run_id": runner.run_id,
            }
        else:
            payload = {"success": True, "waiting": False, "result": result, "run_id": runner.run_id}

        return FlowRunResult(
            success=bool(payload.get("success", False)),
            result=payload.get("result"),
            error=payload.get("error"),
            run_id=payload.get("run_id"),
            waiting=bool(payload.get("waiting", False)),
            wait_key=payload.get("wait_key"),
            prompt=payload.get("prompt"),
            choices=payload.get("choices"),
            allow_free_text=payload.get("allow_free_text"),
        )
    except Exception as e:
        return FlowRunResult(
            success=False,
            error=str(e),
        )


@router.post("/{flow_id}/validate")
async def validate_flow(flow_id: str):
    """Validate a flow without executing it."""
    if flow_id not in _flows:
        raise HTTPException(status_code=404, detail=f"Flow '{flow_id}' not found")

    visual_flow = _flows[flow_id]

    try:
        flow = visual_to_flow(visual_flow)
        errors = flow.validate()
        return {
            "valid": len(errors) == 0,
            "errors": errors,
        }
    except Exception as e:
        return {
            "valid": False,
            "errors": [str(e)],
        }

---
file: web/backend/routes/tools.py
---

"""Tool discovery endpoints for the visual editor.

The visual Agent node needs to present an allowlist of tools. Per the durable
execution design, tool availability is a host/runtime concern, so we source the
defaults from AbstractRuntime's AbstractCore integration.
"""

from __future__ import annotations

from typing import Any, Dict, List

from fastapi import APIRouter, HTTPException

router = APIRouter(tags=["tools"])


def _tool_spec_from_callable(func: Any) -> Dict[str, Any]:
    tool_def = getattr(func, "_tool_definition", None)
    if tool_def is not None and hasattr(tool_def, "to_dict"):
        return dict(tool_def.to_dict())
    from abstractcore.tools.core import ToolDefinition

    return dict(ToolDefinition.from_function(func).to_dict())


@router.get("/tools")
async def list_tools() -> List[Dict[str, Any]]:
    """List available tools (ToolSpec dicts)."""
    try:
        from abstractruntime.integrations.abstractcore.default_tools import list_default_tool_specs
    except ImportError:
        raise HTTPException(
            status_code=500,
            detail="AbstractRuntime not installed. Run: pip install abstractruntime",
        )

    try:
        specs = list_default_tool_specs()
        if not isinstance(specs, list):
            specs = []

        # AbstractCore common tools that are safe but not yet included in AbstractRuntime defaults.
        # Keep discovery aligned with the host executor (`abstractflow.visual.workspace_scoped_tools`).
        try:
            from abstractcore.tools.common_tools import skim_url, skim_websearch

            seen = {str(s.get("name") or "") for s in specs if isinstance(s, dict)}
            for tool_fn in [skim_url, skim_websearch]:
                if not callable(tool_fn):
                    continue
                d = _tool_spec_from_callable(tool_fn)
                name = str(d.get("name") or "").strip()
                if not name or name in seen:
                    continue
                seen.add(name)
                d.setdefault("toolset", "web")
                specs.append(d)
        except Exception:
            pass

        # Add schema-only runtime tools used by agents (no external callable).
        # These are executed as runtime effects by AbstractAgent adapters.
        try:
            from abstractagent.logic.builtins import (  # type: ignore
                ASK_USER_TOOL,
                COMPACT_MEMORY_TOOL,
                DELEGATE_AGENT_TOOL,
                INSPECT_VARS_TOOL,
                RECALL_MEMORY_TOOL,
                REMEMBER_TOOL,
            )
        except Exception:
            return specs

        builtin_defs = [
            (ASK_USER_TOOL, "system", ["builtin", "hitl"]),
            (RECALL_MEMORY_TOOL, "memory", ["builtin", "memory"]),
            (INSPECT_VARS_TOOL, "memory", ["builtin", "debug"]),
            (REMEMBER_TOOL, "memory", ["builtin", "memory"]),
            (COMPACT_MEMORY_TOOL, "memory", ["builtin", "memory"]),
            (DELEGATE_AGENT_TOOL, "system", ["builtin", "agent"]),
        ]

        seen = {str(s.get("name") or "") for s in specs if isinstance(s, dict)}
        for tool_def, toolset, tags in builtin_defs:
            name = getattr(tool_def, "name", None)
            if not isinstance(name, str) or not name.strip():
                continue
            if name in seen:
                continue
            seen.add(name)
            d = tool_def.to_dict()
            if isinstance(d, dict):
                d.setdefault("toolset", toolset)
                d.setdefault("tags", list(tags))
                specs.append(d)

        return specs
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list tools: {e}")

---
file: web/backend/services/paths.py
---

from __future__ import annotations

import os
from pathlib import Path


def _is_repo_checkout() -> bool:
    """Best-effort detection for running from a source checkout.

    In this repository layout:
    - this file lives at `web/backend/services/paths.py`
    - repo root contains `pyproject.toml` and `web/backend/`
    """
    here = Path(__file__).resolve()
    repo_root = here.parents[3]
    return bool((repo_root / "pyproject.toml").is_file() and (repo_root / "web" / "backend").is_dir())


def default_runtime_dir() -> Path:
    """Default on-disk runtime directory (runs/ledger/artifacts and small backend configs).

    - Source checkout: `<repo>/web/runtime` (keeps dev artifacts local to the repo)
    - Installed package: `~/.abstractflow/runtime` (user-writable)
    """
    here = Path(__file__).resolve()
    if _is_repo_checkout():
        return (here.parents[2] / "runtime").resolve()
    return (Path.home() / ".abstractflow" / "runtime").expanduser().resolve()


def resolve_runtime_dir() -> Path:
    """Resolve the runtime directory (env override + mkdir)."""
    raw = os.getenv("ABSTRACTFLOW_RUNTIME_DIR") or ""
    p = Path(str(raw)).expanduser() if str(raw).strip() else default_runtime_dir()
    p = p.resolve()
    p.mkdir(parents=True, exist_ok=True)
    return p

---
file: web/backend/services/execution_workspace.py
---

from __future__ import annotations

import os
import tempfile
import uuid
from pathlib import Path
from typing import Any, Dict, Optional


def _resolve_no_strict(path: Path) -> Path:
    try:
        return path.expanduser().resolve(strict=False)
    except TypeError:  # pragma: no cover (older python)
        return path.expanduser().resolve()


def resolve_base_execution_dir() -> Path:
    """Return the base directory for per-run execution workspaces.

    Priority:
    - `ABSTRACTFLOW_BASE_EXECUTION` env var, if set
    - /tmp (if present)
    - OS temp directory
    """
    env = os.getenv("ABSTRACTFLOW_BASE_EXECUTION")
    raw = env.strip() if isinstance(env, str) else ""
    base = Path(raw) if raw else (Path("/tmp") if Path("/tmp").exists() else Path(tempfile.gettempdir()))
    base = _resolve_no_strict(base)
    if base.exists() and not base.is_dir():
        raise ValueError(f"ABSTRACTFLOW_BASE_EXECUTION must be a directory (got file): {base}")
    base.mkdir(parents=True, exist_ok=True)
    return base


def ensure_default_workspace_root(
    input_data: Dict[str, Any],
    *,
    key: str = "workspace_root",
    base_dir: Optional[Path] = None,
) -> Optional[Path]:
    """Ensure `input_data[key]` points to a per-run workspace directory.

    If the key is already provided by the caller, this is a no-op and returns None.
    """
    raw = input_data.get(key)
    if isinstance(raw, str) and raw.strip():
        return None

    base = base_dir or resolve_base_execution_dir()
    # Keep the real workspace hidden to avoid cluttering /tmp with many folders.
    # A user-friendly alias (base/<run_id>) is created once we know the run_id.
    workspace_dir = base / ".abstractflow" / "runs" / uuid.uuid4().hex
    workspace_dir.mkdir(parents=True, exist_ok=True)
    input_data[key] = str(workspace_dir)
    return workspace_dir


def ensure_run_id_workspace_alias(*, run_id: str, workspace_dir: Path, base_dir: Optional[Path] = None) -> Optional[Path]:
    """Create a stable alias at `<base>/<run_id>` pointing to the workspace directory.

    This makes it easy to locate artifacts by run id while keeping the actual workspace path
    stable (the tool executor is constructed before `run_id` is known).
    """
    rid = str(run_id or "").strip()
    if not rid:
        return None

    base = base_dir or resolve_base_execution_dir()
    alias = base / rid
    alias = _resolve_no_strict(alias)

    # If already present (dir or symlink), keep it.
    try:
        if alias.exists() or alias.is_symlink():
            return alias
    except Exception:
        pass

    try:
        alias.symlink_to(workspace_dir, target_is_directory=True)
        return alias
    except Exception:
        # Fallback (e.g. platforms without symlink privileges): create a small pointer.
        try:
            alias.mkdir(parents=True, exist_ok=True)
            (alias / "WORKSPACE_POINTER.txt").write_text(
                f"This run's workspace is stored at:\n{workspace_dir}\n",
                encoding="utf-8",
            )
            return alias
        except Exception:
            return None

---
file: web/backend/main.py
---

"""AbstractFlow Visual Editor - FastAPI Application."""

from __future__ import annotations

import os
from pathlib import Path

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, HTMLResponse

from .routes import (
    connection_router,
    flows_router,
    gateway_metrics_router,
    memory_kg_router,
    providers_router,
    runs_router,
    semantics_router,
    tools_router,
    ui_config_router,
    ws_router,
)
from .services.gateway_connection import bootstrap_gateway_connection_env

# Best-effort bootstrap so the backend can call the gateway without requiring a restart after UI config.
bootstrap_gateway_connection_env()

# Create FastAPI app
app = FastAPI(
    title="AbstractFlow Visual Editor",
    description="Blueprint-style visual workflow editor for AbstractFlow",
    version="0.1.0",
)

# Configure CORS for development.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify exact origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)

# Include routers
app.include_router(flows_router, prefix="/api")
app.include_router(gateway_metrics_router, prefix="/api")
app.include_router(connection_router, prefix="/api")
app.include_router(providers_router, prefix="/api")
app.include_router(runs_router, prefix="/api")
app.include_router(semantics_router, prefix="/api")
app.include_router(memory_kg_router, prefix="/api")
app.include_router(tools_router, prefix="/api")
app.include_router(ui_config_router, prefix="/api")
app.include_router(ws_router, prefix="/api")

@app.get("/api/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "service": "abstractflow-visual-editor"}


# Serve static frontend files (in production)
# IMPORTANT: These routes must be defined AFTER all /api/* routes
FRONTEND_DIR = Path(__file__).parent.parent / "frontend" / "dist"
if FRONTEND_DIR.exists():
    app.mount("/assets", StaticFiles(directory=FRONTEND_DIR / "assets"), name="assets")

    def _monitor_gpu_enabled() -> bool:
        raw = str(os.getenv("ABSTRACTFLOW_MONITOR_GPU") or os.getenv("ABSTRACT_MONITOR_GPU") or "").strip().lower()
        return raw in {"1", "true", "yes", "on"}

    def _inject_ui_config(html: str) -> str:
        if "window.__ABSTRACT_UI_CONFIG__" in html:
            return html
        snippet = (
            "<script>"
            "window.__ABSTRACT_UI_CONFIG__=Object.assign(window.__ABSTRACT_UI_CONFIG__||{}, { monitor_gpu: true });"
            "</script>"
        )
        if "</head>" in html:
            return html.replace("</head>", f"{snippet}\n</head>")
        if "</body>" in html:
            return html.replace("</body>", f"{snippet}\n</body>")
        return f"{html}\n{snippet}\n"

    def _serve_index():
      index_path = FRONTEND_DIR / "index.html"
      if not _monitor_gpu_enabled():
        return FileResponse(index_path)
      html = index_path.read_text(encoding="utf-8")
      return HTMLResponse(content=_inject_ui_config(html))

    @app.get("/")
    async def serve_frontend():
        """Serve the frontend SPA."""
        return _serve_index()

    @app.get("/{path:path}")
    async def serve_frontend_fallback(path: str):
        """Fallback to index.html for SPA routing (excluding API routes)."""
        # API routes are handled by the routers above
        if path.startswith("api/"):
            from fastapi import HTTPException
            raise HTTPException(status_code=404, detail="API endpoint not found")
        file_path = FRONTEND_DIR / path
        if file_path.exists() and file_path.is_file():
            if file_path.name == "index.html":
                return _serve_index()
            return FileResponse(file_path)
        return _serve_index()


if __name__ == "__main__":
    import uvicorn
    import sys

    if "--monitor-gpu" in sys.argv:
        os.environ["ABSTRACTFLOW_MONITOR_GPU"] = "1"

    uvicorn.run(
        "backend.main:app",
        host="0.0.0.0",
        port=8080,
        reload=True,
    )

---
file: docs/cli.md
---

# CLI (`abstractflow`)

AbstractFlow ships a small CLI focused on:
- **WorkflowBundle** (`.flow`) utilities
- running the **Visual Editor backend** (optional; requires `abstractflow[server]`)

Entry point:
- `abstractflow` (declared in `pyproject.toml` → `project.scripts`)
- implementation: `abstractflow/cli.py`

See also: [../README.md](../README.md), [getting-started.md](getting-started.md), [faq.md](faq.md), [visualflow.md](visualflow.md), [architecture.md](architecture.md).

## WorkflowBundle (.flow)

A `.flow` file is a zip bundle containing:
- `manifest.json`
- `flows/<flow_id>.json` (one or more VisualFlow JSON documents)

Bundling semantics are shared with AbstractRuntime:
- AbstractFlow CLI uses `abstractruntime.workflow_bundle` under the hood.
- Evidence: [../abstractflow/workflow_bundle.py](../abstractflow/workflow_bundle.py), [../abstractflow/cli.py](../abstractflow/cli.py).

## Commands

Pack a bundle from a root VisualFlow JSON (includes referenced subflows as determined by the AbstractRuntime packer):

```bash
abstractflow bundle pack web/flows/ac-echo.json --out /tmp/ac-echo.flow
```

Common options (see `abstractflow bundle pack --help`):
- `--flows-dir <dir>`: where to find `<flow_id>.json` files (defaults to the root file’s directory)
- `--bundle-id <id>`, `--bundle-version <x.y.z>`
- `--entrypoint <flow_id>` (repeatable)

Inspect a bundle manifest:

```bash
abstractflow bundle inspect /tmp/ac-echo.flow
```

Unpack to a directory:

```bash
abstractflow bundle unpack /tmp/ac-echo.flow --dir /tmp/ac-echo
```

Evidence:
- Delegation to AbstractRuntime: [../abstractflow/workflow_bundle.py](../abstractflow/workflow_bundle.py)
- CLI implementation: [../abstractflow/cli.py](../abstractflow/cli.py)
- Tests: [../tests/test_workflow_bundle_pack.py](../tests/test_workflow_bundle_pack.py)

## Serve (Visual Editor backend)

Run the FastAPI backend used by the visual editor UI:

```bash
pip install "abstractflow[editor]"  # or: abstractflow[server]
abstractflow serve --reload --port 8080
```

Notes:
- This starts the backend API on `/api/*` (health: `/api/health`).
- The UI can be served via `npx @abstractframework/flow` (see [web-editor.md](web-editor.md)).

Gateway-related flags (optional):
- `--gateway-url http://127.0.0.1:8081`
- `--gateway-token <token>`

Evidence: [../abstractflow/cli.py](../abstractflow/cli.py), [../web/backend/cli.py](../web/backend/cli.py), [../web/backend/main.py](../web/backend/main.py).

---
file: abstractflow/cli.py
---

"""Command-line interface for AbstractFlow.

Current implemented features:
- WorkflowBundle (.flow) pack/inspect/unpack (backlog 314)

Other commands are intentionally kept minimal for now.
"""

from __future__ import annotations

import argparse
import json
import os
import sys
from typing import List, Optional

from .workflow_bundle import inspect_workflow_bundle, pack_workflow_bundle, unpack_workflow_bundle
from abstractruntime.workflow_bundle import workflow_bundle_manifest_to_dict


def _build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="abstractflow", add_help=True)
    sub = p.add_subparsers(dest="command")

    bundle = sub.add_parser("bundle", help="WorkflowBundle (.flow) tools")
    bundle_sub = bundle.add_subparsers(dest="bundle_cmd")

    pack = bundle_sub.add_parser("pack", help="Pack a .flow bundle from a root VisualFlow JSON file")
    pack.add_argument("root", help="Path to root VisualFlow JSON (e.g., ./flows/<id>.json)")
    pack.add_argument("--out", required=True, help="Output .flow path")
    pack.add_argument("--bundle-id", default=None, help="Bundle id (default: root flow id)")
    pack.add_argument("--bundle-version", default="0.0.0", help="Bundle version (default: 0.0.0)")
    pack.add_argument("--flows-dir", default=None, help="Directory containing flow JSON files (default: root's directory)")
    pack.add_argument(
        "--entrypoint",
        action="append",
        default=None,
        help="Entrypoint flow id (repeatable). Default: root flow id",
    )

    insp = bundle_sub.add_parser("inspect", help="Print bundle manifest (JSON)")
    insp.add_argument("bundle", help="Path to .flow (zip) or extracted directory")

    unpack = bundle_sub.add_parser("unpack", help="Extract a .flow bundle to a directory")
    unpack.add_argument("bundle", help="Path to .flow (zip) or extracted directory")
    unpack.add_argument("--dir", required=True, help="Output directory")

    serve = sub.add_parser("serve", help="Run the Visual Editor backend (FastAPI)")
    serve.add_argument("--host", default=os.getenv("HOST", "0.0.0.0"))
    serve.add_argument("--port", type=int, default=int(os.getenv("PORT", "8080")))
    serve.add_argument("--reload", action="store_true", help="Enable auto-reload (dev)")
    serve.add_argument("--log-level", default=os.getenv("LOG_LEVEL", "info"))
    serve.add_argument("--monitor-gpu", action="store_true", help="Show the small GPU widget in the UI")
    serve.add_argument(
        "--gateway-url",
        default=os.getenv("ABSTRACTFLOW_GATEWAY_URL") or os.getenv("ABSTRACTGATEWAY_URL") or "",
    )
    serve.add_argument(
        "--gateway-token",
        default=os.getenv("ABSTRACTFLOW_GATEWAY_AUTH_TOKEN")
        or os.getenv("ABSTRACTGATEWAY_AUTH_TOKEN")
        or os.getenv("ABSTRACTCODE_GATEWAY_TOKEN")
        or "",
    )

    return p


def main(args: Optional[List[str]] = None) -> int:
    if args is None:
        args = sys.argv[1:]

    parser = _build_parser()
    ns = parser.parse_args(args)

    if ns.command == "bundle":
        if ns.bundle_cmd == "pack":
            packed = pack_workflow_bundle(
                root_flow_json=ns.root,
                out_path=ns.out,
                bundle_id=ns.bundle_id,
                bundle_version=ns.bundle_version,
                flows_dir=ns.flows_dir,
                entrypoints=list(ns.entrypoint) if isinstance(ns.entrypoint, list) and ns.entrypoint else None,
            )
            sys.stdout.write(str(packed.path) + "\n")
            return 0

        if ns.bundle_cmd == "inspect":
            man = inspect_workflow_bundle(bundle_path=ns.bundle)
            sys.stdout.write(json.dumps(workflow_bundle_manifest_to_dict(man), indent=2, ensure_ascii=False) + "\n")
            return 0

        if ns.bundle_cmd == "unpack":
            out = unpack_workflow_bundle(bundle_path=ns.bundle, out_dir=ns.dir)
            sys.stdout.write(str(out) + "\n")
            return 0

        parser.error("Missing bundle subcommand (pack|inspect|unpack)")

    if ns.command == "serve":
        try:
            import uvicorn  # type: ignore
        except Exception:
            sys.stderr.write(
                "Server dependencies are not installed.\n"
                "Install with: pip install \"abstractflow[server]\"\n"
            )
            return 2

        # Validate backend import early so we can give a clear error message.
        try:
            import backend.main  # noqa: F401
        except Exception as e:
            sys.stderr.write(
                "Failed to import the Visual Editor backend.\n"
                f"Error: {e}\n"
                "Install with: pip install \"abstractflow[server]\"\n"
            )
            return 2

        if bool(getattr(ns, "monitor_gpu", False)):
            os.environ["ABSTRACTFLOW_MONITOR_GPU"] = "1"

        gateway_url = str(getattr(ns, "gateway_url", "") or "").strip()
        gateway_token = str(getattr(ns, "gateway_token", "") or "").strip()
        if gateway_url:
            os.environ.setdefault("ABSTRACTFLOW_GATEWAY_URL", gateway_url)
            os.environ.setdefault("ABSTRACTGATEWAY_URL", gateway_url)
        if gateway_token:
            os.environ.setdefault("ABSTRACTGATEWAY_AUTH_TOKEN", gateway_token)
            os.environ.setdefault("ABSTRACTFLOW_GATEWAY_AUTH_TOKEN", gateway_token)

        uvicorn.run(
            "backend.main:app",
            host=str(getattr(ns, "host", "0.0.0.0")),
            port=int(getattr(ns, "port", 8080)),
            reload=bool(getattr(ns, "reload", False)),
            log_level=str(getattr(ns, "log_level", "info")),
        )
        return 0

    parser.print_help()
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

---
file: pyproject.toml
---

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "abstractflow"
version = "0.3.7"
description = "Diagram-based AI workflow generation built on AbstractCore"
readme = "README.md"
license = "MIT"
authors = [
    {name = "AbstractFlow Team", email = "contact@abstractflow.ai"}
]
maintainers = [
    {name = "AbstractFlow Team", email = "contact@abstractflow.ai"}
]
keywords = [
    "ai", "workflow", "diagram", "llm", "automation", 
    "visual-programming", "abstractcore", "machine-learning"
]
classifiers = [
    "Development Status :: 2 - Pre-Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: System :: Distributed Computing",
]
requires-python = ">=3.10"
dependencies = [
    # AbstractFlow is built on AbstractRuntime (durable execution kernel).
    # `abstractflow/__init__.py` imports runtime-backed components (FlowRunner/compile_flow),
    # so this dependency is required (not optional).
    "AbstractRuntime>=0.4.0",
    # Keep this aligned with the latest known published AbstractCore versions
    # (monorepo may carry a newer dev version).
    "abstractcore[tools]>=2.11.8",
    "pydantic>=2.0.0",
    "typing-extensions>=4.0.0",
]

[project.optional-dependencies]
agent = [
    # Enables built-in Agent node creation in the VisualFlow executor.
    "abstractagent>=0.2.0",
]
editor = [
    # Main end-user install for the Visual Editor (backend + Agent node support).
    "abstractagent>=0.3.1",
    "fastapi>=0.100.0",
    "uvicorn[standard]>=0.23.0",
    "websockets>=11.0.0",
]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "flake8>=6.0.0",
    "mypy>=1.0.0",
    "pre-commit>=3.0.0",
]
server = [
    "fastapi>=0.100.0",
    "uvicorn[standard]>=0.23.0",
    "websockets>=11.0.0",
]
ui = [
    "streamlit>=1.28.0",
    "plotly>=5.15.0",
    "networkx>=3.1.0",
]
all = [
    "abstractflow[agent,dev,server,ui]"
]

[project.urls]
Homepage = "https://github.com/lpalbou/AbstractFlow"
Documentation = "https://abstractflow.readthedocs.io"
Repository = "https://github.com/lpalbou/AbstractFlow"
"Bug Tracker" = "https://github.com/lpalbou/AbstractFlow/issues"
Changelog = "https://github.com/lpalbou/AbstractFlow/blob/main/CHANGELOG.md"

[project.scripts]
abstractflow = "abstractflow.cli:main"
abstractflow-backend = "backend.cli:main"

[tool.setuptools.packages.find]
where = [".", "web"]
include = ["abstractflow*", "backend*"]
exclude = ["tests*", "docs*", "examples*"]

[tool.setuptools.package-data]
abstractflow = ["py.typed", "*.json", "*.yaml", "*.yml"]

[tool.black]
line-length = 88
target-version = ['py310']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88
known_first_party = ["abstractflow"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--verbose",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
]

---
file: web/frontend/package.json
---

{
  "name": "@abstractframework/flow",
  "version": "0.1.6",
  "description": "Visual workflow editor for AbstractFramework - drag-and-drop workflow authoring with real-time testing",
  "type": "module",
  "author": "Laurent-Philippe Albou",
  "license": "MIT",
  "keywords": [
    "abstractframework",
    "workflow",
    "visual-editor",
    "drag-and-drop",
    "react"
  ],
  "repository": {
    "type": "git",
    "url": "https://github.com/lpalbou/abstractflow.git",
    "directory": "web/frontend"
  },
  "homepage": "https://github.com/lpalbou/abstractflow#readme",
  "publishConfig": {
    "access": "public"
  },
  "bin": {
    "abstractflow-editor": "./bin/cli.js"
  },
  "files": [
    "dist",
    "bin",
    "README.md"
  ],
  "scripts": {
    "dev": "vite --host --port 3003",
    "build": "tsc && vite build",
    "preview": "vite preview",
    "start": "node ./bin/cli.js",
    "lint": "eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0",
    "prepublishOnly": "npm run build"
  },
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "reactflow": "^11.10.4",
    "@monaco-editor/react": "^4.6.0",
    "@tanstack/react-query": "^5.17.0",
    "zustand": "^4.4.7",
    "react-hot-toast": "^2.4.1",
    "clsx": "^2.1.0",
    "dompurify": "3.2.7",
    "marked": "14.0.0"
  },
  "devDependencies": {
    "@types/react": "^18.2.48",
    "@types/react-dom": "^18.2.18",
    "@typescript-eslint/eslint-plugin": "^8.54.0",
    "@typescript-eslint/parser": "^8.54.0",
    "@vitejs/plugin-react": "^4.7.0",
    "eslint": "^9.39.2",
    "eslint-plugin-react-hooks": "^7.0.1",
    "eslint-plugin-react-refresh": "^0.5.0",
    "typescript": "^5.3.3",
    "vite": "^6.4.1"
  }
}

---
file: tests/test_runner.py
---

"""Tests for FlowRunner."""

from __future__ import annotations

import pytest

from abstractflow import Flow, FlowRunner


class TestFlowRunnerBasic:
    """Basic tests for FlowRunner."""

    def test_create_runner(self):
        """Test creating a FlowRunner."""
        flow = Flow("test")
        flow.add_node("start", lambda x: x)
        flow.set_entry("start")

        runner = FlowRunner(flow)

        assert runner.flow is flow
        assert runner.workflow.workflow_id == "test"
        assert runner.run_id is None

    def test_runner_repr(self):
        """Test FlowRunner __repr__."""
        flow = Flow("test")
        flow.add_node("start", lambda x: x)
        flow.set_entry("start")

        runner = FlowRunner(flow)
        repr_str = repr(runner)

        assert "test" in repr_str
        assert "not started" in repr_str


class TestFlowExecution:
    """Tests for flow execution."""

    def test_run_single_function_flow(self):
        """Test running a flow with a single function node."""
        flow = Flow("single")
        flow.add_node("double", lambda x: x * 2, input_key="value")
        flow.set_entry("double")

        runner = FlowRunner(flow)
        result = runner.run({"value": 21})

        assert result["success"] is True
        assert result["result"] == 42

    def test_run_linear_function_flow(self):
        """Test running a linear flow with multiple function nodes."""
        flow = Flow("linear")
        flow.add_node("double", lambda x: x * 2, input_key="value", output_key="doubled")
        flow.add_node("add_ten", lambda x: x + 10, input_key="doubled", output_key="final")
        flow.add_edge("double", "add_ten")
        flow.set_entry("double")

        runner = FlowRunner(flow)
        result = runner.run({"value": 5})

        # (5 * 2) + 10 = 20
        assert result["success"] is True
        assert result["result"] == 20

    def test_run_flow_with_dict_transformation(self):
        """Test a flow that transforms dictionaries."""
        def extract_name(data):
            return data.get("user", {}).get("name", "unknown")

        def greet(name):
            return f"Hello, {name}!"

        flow = Flow("greet")
        flow.add_node("extract", extract_name, output_key="name")
        flow.add_node("greet", greet, input_key="name")
        flow.add_edge("extract", "greet")
        flow.set_entry("extract")

        runner = FlowRunner(flow)
        result = runner.run({"user": {"name": "Alice"}})

        assert result["success"] is True
        assert result["result"] == "Hello, Alice!"

    def test_run_flow_with_error(self):
        """Test that flow errors are properly raised."""
        def failing_func(x):
            raise ValueError("Intentional error")

        flow = Flow("failing")
        flow.add_node("fail", failing_func)
        flow.set_entry("fail")

        runner = FlowRunner(flow)
        result = runner.run({})

        # Function adapter catches errors and returns error output
        assert result.get("success") is False
        assert "error" in result


class TestFlowState:
    """Tests for flow state management."""

    def test_start_returns_run_id(self):
        """Test that start() returns a run ID."""
        flow = Flow("test")
        flow.add_node("start", lambda x: x)
        flow.set_entry("start")

        runner = FlowRunner(flow)
        run_id = runner.start({})

        assert run_id is not None
        assert runner.run_id == run_id

    def test_get_state(self):
        """Test getting the run state."""
        flow = Flow("test")
        flow.add_node("start", lambda x: x)
        flow.set_entry("start")

        runner = FlowRunner(flow)

        # Before start
        assert runner.get_state() is None

        # After start
        runner.start({})
        state = runner.get_state()

        assert state is not None
        assert state.workflow_id == "test"

    def test_step_without_start_raises(self):
        """Test that step() without start() raises an error."""
        flow = Flow("test")
        flow.add_node("start", lambda x: x)
        flow.set_entry("start")

        runner = FlowRunner(flow)

        with pytest.raises(ValueError, match="No active run"):
            runner.step()


class TestFlowStatusChecks:
    """Tests for flow status check methods."""

    def test_is_running(self):
        """Test is_running() check."""
        flow = Flow("test")
        flow.add_node("start", lambda x: x)
        flow.set_entry("start")

        runner = FlowRunner(flow)

        assert not runner.is_running()

        runner.start({})
        # After start but before tick, should be running
        assert runner.is_running()

    def test_is_complete(self):
        """Test is_complete() check."""
        flow = Flow("test")
        flow.add_node("start", lambda x: x)
        flow.set_entry("start")

        runner = FlowRunner(flow)
        runner.run({})

        assert runner.is_complete()


class TestFlowLedger:
    """Tests for flow execution ledger."""

    def test_get_ledger_empty(self):
        """Test get_ledger() when no run exists."""
        flow = Flow("test")
        flow.add_node("start", lambda x: x)
        flow.set_entry("start")

        runner = FlowRunner(flow)
        ledger = runner.get_ledger()

        assert ledger == []

---
file: tests/test_workflow_bundle_pack.py
---

from __future__ import annotations

from pathlib import Path

from abstractflow.workflow_bundle import pack_workflow_bundle
from abstractruntime.workflow_bundle import open_workflow_bundle
from abstractruntime.visualflow_compiler import compile_visualflow


def test_pack_workflow_bundle_creates_flow_zip_with_manifest_and_flows(tmp_path: Path) -> None:
    # Use a real shipped VisualFlow that includes subflows (ac-echo references multiple subflows).
    root = Path(__file__).resolve().parent.parent / "web" / "flows" / "ac-echo.json"
    assert root.exists()

    out = tmp_path / "ac-echo.flow"
    packed = pack_workflow_bundle(root_flow_json=root, out_path=out)
    assert packed.path.exists()

    b = open_workflow_bundle(out)
    man = b.manifest

    assert man.bundle_id == "ac-echo"
    assert man.bundle_format_version == "1"
    assert any(ep.flow_id == "ac-echo" for ep in man.entrypoints)
    assert man.default_entrypoint == "ac-echo"

    # Must include at least root + one subflow.
    assert "ac-echo" in man.flows
    assert len(man.flows) > 1
    assert man.artifacts == {}

    # Every declared flow must be readable and compile successfully.
    for flow_id, rel in man.flows.items():
        raw = b.read_json(rel)
        assert isinstance(raw, dict)
        spec = compile_visualflow(raw)
        assert spec.workflow_id == flow_id


def test_pack_workflow_bundle_embeds_manifest_metadata(tmp_path: Path) -> None:
    root = Path(__file__).resolve().parent.parent / "web" / "flows" / "ac-echo.json"
    assert root.exists()

    out = tmp_path / "ac-echo-meta.flow"
    meta = {"lineage": {"origin": "ac-echo", "previous": "0.0.0"}, "tags": ["test"]}
    pack_workflow_bundle(root_flow_json=root, out_path=out, bundle_id="ac-echo", bundle_version="0.0.1", metadata=meta)

    b = open_workflow_bundle(out)
    assert b.manifest.bundle_id == "ac-echo"
    assert b.manifest.bundle_version == "0.0.1"
    assert b.manifest.metadata.get("lineage") == {"origin": "ac-echo", "previous": "0.0.0"}
    assert b.manifest.metadata.get("tags") == ["test"]

---
file: tests/test_visual_ws_subflow.py
---

"""WebSocket integration tests for Subflow nodes (START_SUBWORKFLOW).

These tests validate that:
- a parent visual flow can execute a saved child flow as a subworkflow
- waiting in the child (ASK_USER) bubbles up to the parent and can be resumed
"""

from __future__ import annotations

import json

from fastapi.testclient import TestClient

from web.backend.main import app
from web.backend.models import NodeType, Position, VisualEdge, VisualFlow, VisualNode
from web.backend.routes.flows import _flows


def test_ws_subflow_runs_child_and_returns_output() -> None:
    child_id = "child-flow-basic"
    parent_id = "parent-flow-basic"

    child = VisualFlow(
        id=child_id,
        name="child basic",
        entryNode="c1",
        nodes=[
            VisualNode(
                id="c1",
                type=NodeType.CODE,
                position=Position(x=0, y=0),
                data={
                    "code": "def transform(input):\n    return {'child': 7}\n",
                    "functionName": "transform",
                },
            )
        ],
        edges=[],
    )

    parent = VisualFlow(
        id=parent_id,
        name="parent basic",
        entryNode="p1",
        nodes=[
            VisualNode(
                id="p1",
                type=NodeType.ON_USER_REQUEST,
                position=Position(x=0, y=0),
                data={},
            ),
            VisualNode(
                id="p2",
                type=NodeType.SUBFLOW,
                position=Position(x=0, y=0),
                data={"subflowId": child_id},
            ),
            VisualNode(
                id="p3",
                type=NodeType.CODE,
                position=Position(x=0, y=0),
                data={
                    "code": (
                        "def transform(input):\n"
                        "    return {'from_child': input.get('input')}\n"
                    ),
                    "functionName": "transform",
                },
            ),
        ],
        edges=[
            VisualEdge(
                id="e1",
                source="p1",
                sourceHandle="exec-out",
                target="p2",
                targetHandle="exec-in",
            ),
            VisualEdge(
                id="e2",
                source="p2",
                sourceHandle="exec-out",
                target="p3",
                targetHandle="exec-in",
            ),
            VisualEdge(
                id="d1",
                source="p2",
                sourceHandle="output",
                target="p3",
                targetHandle="input",
            ),
        ],
    )

    _flows[child_id] = child
    _flows[parent_id] = parent
    try:
        with TestClient(app) as client:
            with client.websocket_connect(f"/api/ws/{parent_id}") as ws:
                ws.send_text(json.dumps({"type": "run", "input_data": {}}))

                completed = None
                for _ in range(200):
                    msg = ws.receive_json()
                    if msg.get("type") == "flow_complete":
                        completed = msg
                        break

                assert completed is not None
                assert completed["result"]["success"] is True
                assert completed["result"]["from_child"] == {"child": 7, "success": True}
    finally:
        _flows.pop(child_id, None)
        _flows.pop(parent_id, None)


def test_ws_subflow_child_ask_user_waits_then_resume_completes() -> None:
    child_id = "child-flow-ask"
    parent_id = "parent-flow-ask"

    child = VisualFlow(
        id=child_id,
        name="child ask_user",
        entryNode="c1",
        nodes=[
            VisualNode(
                id="c1",
                type=NodeType.ON_USER_REQUEST,
                position=Position(x=0, y=0),
                data={},
            ),
            VisualNode(
                id="prompt",
                type=NodeType.LITERAL_STRING,
                position=Position(x=0, y=0),
                data={"literalValue": "Pick one:"},
            ),
            VisualNode(
                id="choices",
                type=NodeType.LITERAL_ARRAY,
                position=Position(x=0, y=0),
                data={"literalValue": ["alpha", "beta"]},
            ),
            VisualNode(
                id="c2",
                type=NodeType.ASK_USER,
                position=Position(x=0, y=0),
                data={"effectConfig": {"allowFreeText": False}},
            ),
            VisualNode(
                id="c3",
                type=NodeType.CODE,
                position=Position(x=0, y=0),
                data={
                    "code": "def transform(input):\n    return {'answer': input.get('input')}\n",
                    "functionName": "transform",
                },
            ),
        ],
        edges=[
            VisualEdge(
                id="e1",
                source="c1",
                sourceHandle="exec-out",
                target="c2",
                targetHandle="exec-in",
            ),
            VisualEdge(
                id="e2",
                source="c2",
                sourceHandle="exec-out",
                target="c3",
                targetHandle="exec-in",
            ),
            VisualEdge(
                id="d1",
                source="prompt",
                sourceHandle="value",
                target="c2",
                targetHandle="prompt",
            ),
            VisualEdge(
                id="d2",
                source="choices",
                sourceHandle="value",
                target="c2",
                targetHandle="choices",
            ),
            VisualEdge(
                id="d3",
                source="c2",
                sourceHandle="response",
                target="c3",
                targetHandle="input",
            ),
        ],
    )

    parent = VisualFlow(
        id=parent_id,
        name="parent asks via child",
        entryNode="p1",
        nodes=[
            VisualNode(
                id="p1",
                type=NodeType.ON_USER_REQUEST,
                position=Position(x=0, y=0),
                data={},
            ),
            VisualNode(
                id="p2",
                type=NodeType.SUBFLOW,
                position=Position(x=0, y=0),
                data={"subflowId": child_id},
            ),
            VisualNode(
                id="p3",
                type=NodeType.CODE,
                position=Position(x=0, y=0),
                data={
                    "code": "def transform(input):\n    return {'child_answer': input.get('input')}\n",
                    "functionName": "transform",
                },
            ),
        ],
        edges=[
            VisualEdge(
                id="e1",
                source="p1",
                sourceHandle="exec-out",
                target="p2",
                targetHandle="exec-in",
            ),
            VisualEdge(
                id="e2",
                source="p2",
                sourceHandle="exec-out",
                target="p3",
                targetHandle="exec-in",
            ),
            VisualEdge(
                id="d1",
                source="p2",
                sourceHandle="output",
                target="p3",
                targetHandle="input",
            ),
        ],
    )

    _flows[child_id] = child
    _flows[parent_id] = parent
    try:
        with TestClient(app) as client:
            with client.websocket_connect(f"/api/ws/{parent_id}") as ws:
                ws.send_text(json.dumps({"type": "run", "input_data": {}}))

                waiting = None
                for _ in range(200):
                    msg = ws.receive_json()
                    if msg.get("type") == "flow_waiting":
                        waiting = msg
                        break

                assert waiting is not None
                assert waiting["nodeId"] == "p2"
                assert waiting["prompt"] == "Pick one:"
                assert waiting["choices"] == ["alpha", "beta"]
                assert waiting["allow_free_text"] is False

                ws.send_text(json.dumps({"type": "resume", "response": "beta"}))

                subflow_complete = None
                completed = None
                for _ in range(400):
                    msg = ws.receive_json()
                    if msg.get("type") == "node_complete" and msg.get("nodeId") == "p2":
                        subflow_complete = msg
                    if msg.get("type") == "flow_complete":
                        completed = msg
                        break

                assert subflow_complete is not None
                assert isinstance(subflow_complete.get("result"), dict)
                assert subflow_complete["result"].get("output") == {"answer": "beta", "success": True}
                assert isinstance(subflow_complete.get("meta"), dict)
                assert isinstance(subflow_complete["meta"].get("duration_ms"), (int, float))

                assert completed is not None
                assert completed["result"]["success"] is True
                assert completed["result"]["child_answer"] == {"answer": "beta", "success": True}
    finally:
        _flows.pop(child_id, None)
        _flows.pop(parent_id, None)


def test_ws_subflow_can_inherit_parent_context_messages_when_configured() -> None:
    child_id = "child-flow-inherit-context"
    parent_id = "parent-flow-inherit-context"

    child = VisualFlow(
        id=child_id,
        name="child inherit context",
        entryNode="c1",
        nodes=[
            VisualNode(
                id="c1",
                type=NodeType.CODE,
                position=Position(x=0, y=0),
                data={
                    "inputKey": "context",
                    "code": (
                        "def transform(input):\n"
                        "    msgs = input.get('messages') if isinstance(input, dict) else []\n"
                        "    if not isinstance(msgs, list):\n"
                        "        return {'inherited': False}\n"
                        "    for m in msgs:\n"
                        "        if not isinstance(m, dict):\n"
                        "            continue\n"
                        "        if str(m.get('content') or '') == 'PARENT_CTX':\n"
                        "            return {'inherited': True}\n"
                        "    return {'inherited': False}\n"
                    ),
                    "functionName": "transform",
                },
            )
        ],
        edges=[],
    )

    parent = VisualFlow(
        id=parent_id,
        name="parent inherit context",
        entryNode="p1",
        nodes=[
            VisualNode(
                id="p1",
                type=NodeType.ON_USER_REQUEST,
                position=Position(x=0, y=0),
                data={},
            ),
            VisualNode(
                id="p2",
                type=NodeType.SUBFLOW,
                position=Position(x=0, y=0),
                data={"subflowId": child_id, "effectConfig": {"inherit_context": True}},
            ),
            VisualNode(
                id="p3",
                type=NodeType.CODE,
                position=Position(x=0, y=0),
                data={
                    "code": (
                        "def transform(input):\n"
                        "    out = input.get('input') if isinstance(input, dict) else None\n"
                        "    return {'inherited': (out or {}).get('inherited') if isinstance(out, dict) else False}\n"
                    ),
                    "functionName": "transform",
                },
            ),
        ],
        edges=[
            VisualEdge(id="e1", source="p1", sourceHandle="exec-out", target="p2", targetHandle="exec-in"),
            VisualEdge(id="e2", source="p2", sourceHandle="exec-out", target="p3", targetHandle="exec-in"),
            VisualEdge(id="d1", source="p2", sourceHandle="output", target="p3", targetHandle="input"),
        ],
    )

    _flows[child_id] = child
    _flows[parent_id] = parent
    try:
        with TestClient(app) as client:
            with client.websocket_connect(f"/api/ws/{parent_id}") as ws:
                ws.send_text(
                    json.dumps(
                        {
                            "type": "run",
                            "input_data": {
                                "context": {"messages": [{"role": "system", "content": "PARENT_CTX"}]},
                            },
                        }
                    )
                )

                completed = None
                for _ in range(200):
                    msg = ws.receive_json()
                    if msg.get("type") == "flow_complete":
                        completed = msg
                        break

                assert completed is not None
                assert completed["result"]["success"] is True
                assert completed["result"]["inherited"] is True
    finally:
        _flows.pop(child_id, None)
        _flows.pop(parent_id, None)

---
file: CONTRIBUTING.md
---

# Contributing

Thanks for your interest in contributing to AbstractFlow.

Quick links:
- Docs index: `docs/README.md`
- Getting started: `docs/getting-started.md`
- Architecture: `docs/architecture.md`
- Web editor run guide: `docs/web-editor.md`
- Security reporting: `SECURITY.md` (please use for vulnerability reports)

## Ways to contribute

- Bug reports with minimal repros (include flow JSON when relevant)
- Documentation improvements (especially accuracy + cross-links)
- Focused fixes and small features via pull requests

Security issues: please follow `SECURITY.md` and avoid public disclosure.

## Development setup

Requirements:
- Python **3.10+**
- Node.js **18+** (only if you work on the visual editor in `web/frontend/`)

Create a virtual environment and install the package in editable mode:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev,server,agent]"
```

Run tests:

```bash
pytest -q
```

## Working on the visual editor

The visual editor is a reference/dev app under `web/`:
- Backend: FastAPI (`web/backend/`)
- Frontend: React/Vite (`web/frontend/`)

Run instructions: `docs/web-editor.md`.

## Style and quality

- Keep changes focused and well-scoped.
- Prefer adding/adjusting tests when changing behavior (`tests/`).
- Keep docs concise and accurate; update cross-references when adding new docs (`docs/README.md` is the index).
- If you change docs, regenerate the full agentic pack: `python scripts/generate_llms_full.py` (updates `llms-full.txt`).

Optional local tooling (if you use it):

```bash
python -m black .
python -m isort .
python -m flake8
python -m mypy abstractflow
pre-commit run -a
```

## Pull request checklist

- Tests pass (`pytest -q`)
- Docs updated (when behavior changes) and `docs/README.md` stays a good entrypoint
- `CHANGELOG.md` updated for user-visible changes

---
file: SECURITY.md
---

# Security policy

We take security reports seriously and appreciate responsible disclosure.

## Reporting a vulnerability

Please **do not** open a public GitHub issue for security-sensitive reports.

Instead, report vulnerabilities by email:
- `contact@abstractflow.ai`

Include as much of the following as possible:
- A clear description of the issue and potential impact
- Steps to reproduce (or a minimal proof-of-concept)
- Affected component(s) (e.g. `abstractflow` library vs the `web/` editor backend)
- Version information (`abstractflow.__version__`, Python version, OS)
- Any relevant logs/config (please redact secrets)
- If applicable: the smallest `VisualFlow` JSON that reproduces the issue

We will respond as quickly as we can and coordinate a fix and disclosure timeline with you.

## Scope (what to report here)

This policy covers:
- The published Python package (`abstractflow/`)
- The reference visual editor app shipped in this repository (`web/`)
- Packaging/release issues affecting published artifacts (PyPI / npm), when applicable

## Coordinated disclosure

- Please avoid testing on systems you don’t own or have permission to test.
- If you’d like public credit for your report, tell us what name/handle to use.
- If you need encrypted communication, email us and we’ll coordinate a safe channel.

## Supported versions

AbstractFlow is currently **Pre-alpha**. We recommend staying on the latest patch release.

Evidence: `pyproject.toml` (`Development Status :: 2 - Pre-Alpha`).

---
file: ACKNOWLEDMENTS.md
---

# Acknowledgments

AbstractFlow is built on the shoulders of many projects and ideas. Thank you to the maintainers and contributors of the libraries and tools that make this project possible.

## Inspiration

- The visual workflow UX is inspired by Unreal Engine (UE4/UE5) Blueprints: execution pins, typed pins, and “graph as program”.

Evidence: [docs/architecture.md](docs/architecture.md).

## Core building blocks

- AbstractFramework (ecosystem): https://github.com/lpalbou/AbstractFramework
- AbstractRuntime (durable execution kernel: runs, waits, ledgers, artifacts)
- AbstractCore (providers/models/tools integration used by runtime effects)
- AbstractAgent (optional ReAct/CodeAct agent workflows used by the Visual Agent node)

See `README.md` for links to the upstream repositories.

Evidence: [pyproject.toml](pyproject.toml) (dependencies and extras), [abstractflow/visual/executor.py](abstractflow/visual/executor.py) (Agent + memory wiring).

## Open-source libraries used in this repo

This list is intentionally focused on **direct dependencies** declared by the repository (plus a small number of optional integrations that are referenced in code). For the authoritative list, use the manifests below.

Evidence: `pyproject.toml`, `web/frontend/package.json`.

### Python package (`abstractflow/`)

Declared runtime dependencies:
- AbstractRuntime
- abstractcore (`abstractcore[tools]`)
- Pydantic
- typing-extensions

Evidence: `pyproject.toml` (`project.dependencies`).

Optional Python extras:
- `abstractflow[agent]`: AbstractAgent
- `abstractflow[server]`: FastAPI, Uvicorn, websockets
- `abstractflow[ui]`: Streamlit, Plotly, NetworkX
- `abstractflow[dev]`: pytest, pytest-asyncio, Black, isort, Flake8, mypy, pre-commit

Evidence: `pyproject.toml` (`project.optional-dependencies`).

Optional integration (not installed by default):
- AbstractMemory: used when executing `memory_kg_*` nodes; requires `abstractmemory` (and a LanceDB-backed store when configured).

Evidence: `abstractflow/visual/executor.py` (imports + error messages for missing installs).

### Reference web editor (`web/`)

Backend (FastAPI):
- FastAPI (API + WebSockets)
- Uvicorn (ASGI server)

Evidence: `web/backend/main.py`, `pyproject.toml` (`server` extra).

Frontend (React/Vite):
- React, React DOM
- React Flow (graph editor)
- Vite, TypeScript
- Monaco editor (`@monaco-editor/react`)
- TanStack Query (`@tanstack/react-query`)
- Zustand (state)
- DOMPurify (HTML sanitization)
- Marked (Markdown rendering)
- clsx (className composition)
- react-hot-toast (toasts)

Evidence: `web/frontend/package.json`.

Developer tooling (frontend):
- ESLint + TypeScript ESLint
- `@types/*` typings packages

Evidence: `web/frontend/package.json` (`devDependencies`).

---
file: CHANGELOG.md
---

# Changelog

All notable changes to AbstractFlow will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- **AbstractCode UI event demo flows** (`web/flows/*.json`):
  - `acagent_message_demo.json`: `abstractcode.message`
  - `acagent_ask_demo.json`: durable ask+wait via `wait_event.prompt`
  - `acagent_tool_events_demo.json`: `abstractcode.tool_execution` + `abstractcode.tool_result`
- **Tool observability wiring improvements (Visual nodes)**:
  - `LLM Call` exposes `tool_calls` as a first-class output pin (same as `result.tool_calls`) for easier wiring into `Tool Calls` / `Emit Event`.
  - `Agent` exposes best-effort `tool_calls` / `tool_results` extracted from its scratchpad trace (post-run ergonomics).
- **Pure Utility Nodes (Runtime-backed)**:
  - `Stringify JSON` (`stringify_json`): Render JSON (or JSON-ish strings) into text with a `mode` dropdown (`none` | `beautify` | `minified`). Implementation delegates to `abstractruntime.rendering.stringify_json` for consistent host behavior.
  - `Agent Trace Report` (`agent_trace_report`): Render an agent scratchpad (`node_traces`) into a condensed Markdown timeline of LLM calls and tool actions (full tool args + results, no truncation). Implementation delegates to `abstractruntime.rendering.render_agent_trace_markdown`.

### Changed
- **Run Flow modal (array parameters)**: Array pins now render as a Blueprint-style item list (add/remove items) with a "Raw JSON (advanced)" escape hatch for non-string arrays.

### Fixed
- **FlowRunner SUBWORKFLOW auto-drive**: `FlowRunner.run()` no longer hangs if the runtime registry contains only subworkflow specs (common in unit tests). It now falls back to the runner’s own root `WorkflowSpec` when resuming/bubbling parents.

## [0.3.4] - 2026-02-06

### Added
- **More AbstractCore “common tools” in the editor**: `skim_url` and `skim_websearch` are now included in `/api/tools` and are executable by the default host tool executor.
- **Comms tools documentation**: clarified how to opt into email/WhatsApp/Telegram tools via env flags.

## [0.3.3] - 2026-02-06

### Added
- **`abstractflow[editor]` extra** as the recommended install for the Visual Editor backend (equivalent to `abstractflow[server]` + `abstractflow[agent]`).

## [0.3.2] - 2026-02-06

### Added
- **Packaged visual editor backend** (FastAPI) as part of `abstractflow[server]`:
  - `abstractflow serve ...` CLI subcommand
  - `abstractflow-backend ...` console script (alias of `python -m backend`)

### Changed
- **Backend runtime directory defaults**:
  - source checkout: `web/runtime/`
  - installed package: `~/.abstractflow/runtime`
  - override: `ABSTRACTFLOW_RUNTIME_DIR`
- **Backend flow storage can be overridden** via `ABSTRACTFLOW_FLOWS_DIR` (default remains `./flows`).
- **Default publish directory** is now `./flows/bundles/` (override via `ABSTRACTFLOW_PUBLISH_DIR`).

### Fixed
- **`npx @abstractframework/flow` UI server now proxies `/api/*`** (HTTP + WebSocket) to the backend, preventing “Save failed: JSON.parse …” when the backend is running.

## [0.3.1] - 2026-02-04

### Added
- **User-facing documentation set** for public release:
  - Core docs: `README.md`, `docs/getting-started.md`, `docs/architecture.md`, `docs/api.md`, `docs/faq.md`
  - Repo policies: `CONTRIBUTING.md`, `SECURITY.md`, `ACKNOWLEDMENTS.md`
  - Agentic index: `llms.txt`, `llms-full.txt`

### Changed
- **Documentation accuracy + structure**: refreshed docs to match the implemented code (VisualFlow portability, runtime wiring, CLI bundle tooling, web editor layout) and improved cross-references for first-time users.

## [0.3.0] - 2025-01-06

### Added
- **VisualFlow Interface System** (`abstractflow/visual/interfaces.py`): Declarative workflow interface markers for portable host validation, enabling workflows to be run as specialized capabilities with known IO contracts
  - `abstractcode.agent.v1` interface: Host-configurable prompt → response contract for running a workflow as an AbstractCode agent
  - Interface validation with required/recommended pin specifications (provider/model/tools/prompt/response)
  - Auto-scaffolding support: enabling `abstractcode.agent.v1` auto-creates `On Flow Start` / `On Flow End` nodes with required pins
- **Structured Output Support**: Visual `LLM Call` and `Agent` nodes accept optional `response_schema` input pin (JSON Schema object) for schema-conformant responses
  - New literal node `JSON Schema` (`json_schema`) to author schema objects
  - New `JsonSchemaNodeEditor` UI component for authoring schemas in the visual editor
  - Pin-driven schema overrides node config and enables durable structured-output enforcement via AbstractRuntime `LLM_CALL`
- **Tool Calling Infrastructure**:
  - Visual `LLM Call` nodes support optional **tool calling** via `tools` allowlist input (pin or node config)
  - Expose structured `result` output object (normalized LLM response including `tool_calls`, `usage`, `trace_id`)
  - Inline tools dropdown in node UI (when `tools` pin not connected)
  - Visual `Tool Calls` node (`tool_calls`) to execute tool call requests via AbstractRuntime `EffectType.TOOL_CALLS`
  - New pure node `Tools Allowlist` (`tools_allowlist`) with inline multi-select for workflow-scope tool lists
  - Dedicated `tools` pin type (specialized `string[]`) for `On Flow Start` parameters
- **Control Flow & Loop Enhancements**:
  - New control node `For` (`for`) for numeric loops with `start`/`end`/`step` inputs and `i`/`index` outputs
  - `While` node now exposes `index` output pin (0-based iteration count) and `item:any` output pin for parity with `ForEach`
  - `Loop` (Foreach) now invalidates cached pure-node outputs per-iteration (fixes scratchpad accumulation)
- **Workflow Variables**:
  - New pure node `Variable` (`var_decl`) to declare workflow-scope persistent variables with explicit types
  - New pure node `Bool Variable` (`bool_var`) for boolean variables with typed outputs
  - New execution node `Set Variables` (`set_vars`) to update multiple variables in a single step
  - New execution node `Set Variable Property` (`set_var_property`) to update nested object properties
  - `Get Variable` (`get_var`) reads from durable `run.vars` by dotted path
  - `Set Variable` (`set_var`) updates `run.vars` with pass-through execution semantics
- **Custom Events** (Blueprint-style):
  - `On Event` listeners compiled into dedicated durable subworkflows (auto-started, session-scoped)
  - `Emit Event` node dispatches durable events via AbstractRuntime
- **Run History & Observability**:
  - New web API endpoints: `/api/runs`, `/api/runs/{run_id}/history`, `/api/runs/{run_id}/artifacts/{artifact_id}`
  - UI "Run History" picker (🕘) to open past runs and apply pause/resume/cancel controls
  - Run modal shows clickable **run id** pill (hover → copy to clipboard)
  - Run modal header token badge reflects cumulative LLM usage across entire run tree
  - WebSocket events include JSON-safe ISO timestamp (`ts`)
  - Runtime node trace entries streamed incrementally over WebSocket (`trace_update`)
  - Agent details panel renders live sub-run trace with expandable prompts/responses/errors
- **Pure Utility Nodes**:
  - `Parse JSON` (`parse_json`) to convert JSON/JSON-ish strings into objects
  - `coalesce` (first non-null selection by pin order)
  - `string_template` (render `{{path.to.value}}` with filters: json, join, trim)
  - `array_length`, `array_append`, `array_dedup`
  - `Compare` (`compare`) now has `op` input pin supporting `==`, `>=`, `>`, `<=`, `<`
  - `get` (Get Property) supports `default` input and safer nested path handling (e.g. `a[0].b`)
- **Memory Node Enhancements**:
  - `Memorize` (`memory_note`) adds optional `location` input
  - `Memorize` supports **Keep in context** toggle to rehydrate notes into `context.messages`
  - `Recall` (`memory_query`) adds `tags_mode` (all/any), `usernames`, `locations` inputs
- **Subflow Enhancements**:
  - `Subflow` supports **Inherit context** toggle to seed child run's `context.messages` from parent
  - `multi_agent_state_machine` accepts `workspace_root` parameter to scope agent file/system tools
- **Visual Execution Defaults**:
  - Default **LLM HTTP timeout** (7200s, overrideable via `ABSTRACTFLOW_LLM_TIMEOUT_S`)
  - Default **max output token cap** (4096, overrideable via `ABSTRACTFLOW_LLM_MAX_OUTPUT_TOKENS`)
- **UI/UX Improvements**:
  - Run preflight validation panel with itemized "Fix before running" checklist
  - Node tooltips available in palette and on-canvas (hover > 1s)
  - Node palette exposed transforms (`trim`, `substring`, `format`) and math ops (`modulo`, `power`)
  - Enhanced `PropertiesPanel` with structured output configuration
  - Improved `RunFlowModal` with better input validation and error display
  - JSON validation and error handling across executor and frontend (`web/frontend/src/utils/validation.ts`)

### Changed
- **Workflow-Agent Interface UX**: Enabling `abstractcode.agent.v1` auto-scaffolds `On Flow Start` / `On Flow End` pins (provider/model/tools)
- **Memory Nodes UX**: `memory_note` labeled **Memorize** (was Remember) to align with AbstractCode `/memorize`
- **Flow Library Modal**: Flow name/description edited via inline pencil icons (removed Rename/Edit Description buttons)
- **Run Modal UX**:
  - String inputs default to 3-line textarea
  - Modal actions pinned in footer (body scrolls)
  - No truncation of sub-run/memory previews (full content on demand)
  - JSON panels (`Raw JSON`, `Trace JSON`, `Scratchpad`) syntax-highlighted
- **Node Palette Organization**:
  - Removed **Effects** category
  - Added **Memory** category (memories + file IO)
  - Added **Math** category (after Variables)
  - Moved **Delay** to **Events**
  - Split into **Literals**, **Variables**, **Data** (renamed from "Data" to **Transforms**)
  - Reordered **Control** nodes (loops → branching → conditions)
  - `System Date/Time` moved to **Events**
  - `Provider Catalog` + `Models Catalog` moved to **Literals**
  - `Tool Calls` moved from **Effects** to **Core** (reordered: Subflow, Agent, LLM Call, Tool Calls, Ask User, Answer User)
- **Models Catalog**: Removed deprecated `allowed_models` input pin (in-node multi-select synced with right panel)
- **Node/Pin Tooltips**: Appear after 2s hover, rendered in overlay layer (no clipping)
- **Python Code Nodes**: Include in-node **Edit Code** button; editor injects "Available variables" comment block
- **Execution Highlighting**: Stronger, more diffuse bloom for readability during runs; afterglow decays smoothly (3s), highlights only taken edges
- **Data Edges**: Colored by data type (based on source pin type)

### Fixed
- **Recursive Subflows**: Visual data-edge cache (`flow._node_outputs`) now isolated per `run_id` to prevent stale outputs leaking across nested runs (fixes self/mutual recursion with pure nodes like `compare`, `subtract`)
- **Durable Persistence**: `on_flow_start` no longer leaks internal `_temp` into cached node outputs (prevented `RecursionError: maximum recursion depth exceeded`)
- **WebSocket Run Controls**: Pause/resume/cancel no longer block on per-connection execution lock (responsive during long-running LLM/Agent nodes)
- **WebSocket Resilience**:
  - Controls resilient to transient disconnects (can send with explicit `run_id`, UI reconnects-and-sends)
  - Execution resilient to UI disconnects (dropped connection doesn't cancel in-flight run)
- **VisualFlow Execution**: Ignores unreachable/disconnected execution nodes (orphan `llm_call`/`subflow` can't fail initialization)
- **Loop Nodes**:
  - `Split` avoids spurious empty trailing items (e.g. `"A@@B@@"`) so `Loop` doesn't execute extra empty iteration
  - Scheduler-node outputs in WebSocket `node_complete`: Loop/While/For sync persisted `{index,...}` outputs to `flow._node_outputs` (UI no longer shows stale index)
- **Pure Node Behavior**:
  - `Concat` infers stable pin order (a..z) when template metadata missing
  - `Set Variable` defaulting for typed primitives: `boolean/number/string` pins default to `false/0/""` instead of `None`
- **Agent Nodes**: Reset per-node state when re-entered (e.g. inside `Loop` iterations) so each iteration re-resolves inputs
- **Run Modal Observability**:
  - WebSocket `node_start`/`node_complete` events include `runId` (distinguish root vs child runs)
  - Visual Agent nodes start ReAct subworkflow in **async+wait** mode for incremental ticking
  - Run history replay synthesizes missing `node_complete` events for steps left open in durable ledger
- **Canvas Highlighting**: Robust to fast child-run emissions (race with `node_start` before `runId` state update fixed)
- **WebSocket Subworkflow Waits**: Correctly close waiting node when run resumes past `WAITING(reason=SUBWORKFLOW)`
- **Web Run History**: Reliably shows persisted runs regardless of server working directory (backend defaults to `web/runtime` unless `ABSTRACTFLOW_RUNTIME_DIR` set)
- **Cancel Run**: No longer surfaces as `flow_error` from `asyncio.CancelledError` (treated as expected control-plane operation)
- **Markdown Code Blocks**: "Copy" now copies original raw code (preserves newlines/indentation) after syntax highlighting

### Technical Details
- **13 commits**, **48 files changed**: 12,142 insertions, 368 deletions
- New module: `abstractflow/visual/interfaces.py` (347 lines)
- New UI component: `web/frontend/src/components/JsonSchemaNodeEditor.tsx` (460 lines)
- New tests: `test_visual_interfaces.py`, `test_visual_agent_structured_output_pin.py`, `test_visual_llm_call_structured_output_pin.py`, `test_visual_subflow_recursion.py`
- Compiler enhancements: Interface validation, per-run cache isolation, structured output pin support
- Executor optimizations: Performance improvements for VisualFlow execution
- 12 new example workflow JSON files in `web/flows/`

### Notes
- This repository includes the published Python package (`abstractflow/`) and a reference visual editor app (`web/`).

## [0.1.0] - 2025-01-15

### Added
- Initial placeholder package to reserve PyPI name
- Basic project structure and packaging configuration
- Comprehensive README with project vision and roadmap
- MIT license and contribution guidelines
- CLI placeholder with planned command structure

### Notes
- This is a placeholder release to secure the `abstractflow` name on PyPI
- No functional code is included in this version
- Follow the GitHub repository for development updates and release timeline

---
file: scripts/generate_llms_full.py
---

from __future__ import annotations

import re
from pathlib import Path


LLMS_TXT = Path("llms.txt")
OUTPUT = Path("llms-full.txt")

_LINK_RE = re.compile(r"^\s*-\s*\[[^\]]+\]\((?P<url>[^)]+)\)")


def _parse_llms_txt_links(text: str) -> list[str]:
    """Extract link URLs from `llms.txt` file-list sections.

    This is intentionally conservative:
    - We only read Markdown list items of the form: `- [Title](URL)`
    - We only start parsing after the first `##` section header (i.e., the file-list area).
    - We ignore any other links that appear in prose/supporting context.
    """
    out: list[str] = []
    in_file_sections = False
    for line in text.splitlines():
        if line.startswith("## "):
            in_file_sections = True
        if not in_file_sections:
            continue
        m = _LINK_RE.match(line)
        if not m:
            continue
        url = str(m.group("url") or "").strip()
        if url:
            out.append(url)
    return out


def _is_external_url(url: str) -> bool:
    u = str(url or "").strip().lower()
    return u.startswith("http://") or u.startswith("https://")


def _normalize_local_path(url: str) -> str:
    # Remove fragments/query (we inline the file, not a view).
    u = str(url or "").strip()
    u = u.split("#", 1)[0].split("?", 1)[0].strip()
    return u


def _unique_in_order(items: list[str]) -> list[str]:
    seen: set[str] = set()
    out: list[str] = []
    for x in items:
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


def _build() -> str:
    if not LLMS_TXT.exists():
        raise FileNotFoundError("Missing llms.txt at repo root")

    llms_text = LLMS_TXT.read_text(encoding="utf-8")
    urls = _parse_llms_txt_links(llms_text)

    # Always include llms.txt itself at the top for a self-contained pack.
    local_paths: list[str] = ["llms.txt"]
    external_urls: list[str] = []

    for u in urls:
        if _is_external_url(u):
            external_urls.append(u)
            continue
        p = _normalize_local_path(u)
        if not p:
            continue
        # Avoid self-recursive expansion.
        if p == str(OUTPUT):
            continue
        local_paths.append(p)

    local_paths = _unique_in_order(local_paths)
    external_urls = _unique_in_order(external_urls)

    missing_local = [p for p in local_paths if not Path(p).exists()]
    if missing_local:
        raise FileNotFoundError(f"llms.txt references missing local paths: {missing_local}")

    out: list[str] = []
    out.append("# AbstractFlow (llms-full)")
    out.append("")
    out.append(
        "> Generated file: a single, offline-friendly context pack for agents. "
        "It inlines local files linked from `llms.txt` (including `Optional`). "
        "Prefer `llms.txt` for navigation and treat in-repo files as source of truth."
    )
    out.append("")
    out.append("Generated by: `scripts/generate_llms_full.py`")
    out.append("")
    out.append("Included local files (repo-root relative, in order):")
    out.extend([f"- {p}" for p in local_paths])
    out.append("")

    if external_urls:
        out.append("External references (not inlined):")
        out.extend([f"- {u}" for u in external_urls])
        out.append("")

    for p in local_paths:
        out.append("---")
        out.append(f"file: {p}")
        out.append("---")
        out.append("")
        content = Path(p).read_text(encoding="utf-8")
        out.append(content.rstrip())
        out.append("")

    return "\n".join(out).rstrip() + "\n"


def main() -> None:
    OUTPUT.write_text(_build(), encoding="utf-8")


if __name__ == "__main__":
    main()

---
file: web/flows/ac-echo.json
---

{
  "id": "ac-echo",
  "name": "AbstractCode Echo Service",
  "description": "",
  "interfaces": [
    "abstractcode.agent.v1"
  ],
  "nodes": [
    {
      "id": "node-1",
      "type": "on_flow_start",
      "position": {
        "x": -432.0,
        "y": 16.0
      },
      "data": {
        "nodeType": "on_flow_start",
        "label": "On Flow Start",
        "icon": "&#x1F3C1;",
        "headerColor": "#C0392B",
        "inputs": [],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "prompt",
            "label": "prompt",
            "type": "string"
          },
          {
            "id": "provider",
            "label": "provider",
            "type": "provider"
          },
          {
            "id": "model",
            "label": "model",
            "type": "model"
          },
          {
            "id": "tools",
            "label": "tools",
            "type": "tools"
          },
          {
            "id": "context",
            "label": "context",
            "type": "object"
          },
          {
            "id": "max_iterations",
            "label": "max_iterations",
            "type": "number"
          }
        ]
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-2",
      "type": "on_flow_end",
      "position": {
        "x": 528.0,
        "y": 16.0
      },
      "data": {
        "nodeType": "on_flow_end",
        "label": "On Flow End",
        "icon": "&#x23F9;",
        "headerColor": "#C0392B",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "response",
            "label": "response",
            "type": "string"
          },
          {
            "id": "result",
            "label": "result",
            "type": "object"
          },
          {
            "id": "meta",
            "label": "meta",
            "type": "object"
          },
          {
            "id": "scratchpad",
            "label": "scratchpad",
            "type": "object"
          }
        ],
        "outputs": []
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-3",
      "type": "concat",
      "position": {
        "x": 32.0,
        "y": 96.0
      },
      "data": {
        "nodeType": "concat",
        "label": "Concat",
        "icon": "&#x2795;",
        "headerColor": "#E74C3C",
        "inputs": [
          {
            "id": "a",
            "label": "a",
            "type": "string"
          },
          {
            "id": "b",
            "label": "b",
            "type": "string"
          }
        ],
        "outputs": [
          {
            "id": "result",
            "label": "result",
            "type": "string"
          }
        ],
        "concatConfig": {
          "separator": " "
        },
        "pinDefaults": {
          "a": "ECHO SERVER : "
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-4",
      "type": "subflow",
      "position": {
        "x": -160.0,
        "y": -160.0
      },
      "data": {
        "nodeType": "subflow",
        "label": "Subflow",
        "icon": "&#x1F4E6;",
        "headerColor": "#00CCCC",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "inherit_context",
            "label": "inherit_context",
            "type": "boolean",
            "description": "When true, seed the child run's context.messages from the parent's active context messages. If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "input",
            "label": "input",
            "type": "object"
          },
          {
            "id": "state",
            "label": "state",
            "type": "string"
          },
          {
            "id": "post_delay",
            "label": "post_delay",
            "type": "number"
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          }
        ],
        "subflowId": "15f19f7f",
        "pinDefaults": {
          "state": "Starting..",
          "post_delay": 2
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-5",
      "type": "subflow",
      "position": {
        "x": 176.0,
        "y": -160.0
      },
      "data": {
        "nodeType": "subflow",
        "label": "Subflow",
        "icon": "&#x1F4E6;",
        "headerColor": "#00CCCC",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "state",
            "label": "state",
            "type": "string"
          },
          {
            "id": "post_delay",
            "label": "post_delay",
            "type": "number"
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          }
        ],
        "subflowId": "15f19f7f",
        "pinDefaults": {
          "state": "Ending..",
          "post_delay": 2
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    }
  ],
  "edges": [
    {
      "id": "edge-1767677039089",
      "source": "node-1",
      "sourceHandle": "prompt",
      "target": "node-3",
      "targetHandle": "b",
      "animated": false
    },
    {
      "id": "edge-1767677041537",
      "source": "node-3",
      "sourceHandle": "result",
      "target": "node-2",
      "targetHandle": "response",
      "animated": false
    },
    {
      "id": "edge-1767736920933",
      "source": "node-1",
      "sourceHandle": "exec-out",
      "target": "node-4",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767736941927",
      "source": "node-4",
      "sourceHandle": "exec-out",
      "target": "node-5",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767736951508",
      "source": "node-5",
      "sourceHandle": "exec-out",
      "target": "node-2",
      "targetHandle": "exec-in",
      "animated": true
    }
  ],
  "entryNode": "node-1",
  "created_at": "2026-01-06T05:23:42.931416",
  "updated_at": "2026-01-06T22:03:38.958643"
}
