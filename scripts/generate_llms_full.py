from __future__ import annotations

import re
from pathlib import Path


LLMS_TXT = Path("llms.txt")
OUTPUT = Path("llms-full.txt")

_LINK_RE = re.compile(r"^\s*-\s*\[[^\]]+\]\((?P<url>[^)]+)\)")


def _parse_llms_txt_links(text: str) -> list[str]:
    """Extract link URLs from `llms.txt` file-list sections.

    This is intentionally conservative:
    - We only read Markdown list items of the form: `- [Title](URL)`
    - We only start parsing after the first `##` section header (i.e., the file-list area).
    - We ignore any other links that appear in prose/supporting context.
    """
    out: list[str] = []
    in_file_sections = False
    for line in text.splitlines():
        if line.startswith("## "):
            in_file_sections = True
        if not in_file_sections:
            continue
        m = _LINK_RE.match(line)
        if not m:
            continue
        url = str(m.group("url") or "").strip()
        if url:
            out.append(url)
    return out


def _is_external_url(url: str) -> bool:
    u = str(url or "").strip().lower()
    return u.startswith("http://") or u.startswith("https://")


def _normalize_local_path(url: str) -> str:
    # Remove fragments/query (we inline the file, not a view).
    u = str(url or "").strip()
    u = u.split("#", 1)[0].split("?", 1)[0].strip()
    return u


def _unique_in_order(items: list[str]) -> list[str]:
    seen: set[str] = set()
    out: list[str] = []
    for x in items:
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


def _build() -> str:
    if not LLMS_TXT.exists():
        raise FileNotFoundError("Missing llms.txt at repo root")

    llms_text = LLMS_TXT.read_text(encoding="utf-8")
    urls = _parse_llms_txt_links(llms_text)

    # Always include llms.txt itself at the top for a self-contained pack.
    local_paths: list[str] = ["llms.txt"]
    external_urls: list[str] = []

    for u in urls:
        if _is_external_url(u):
            external_urls.append(u)
            continue
        p = _normalize_local_path(u)
        if not p:
            continue
        # Avoid self-recursive expansion.
        if p == str(OUTPUT):
            continue
        local_paths.append(p)

    local_paths = _unique_in_order(local_paths)
    external_urls = _unique_in_order(external_urls)

    missing_local = [p for p in local_paths if not Path(p).exists()]
    if missing_local:
        raise FileNotFoundError(f"llms.txt references missing local paths: {missing_local}")

    out: list[str] = []
    out.append("# AbstractFlow (llms-full)")
    out.append("")
    out.append(
        "> Generated file: a single, offline-friendly context pack for agents. "
        "It inlines local files linked from `llms.txt` (including `Optional`). "
        "Prefer `llms.txt` for navigation and treat in-repo files as source of truth."
    )
    out.append("")
    out.append("Generated by: `scripts/generate_llms_full.py`")
    out.append("")
    out.append("Included local files (repo-root relative, in order):")
    out.extend([f"- {p}" for p in local_paths])
    out.append("")

    if external_urls:
        out.append("External references (not inlined):")
        out.extend([f"- {u}" for u in external_urls])
        out.append("")

    for p in local_paths:
        out.append("---")
        out.append(f"file: {p}")
        out.append("---")
        out.append("")
        content = Path(p).read_text(encoding="utf-8")
        out.append(content.rstrip())
        out.append("")

    return "\n".join(out).rstrip() + "\n"


def main() -> None:
    OUTPUT.write_text(_build(), encoding="utf-8")


if __name__ == "__main__":
    main()
