{
  "id": "bff163ad",
  "name": "memory-recall-tests",
  "description": "This is a series of tests to explore how the memorize and recall work, including in terms of \"use context\". \n\nThis is crucial to make sure the memory works as expected",
  "interfaces": [],
  "nodes": [
    {
      "id": "node-1",
      "type": "on_flow_start",
      "position": {
        "x": 144.0,
        "y": 112.0
      },
      "data": {
        "nodeType": "on_flow_start",
        "label": "On Flow Start",
        "icon": "&#x1F3C1;",
        "headerColor": "#C0392B",
        "inputs": [],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "provider",
            "label": "provider",
            "type": "provider"
          },
          {
            "id": "model",
            "label": "model",
            "type": "model"
          }
        ],
        "pinDefaults": {
          "provider": "lmstudio",
          "model": "qwen/qwen3-next-80b"
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-3",
      "type": "llm_call",
      "position": {
        "x": 384.0,
        "y": 112.0
      },
      "data": {
        "nodeType": "llm_call",
        "label": "LLM Call",
        "icon": "&#x1F4AD;",
        "headerColor": "#3498DB",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "use_context",
            "label": "use_context",
            "type": "boolean",
            "description": "When true, include this run's active context messages (context.messages) in the LLM request. If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "context",
            "label": "context",
            "type": "object",
            "description": "Optional explicit context object for this call (e.g. {messages:[...]}). If provided, context.messages overrides inherited run context messages."
          },
          {
            "id": "provider",
            "label": "provider",
            "type": "provider",
            "description": "LLM provider id (e.g. LMStudio). If unset, uses the node’s configured provider."
          },
          {
            "id": "model",
            "label": "model",
            "type": "model",
            "description": "LLM model id/name. If unset, uses the node’s configured model."
          },
          {
            "id": "system",
            "label": "system",
            "type": "string",
            "description": "Optional system prompt for this single call."
          },
          {
            "id": "prompt",
            "label": "prompt",
            "type": "string",
            "description": "User prompt/content for this single call."
          },
          {
            "id": "tools",
            "label": "tools",
            "type": "tools",
            "description": "Allowlist of tools exposed to the model as ToolSpecs (model may request tool calls; execution is done via a Tool Calls node)."
          },
          {
            "id": "max_in_tokens",
            "label": "max_in_tokens",
            "type": "number",
            "description": "Optional per-call input token budget (max_input_tokens). When set, overrides the run's default _limits.max_input_tokens for this call."
          },
          {
            "id": "temperature",
            "label": "temperature",
            "type": "number",
            "description": "Sampling temperature (0 = deterministic). If unset, uses the node’s configured temperature."
          },
          {
            "id": "seed",
            "label": "seed",
            "type": "number",
            "description": "Seed for deterministic sampling (-1 = random/unset). If unset, uses the node’s configured seed."
          },
          {
            "id": "resp_schema",
            "label": "resp_schema",
            "type": "object",
            "description": "Optional JSON Schema object (type=object) the assistant content must conform to."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "response",
            "label": "response",
            "type": "string",
            "description": "Assistant text content (best-effort). For tool calls, content may be empty."
          },
          {
            "id": "success",
            "label": "success",
            "type": "boolean",
            "description": "True if the LLM call completed successfully."
          },
          {
            "id": "meta",
            "label": "meta",
            "type": "object",
            "description": "Host-facing meta envelope (schema=abstractflow.llm_call.v1.meta). Includes provider/model, usage, trace ids, and lightweight execution metadata."
          },
          {
            "id": "tool_calls",
            "label": "tool_calls",
            "type": "array",
            "description": "Normalized tool call requests. This pin exists to make wiring into Tool Calls / Emit Event nodes simpler."
          },
          {
            "id": "result",
            "label": "result",
            "type": "object",
            "description": "Full normalized LLM result (content, tool_calls, usage, provider/model metadata, trace_id)."
          }
        ],
        "effectConfig": {
          "provider": "lmstudio",
          "model": "devstral-2-123b-instruct-2512"
        },
        "pinDefaults": {
          "prompt": "do you have anything in active context ?",
          "prompt": "do you know anything about me ? "
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-4",
      "type": "memory_note",
      "position": {
        "x": 672.0,
        "y": 112.0
      },
      "data": {
        "nodeType": "memory_note",
        "label": "Memorize",
        "icon": "&#x1F4DD;",
        "headerColor": "#2ECC71",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "keep_in_context",
            "label": "in_context",
            "type": "boolean",
            "description": "When true, also insert the stored note into this run's context.messages (synthetic system message). If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "scope",
            "label": "scope",
            "type": "string",
            "description": "Where to store/index the note: run (this run), session (all runs with the same session_id; owned by an internal session memory run), or global (shared global memory run). If session_id is missing, session falls back to the run-tree root."
          },
          {
            "id": "content",
            "label": "content",
            "type": "string",
            "description": "The note text to store durably (keep it short; prefer references in sources for large payloads)."
          },
          {
            "id": "location",
            "label": "location",
            "type": "string",
            "description": "Optional location label (where the note was produced, e.g. \"flow:my_flow/node-12\"). Useful for filtering."
          },
          {
            "id": "tags",
            "label": "tags",
            "type": "object",
            "description": "Key/value tags for filtering (e.g. {topic:\"memory\", person:\"laurent\"}). Values must be strings."
          },
          {
            "id": "sources",
            "label": "sources",
            "type": "object",
            "description": "Optional provenance refs (e.g. {run_id, span_ids, message_ids}). The note stores refs, not the full source content."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "note_id",
            "label": "note_id",
            "type": "string",
            "description": "The stored note’s span_id / artifact_id. Use it for Recall into context (span_ids) or for precise Recall."
          }
        ],
        "pinDefaults": {
          "keep_in_context": false
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-6",
      "type": "memory_rehydrate",
      "position": {
        "x": 1216.0,
        "y": 256.0
      },
      "data": {
        "nodeType": "memory_rehydrate",
        "label": "Recall into context",
        "icon": "&#x1F4AC;",
        "headerColor": "#2ECC71",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "span_ids",
            "label": "span_ids",
            "type": "array",
            "description": "List of span_ids (artifact_ids) to insert into context.messages. Typically comes from Recall results."
          },
          {
            "id": "placement",
            "label": "placement",
            "type": "string",
            "description": "Where to insert: after_summary | after_system | end."
          },
          {
            "id": "max_messages",
            "label": "max_messages",
            "type": "number",
            "description": "Optional cap on inserted messages across all spans (None/empty = unlimited). Useful to avoid huge contexts."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "inserted",
            "label": "inserted",
            "type": "number",
            "description": "Number of messages inserted into context.messages."
          },
          {
            "id": "skipped",
            "label": "skipped",
            "type": "number",
            "description": "Number of messages skipped (usually due to dedup)."
          }
        ],
        "pinDefaults": {
          "placement": "end"
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-8",
      "type": "make_array",
      "position": {
        "x": 960.0,
        "y": 336.0
      },
      "data": {
        "nodeType": "make_array",
        "label": "Make Array",
        "icon": "[]",
        "headerColor": "#3498DB",
        "inputs": [
          {
            "id": "a",
            "label": "a",
            "type": "any"
          },
          {
            "id": "b",
            "label": "b",
            "type": "any"
          }
        ],
        "outputs": [
          {
            "id": "result",
            "label": "result",
            "type": "array"
          }
        ]
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-9",
      "type": "sequence",
      "position": {
        "x": 960.0,
        "y": 112.0
      },
      "data": {
        "nodeType": "sequence",
        "label": "Sequence",
        "icon": "&#x21E5;",
        "headerColor": "#F39C12",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          }
        ],
        "outputs": [
          {
            "id": "then:0",
            "label": "Then 0",
            "type": "execution"
          },
          {
            "id": "then:1",
            "label": "Then 1",
            "type": "execution"
          },
          {
            "id": "then:2",
            "label": "Then 2",
            "type": "execution"
          },
          {
            "id": "then:3",
            "label": "Then 3",
            "type": "execution"
          }
        ]
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-12",
      "type": "literal_string",
      "position": {
        "x": 672.0,
        "y": 352.0
      },
      "data": {
        "nodeType": "literal_string",
        "label": "Info About Me",
        "icon": "\"",
        "headerColor": "#FF00FF",
        "inputs": [],
        "outputs": [
          {
            "id": "value",
            "label": "value",
            "type": "string"
          }
        ],
        "literalValue": "i am laurent-philippe, a french research scientist"
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-13",
      "type": "llm_call",
      "position": {
        "x": 1216.0,
        "y": -240.0
      },
      "data": {
        "nodeType": "llm_call",
        "label": "NO Recall + Context",
        "icon": "&#x1F4AD;",
        "headerColor": "#3498DB",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "use_context",
            "label": "use_context",
            "type": "boolean",
            "description": "When true, include this run's active context messages (context.messages) in the LLM request. If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "context",
            "label": "context",
            "type": "object",
            "description": "Optional explicit context object for this call (e.g. {messages:[...]}). If provided, context.messages overrides inherited run context messages."
          },
          {
            "id": "provider",
            "label": "provider",
            "type": "provider",
            "description": "LLM provider id (e.g. LMStudio). If unset, uses the node’s configured provider."
          },
          {
            "id": "model",
            "label": "model",
            "type": "model",
            "description": "LLM model id/name. If unset, uses the node’s configured model."
          },
          {
            "id": "system",
            "label": "system",
            "type": "string",
            "description": "Optional system prompt for this single call."
          },
          {
            "id": "prompt",
            "label": "prompt",
            "type": "string",
            "description": "User prompt/content for this single call."
          },
          {
            "id": "tools",
            "label": "tools",
            "type": "tools",
            "description": "Allowlist of tools exposed to the model as ToolSpecs (model may request tool calls; execution is done via a Tool Calls node)."
          },
          {
            "id": "max_in_tokens",
            "label": "max_in_tokens",
            "type": "number",
            "description": "Optional per-call input token budget (max_input_tokens). When set, overrides the run's default _limits.max_input_tokens for this call."
          },
          {
            "id": "temperature",
            "label": "temperature",
            "type": "number",
            "description": "Sampling temperature (0 = deterministic). If unset, uses the node’s configured temperature."
          },
          {
            "id": "seed",
            "label": "seed",
            "type": "number",
            "description": "Seed for deterministic sampling (-1 = random/unset). If unset, uses the node’s configured seed."
          },
          {
            "id": "resp_schema",
            "label": "resp_schema",
            "type": "object",
            "description": "Optional JSON Schema object (type=object) the assistant content must conform to."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "response",
            "label": "response",
            "type": "string",
            "description": "Assistant text content (best-effort). For tool calls, content may be empty."
          },
          {
            "id": "success",
            "label": "success",
            "type": "boolean",
            "description": "True if the LLM call completed successfully."
          },
          {
            "id": "meta",
            "label": "meta",
            "type": "object",
            "description": "Host-facing meta envelope (schema=abstractflow.llm_call.v1.meta). Includes provider/model, usage, trace ids, and lightweight execution metadata."
          },
          {
            "id": "tool_calls",
            "label": "tool_calls",
            "type": "array",
            "description": "Normalized tool call requests. This pin exists to make wiring into Tool Calls / Emit Event nodes simpler."
          },
          {
            "id": "result",
            "label": "result",
            "type": "object",
            "description": "Full normalized LLM result (content, tool_calls, usage, provider/model metadata, trace_id)."
          }
        ],
        "effectConfig": {
          "provider": "lmstudio",
          "model": "gemma-3-1b-it"
        },
        "pinDefaults": {
          "system": "",
          "use_context": false,
          "prompt": "do you know who i am ?",
          "prompt": "do you know anything about me ? "
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-14",
      "type": "llm_call",
      "position": {
        "x": 1536.0,
        "y": 256.0
      },
      "data": {
        "nodeType": "llm_call",
        "label": "Recall + Context",
        "icon": "&#x1F4AD;",
        "headerColor": "#3498DB",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "use_context",
            "label": "use_context",
            "type": "boolean",
            "description": "When true, include this run's active context messages (context.messages) in the LLM request. If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "context",
            "label": "context",
            "type": "object",
            "description": "Optional explicit context object for this call (e.g. {messages:[...]}). If provided, context.messages overrides inherited run context messages."
          },
          {
            "id": "provider",
            "label": "provider",
            "type": "provider",
            "description": "LLM provider id (e.g. LMStudio). If unset, uses the node’s configured provider."
          },
          {
            "id": "model",
            "label": "model",
            "type": "model",
            "description": "LLM model id/name. If unset, uses the node’s configured model."
          },
          {
            "id": "system",
            "label": "system",
            "type": "string",
            "description": "Optional system prompt for this single call."
          },
          {
            "id": "prompt",
            "label": "prompt",
            "type": "string",
            "description": "User prompt/content for this single call."
          },
          {
            "id": "tools",
            "label": "tools",
            "type": "tools",
            "description": "Allowlist of tools exposed to the model as ToolSpecs (model may request tool calls; execution is done via a Tool Calls node)."
          },
          {
            "id": "max_in_tokens",
            "label": "max_in_tokens",
            "type": "number",
            "description": "Optional per-call input token budget (max_input_tokens). When set, overrides the run's default _limits.max_input_tokens for this call."
          },
          {
            "id": "temperature",
            "label": "temperature",
            "type": "number",
            "description": "Sampling temperature (0 = deterministic). If unset, uses the node’s configured temperature."
          },
          {
            "id": "seed",
            "label": "seed",
            "type": "number",
            "description": "Seed for deterministic sampling (-1 = random/unset). If unset, uses the node’s configured seed."
          },
          {
            "id": "resp_schema",
            "label": "resp_schema",
            "type": "object",
            "description": "Optional JSON Schema object (type=object) the assistant content must conform to."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "response",
            "label": "response",
            "type": "string",
            "description": "Assistant text content (best-effort). For tool calls, content may be empty."
          },
          {
            "id": "success",
            "label": "success",
            "type": "boolean",
            "description": "True if the LLM call completed successfully."
          },
          {
            "id": "meta",
            "label": "meta",
            "type": "object",
            "description": "Host-facing meta envelope (schema=abstractflow.llm_call.v1.meta). Includes provider/model, usage, trace ids, and lightweight execution metadata."
          },
          {
            "id": "tool_calls",
            "label": "tool_calls",
            "type": "array",
            "description": "Normalized tool call requests. This pin exists to make wiring into Tool Calls / Emit Event nodes simpler."
          },
          {
            "id": "result",
            "label": "result",
            "type": "object",
            "description": "Full normalized LLM result (content, tool_calls, usage, provider/model metadata, trace_id)."
          }
        ],
        "effectConfig": {
          "provider": "lmstudio",
          "model": "gemma-3-1b-it"
        },
        "pinDefaults": {
          "use_context": true,
          "prompt": "do you know who i am ?",
          "prompt": "do you know anything about me ? "
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-17",
      "type": "memory_query",
      "position": {
        "x": 1216.0,
        "y": 992.0
      },
      "data": {
        "nodeType": "memory_query",
        "label": "Recall (who)",
        "icon": "&#x1F50D;",
        "headerColor": "#2ECC71",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "query",
            "label": "query",
            "type": "string",
            "description": "Keyword query (substring match over span metadata and small previews). Combined with tags/authors/locations using AND semantics."
          },
          {
            "id": "limit",
            "label": "limit",
            "type": "number",
            "description": "Maximum number of spans to return (limit_spans). Default: 5."
          },
          {
            "id": "tags",
            "label": "tags",
            "type": "object",
            "description": "Tag filters as key→string or key→list[string]. Reserved key \"kind\" is ignored."
          },
          {
            "id": "tags_mode",
            "label": "tags_mode",
            "type": "string",
            "description": "How to combine tag keys: all (AND) or any (OR). Within a single key, list values are OR."
          },
          {
            "id": "usernames",
            "label": "usernames",
            "type": "array",
            "description": "Filter by created_by (actor id). Case-insensitive exact match. Empty means no filter."
          },
          {
            "id": "locations",
            "label": "locations",
            "type": "array",
            "description": "Filter by location metadata (or tags.location). Case-insensitive exact match."
          },
          {
            "id": "since",
            "label": "since",
            "type": "string",
            "description": "ISO8601 start time. Matches spans whose [from,to] intersects this range."
          },
          {
            "id": "until",
            "label": "until",
            "type": "string",
            "description": "ISO8601 end time. Matches spans whose [from,to] intersects this range."
          },
          {
            "id": "scope",
            "label": "scope",
            "type": "string",
            "description": "Which span index to query: run | session | global | all. (all queries run+session+global. session uses session_id authority; if session_id is missing, it falls back to the run-tree root.)"
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "results",
            "label": "results",
            "type": "array",
            "description": "Structured match list (meta.matches). Use it to extract span_ids for Recall into context."
          },
          {
            "id": "rendered",
            "label": "rendered",
            "type": "string",
            "description": "Human-readable recall summary (tool-style output string)."
          }
        ],
        "pinDefaults": {
          "query": "laurent",
          "tags_mode": "any"
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-18",
      "type": "llm_call",
      "position": {
        "x": 1216.0,
        "y": 496.0
      },
      "data": {
        "nodeType": "llm_call",
        "label": "Recall + NO Context",
        "icon": "&#x1F4AD;",
        "headerColor": "#3498DB",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "use_context",
            "label": "use_context",
            "type": "boolean",
            "description": "When true, include this run's active context messages (context.messages) in the LLM request. If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "context",
            "label": "context",
            "type": "object",
            "description": "Optional explicit context object for this call (e.g. {messages:[...]}). If provided, context.messages overrides inherited run context messages."
          },
          {
            "id": "provider",
            "label": "provider",
            "type": "provider",
            "description": "LLM provider id (e.g. LMStudio). If unset, uses the node’s configured provider."
          },
          {
            "id": "model",
            "label": "model",
            "type": "model",
            "description": "LLM model id/name. If unset, uses the node’s configured model."
          },
          {
            "id": "system",
            "label": "system",
            "type": "string",
            "description": "Optional system prompt for this single call."
          },
          {
            "id": "prompt",
            "label": "prompt",
            "type": "string",
            "description": "User prompt/content for this single call."
          },
          {
            "id": "tools",
            "label": "tools",
            "type": "tools",
            "description": "Allowlist of tools exposed to the model as ToolSpecs (model may request tool calls; execution is done via a Tool Calls node)."
          },
          {
            "id": "max_in_tokens",
            "label": "max_in_tokens",
            "type": "number",
            "description": "Optional per-call input token budget (max_input_tokens). When set, overrides the run's default _limits.max_input_tokens for this call."
          },
          {
            "id": "temperature",
            "label": "temperature",
            "type": "number",
            "description": "Sampling temperature (0 = deterministic). If unset, uses the node’s configured temperature."
          },
          {
            "id": "seed",
            "label": "seed",
            "type": "number",
            "description": "Seed for deterministic sampling (-1 = random/unset). If unset, uses the node’s configured seed."
          },
          {
            "id": "resp_schema",
            "label": "resp_schema",
            "type": "object",
            "description": "Optional JSON Schema object (type=object) the assistant content must conform to."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "response",
            "label": "response",
            "type": "string",
            "description": "Assistant text content (best-effort). For tool calls, content may be empty."
          },
          {
            "id": "success",
            "label": "success",
            "type": "boolean",
            "description": "True if the LLM call completed successfully."
          },
          {
            "id": "meta",
            "label": "meta",
            "type": "object",
            "description": "Host-facing meta envelope (schema=abstractflow.llm_call.v1.meta). Includes provider/model, usage, trace ids, and lightweight execution metadata."
          },
          {
            "id": "tool_calls",
            "label": "tool_calls",
            "type": "array",
            "description": "Normalized tool call requests. This pin exists to make wiring into Tool Calls / Emit Event nodes simpler."
          },
          {
            "id": "result",
            "label": "result",
            "type": "object",
            "description": "Full normalized LLM result (content, tool_calls, usage, provider/model metadata, trace_id)."
          }
        ],
        "effectConfig": {
          "provider": "lmstudio",
          "model": "gemma-3-1b-it"
        },
        "pinDefaults": {
          "use_context": false,
          "prompt": "do you know who i am ?",
          "prompt": "do you know anything about me ? "
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-19",
      "type": "llm_call",
      "position": {
        "x": 1504.0,
        "y": 896.0
      },
      "data": {
        "nodeType": "llm_call",
        "label": "Direct Recall + No Context",
        "icon": "&#x1F4AD;",
        "headerColor": "#3498DB",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "use_context",
            "label": "use_context",
            "type": "boolean",
            "description": "When true, include this run's active context messages (context.messages) in the LLM request. If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "context",
            "label": "context",
            "type": "object",
            "description": "Optional explicit context object for this call (e.g. {messages:[...]}). If provided, context.messages overrides inherited run context messages."
          },
          {
            "id": "provider",
            "label": "provider",
            "type": "provider",
            "description": "LLM provider id (e.g. LMStudio). If unset, uses the node’s configured provider."
          },
          {
            "id": "model",
            "label": "model",
            "type": "model",
            "description": "LLM model id/name. If unset, uses the node’s configured model."
          },
          {
            "id": "system",
            "label": "system",
            "type": "string",
            "description": "Optional system prompt for this single call."
          },
          {
            "id": "prompt",
            "label": "prompt",
            "type": "string",
            "description": "User prompt/content for this single call."
          },
          {
            "id": "tools",
            "label": "tools",
            "type": "tools",
            "description": "Allowlist of tools exposed to the model as ToolSpecs (model may request tool calls; execution is done via a Tool Calls node)."
          },
          {
            "id": "max_in_tokens",
            "label": "max_in_tokens",
            "type": "number",
            "description": "Optional per-call input token budget (max_input_tokens). When set, overrides the run's default _limits.max_input_tokens for this call."
          },
          {
            "id": "temperature",
            "label": "temperature",
            "type": "number",
            "description": "Sampling temperature (0 = deterministic). If unset, uses the node’s configured temperature."
          },
          {
            "id": "seed",
            "label": "seed",
            "type": "number",
            "description": "Seed for deterministic sampling (-1 = random/unset). If unset, uses the node’s configured seed."
          },
          {
            "id": "resp_schema",
            "label": "resp_schema",
            "type": "object",
            "description": "Optional JSON Schema object (type=object) the assistant content must conform to."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "response",
            "label": "response",
            "type": "string",
            "description": "Assistant text content (best-effort). For tool calls, content may be empty."
          },
          {
            "id": "success",
            "label": "success",
            "type": "boolean",
            "description": "True if the LLM call completed successfully."
          },
          {
            "id": "meta",
            "label": "meta",
            "type": "object",
            "description": "Host-facing meta envelope (schema=abstractflow.llm_call.v1.meta). Includes provider/model, usage, trace ids, and lightweight execution metadata."
          },
          {
            "id": "tool_calls",
            "label": "tool_calls",
            "type": "array",
            "description": "Normalized tool call requests. This pin exists to make wiring into Tool Calls / Emit Event nodes simpler."
          },
          {
            "id": "result",
            "label": "result",
            "type": "object",
            "description": "Full normalized LLM result (content, tool_calls, usage, provider/model metadata, trace_id)."
          }
        ],
        "effectConfig": {
          "provider": "lmstudio",
          "model": "gemma-3-1b-it"
        },
        "pinDefaults": {
          "use_context": false,
          "prompt": "do you know who i am ?",
          "prompt": "do you know anything about me ? "
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-20",
      "type": "literal_json",
      "position": {
        "x": 672.0,
        "y": 0.0
      },
      "data": {
        "nodeType": "literal_json",
        "label": "Tag Who",
        "icon": "{}",
        "headerColor": "#00FFFF",
        "inputs": [],
        "outputs": [
          {
            "id": "value",
            "label": "value",
            "type": "object"
          }
        ],
        "literalValue": {
          "who": "who"
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    }
  ],
  "edges": [
    {
      "id": "edge-1767380748253",
      "source": "node-1",
      "sourceHandle": "provider",
      "target": "node-3",
      "targetHandle": "provider",
      "animated": false
    },
    {
      "id": "edge-1767380750180",
      "source": "node-1",
      "sourceHandle": "model",
      "target": "node-3",
      "targetHandle": "model",
      "animated": false
    },
    {
      "id": "edge-1767380751967",
      "source": "node-1",
      "sourceHandle": "exec-out",
      "target": "node-3",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767382323835",
      "source": "node-3",
      "sourceHandle": "exec-out",
      "target": "node-4",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767384346952",
      "source": "node-4",
      "sourceHandle": "exec-out",
      "target": "node-9",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767384381685",
      "source": "node-9",
      "sourceHandle": "then:1",
      "target": "node-6",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767384393877",
      "source": "node-8",
      "sourceHandle": "result",
      "target": "node-6",
      "targetHandle": "span_ids",
      "animated": false
    },
    {
      "id": "edge-1767384401885",
      "source": "node-4",
      "sourceHandle": "note_id",
      "target": "node-8",
      "targetHandle": "a",
      "animated": false
    },
    {
      "id": "edge-1767384560004",
      "source": "node-12",
      "sourceHandle": "value",
      "target": "node-4",
      "targetHandle": "content",
      "animated": false
    },
    {
      "id": "edge-1767384655421",
      "source": "node-9",
      "sourceHandle": "then:0",
      "target": "node-13",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767384682173",
      "source": "node-6",
      "sourceHandle": "exec-out",
      "target": "node-14",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767395723269",
      "source": "node-9",
      "sourceHandle": "then:2",
      "target": "node-18",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767395745661",
      "source": "node-9",
      "sourceHandle": "then:3",
      "target": "node-17",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767395757402",
      "source": "node-17",
      "sourceHandle": "exec-out",
      "target": "node-19",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1767395818142",
      "source": "node-17",
      "sourceHandle": "rendered",
      "target": "node-19",
      "targetHandle": "system",
      "animated": false
    },
    {
      "id": "edge-1767395931177",
      "source": "node-20",
      "sourceHandle": "value",
      "target": "node-4",
      "targetHandle": "tags",
      "animated": false
    }
  ],
  "entryNode": "node-1",
  "created_at": "2026-01-02T19:06:00.895671",
  "updated_at": "2026-01-16T17:13:47.748831"
}
