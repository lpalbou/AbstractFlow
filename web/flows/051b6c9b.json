{
  "id": "051b6c9b",
  "name": "debate-topic",
  "description": "",
  "interfaces": [],
  "nodes": [
    {
      "id": "node-1",
      "type": "on_flow_start",
      "position": {
        "x": -672.0,
        "y": 304.0
      },
      "data": {
        "nodeType": "on_flow_start",
        "label": "On Flow Start",
        "icon": "&#x1F3C1;",
        "headerColor": "#C0392B",
        "inputs": [],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "topic",
            "label": "topic",
            "type": "string"
          },
          {
            "id": "max_turns",
            "label": "max_turns",
            "type": "number"
          }
        ],
        "pinDefaults": {
          "max_turns": 5
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-3",
      "type": "for",
      "position": {
        "x": 80.0,
        "y": 304.0
      },
      "data": {
        "nodeType": "for",
        "label": "For",
        "icon": "&#x1F522;",
        "headerColor": "#F39C12",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "start",
            "label": "start",
            "type": "number"
          },
          {
            "id": "end",
            "label": "end",
            "type": "number"
          },
          {
            "id": "step",
            "label": "step",
            "type": "number"
          }
        ],
        "outputs": [
          {
            "id": "loop",
            "label": "loop",
            "type": "execution"
          },
          {
            "id": "done",
            "label": "done",
            "type": "execution"
          },
          {
            "id": "i",
            "label": "i",
            "type": "number"
          },
          {
            "id": "index",
            "label": "index",
            "type": "number"
          }
        ],
        "pinDefaults": {
          "start": 0,
          "end": 5,
          "step": 1
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-4",
      "type": "sequence",
      "position": {
        "x": 320.0,
        "y": 304.0
      },
      "data": {
        "nodeType": "sequence",
        "label": "Sequence",
        "icon": "&#x21E5;",
        "headerColor": "#F39C12",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          }
        ],
        "outputs": [
          {
            "id": "then:0",
            "label": "Then 0",
            "type": "execution"
          },
          {
            "id": "then:1",
            "label": "Then 1",
            "type": "execution"
          },
          {
            "id": "then:2",
            "label": "Then 2",
            "type": "execution"
          }
        ]
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-5",
      "type": "llm_call",
      "position": {
        "x": 624.0,
        "y": 208.0
      },
      "data": {
        "nodeType": "llm_call",
        "label": "LLM Call",
        "icon": "&#x1F4AD;",
        "headerColor": "#3498DB",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "include_context",
            "label": "use_context",
            "type": "boolean",
            "description": "When true, include this run's active context messages (context.messages) in the LLM request. If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "max_input_tokens",
            "label": "max_input_tokens",
            "type": "number",
            "description": "Optional per-call input token budget (max_input_tokens). When set, overrides the run's default _limits.max_input_tokens for this call."
          },
          {
            "id": "provider",
            "label": "provider",
            "type": "provider",
            "description": "LLM provider id (e.g. LMStudio). If unset, uses the node’s configured provider."
          },
          {
            "id": "model",
            "label": "model",
            "type": "model",
            "description": "LLM model id/name. If unset, uses the node’s configured model."
          },
          {
            "id": "temperature",
            "label": "temperature",
            "type": "number",
            "description": "Sampling temperature (0 = deterministic). If unset, uses the node’s configured temperature."
          },
          {
            "id": "seed",
            "label": "seed",
            "type": "number",
            "description": "Seed for deterministic sampling (-1 = random/unset). If unset, uses the node’s configured seed."
          },
          {
            "id": "system",
            "label": "system",
            "type": "string",
            "description": "Optional system prompt for this single call."
          },
          {
            "id": "prompt",
            "label": "prompt",
            "type": "string",
            "description": "User prompt/content for this single call."
          },
          {
            "id": "tools",
            "label": "tools",
            "type": "tools",
            "description": "Allowlist of tools exposed to the model as ToolSpecs (model may request tool calls; execution is done via a Tool Calls node)."
          },
          {
            "id": "response_schema",
            "label": "structured_output",
            "type": "object",
            "description": "Optional JSON Schema object (type=object) the assistant content must conform to."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "response",
            "label": "response",
            "type": "string",
            "description": "Assistant text content (best-effort). For tool calls, content may be empty."
          },
          {
            "id": "tool_calls",
            "label": "tool_calls",
            "type": "array",
            "description": "Normalized tool call requests (same as result.tool_calls). This pin exists to make wiring into Tool Calls / Emit Event nodes simpler."
          },
          {
            "id": "result",
            "label": "result",
            "type": "object",
            "description": "Full normalized LLM result (content, tool_calls, usage, provider/model metadata, trace_id)."
          }
        ],
        "pinDefaults": {
          "system": "You are AGENT ONE and you are engaged in a genuine debate with AGENT TWO. At every turn, try to bring a relevant element to help the debate move deeper",
          "include_context": true
        },
        "effectConfig": {
          "provider": "lmstudio",
          "model": "gemma-3n-e2b-it-mlx"
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-6",
      "type": "llm_call",
      "position": {
        "x": 624.0,
        "y": 624.0
      },
      "data": {
        "nodeType": "llm_call",
        "label": "LLM Call",
        "icon": "&#x1F4AD;",
        "headerColor": "#3498DB",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "include_context",
            "label": "use_context",
            "type": "boolean",
            "description": "When true, include this run's active context messages (context.messages) in the LLM request. If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "max_input_tokens",
            "label": "max_input_tokens",
            "type": "number",
            "description": "Optional per-call input token budget (max_input_tokens). When set, overrides the run's default _limits.max_input_tokens for this call."
          },
          {
            "id": "provider",
            "label": "provider",
            "type": "provider",
            "description": "LLM provider id (e.g. LMStudio). If unset, uses the node’s configured provider."
          },
          {
            "id": "model",
            "label": "model",
            "type": "model",
            "description": "LLM model id/name. If unset, uses the node’s configured model."
          },
          {
            "id": "temperature",
            "label": "temperature",
            "type": "number",
            "description": "Sampling temperature (0 = deterministic). If unset, uses the node’s configured temperature."
          },
          {
            "id": "seed",
            "label": "seed",
            "type": "number",
            "description": "Seed for deterministic sampling (-1 = random/unset). If unset, uses the node’s configured seed."
          },
          {
            "id": "system",
            "label": "system",
            "type": "string",
            "description": "Optional system prompt for this single call."
          },
          {
            "id": "prompt",
            "label": "prompt",
            "type": "string",
            "description": "User prompt/content for this single call."
          },
          {
            "id": "tools",
            "label": "tools",
            "type": "tools",
            "description": "Allowlist of tools exposed to the model as ToolSpecs (model may request tool calls; execution is done via a Tool Calls node)."
          },
          {
            "id": "response_schema",
            "label": "structured_output",
            "type": "object",
            "description": "Optional JSON Schema object (type=object) the assistant content must conform to."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "response",
            "label": "response",
            "type": "string",
            "description": "Assistant text content (best-effort). For tool calls, content may be empty."
          },
          {
            "id": "tool_calls",
            "label": "tool_calls",
            "type": "array",
            "description": "Normalized tool call requests (same as result.tool_calls). This pin exists to make wiring into Tool Calls / Emit Event nodes simpler."
          },
          {
            "id": "result",
            "label": "result",
            "type": "object",
            "description": "Full normalized LLM result (content, tool_calls, usage, provider/model metadata, trace_id)."
          }
        ],
        "pinDefaults": {
          "system": "You are AGENT TWO and you are engaged in a genuine debate with AGENT ONE. At every turn, try to bring a relevant element to help the debate move deeper",
          "prompt": "speak with agent1 of the subject that interest you, answer him as well",
          "include_context": true
        },
        "effectConfig": {
          "provider": "lmstudio",
          "model": "gemma-3n-e2b-it-mlx"
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-7",
      "type": "concat",
      "position": {
        "x": -448.0,
        "y": 432.0
      },
      "data": {
        "nodeType": "concat",
        "label": "Concat",
        "icon": "&#x2795;",
        "headerColor": "#E74C3C",
        "inputs": [
          {
            "id": "a",
            "label": "a",
            "type": "string"
          },
          {
            "id": "b",
            "label": "b",
            "type": "string"
          }
        ],
        "outputs": [
          {
            "id": "result",
            "label": "result",
            "type": "string"
          }
        ],
        "concatConfig": {
          "separator": " "
        },
        "pinDefaults": {
          "a": "Let's discuss this TOPIC : "
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-8",
      "type": "memory_note",
      "position": {
        "x": -208.0,
        "y": 304.0
      },
      "data": {
        "nodeType": "memory_note",
        "label": "Memorize",
        "icon": "&#x1F4DD;",
        "headerColor": "#2ECC71",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "keep_in_context",
            "label": "in_context",
            "type": "boolean",
            "description": "When true, also insert the stored note into this run's context.messages (synthetic system message). If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "scope",
            "label": "scope",
            "type": "string",
            "description": "Where to store/index the note: run (this run), session (all runs with the same session_id; owned by an internal session memory run), or global (shared global memory run). If session_id is missing, session falls back to the run-tree root."
          },
          {
            "id": "content",
            "label": "content",
            "type": "string",
            "description": "The note text to store durably (keep it short; prefer references in sources for large payloads)."
          },
          {
            "id": "location",
            "label": "location",
            "type": "string",
            "description": "Optional location label (where the note was produced, e.g. \"flow:my_flow/node-12\"). Useful for filtering."
          },
          {
            "id": "tags",
            "label": "tags",
            "type": "object",
            "description": "Key/value tags for filtering (e.g. {topic:\"memory\", person:\"laurent\"}). Values must be strings."
          },
          {
            "id": "sources",
            "label": "sources",
            "type": "object",
            "description": "Optional provenance refs (e.g. {run_id, span_ids, message_ids}). The note stores refs, not the full source content."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "note_id",
            "label": "note_id",
            "type": "string",
            "description": "The stored note’s span_id / artifact_id. Use it for Recall into context (span_ids) or for precise Recall."
          }
        ],
        "pinDefaults": {
          "keep_in_context": true
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-9",
      "type": "concat",
      "position": {
        "x": 224.0,
        "y": 576.0
      },
      "data": {
        "nodeType": "concat",
        "label": "Concat",
        "icon": "&#x2795;",
        "headerColor": "#E74C3C",
        "inputs": [
          {
            "id": "a",
            "label": "a",
            "type": "string"
          },
          {
            "id": "b",
            "label": "b",
            "type": "string"
          },
          {
            "id": "c",
            "label": "c",
            "type": "string"
          }
        ],
        "outputs": [
          {
            "id": "result",
            "label": "result",
            "type": "string"
          }
        ],
        "concatConfig": {
          "separator": " "
        },
        "pinDefaults": {
          "a": "Continue your debate with AGENT TWO on  : ",
          "c": "\\n\\nAlways start with \"AGENT ONE: \""
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-10",
      "type": "concat",
      "position": {
        "x": 224.0,
        "y": 848.0
      },
      "data": {
        "nodeType": "concat",
        "label": "Concat",
        "icon": "&#x2795;",
        "headerColor": "#E74C3C",
        "inputs": [
          {
            "id": "a",
            "label": "a",
            "type": "string"
          },
          {
            "id": "b",
            "label": "b",
            "type": "string"
          },
          {
            "id": "c",
            "label": "c",
            "type": "string"
          }
        ],
        "outputs": [
          {
            "id": "result",
            "label": "result",
            "type": "string"
          }
        ],
        "concatConfig": {
          "separator": " "
        },
        "pinDefaults": {
          "a": "Continue your debate with AGENT ONE on : ",
          "c": "\\n\\nAlways start with \"AGENT TWO: \""
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-11",
      "type": "llm_call",
      "position": {
        "x": 560.0,
        "y": 1088.0
      },
      "data": {
        "nodeType": "llm_call",
        "label": "LLM Call",
        "icon": "&#x1F4AD;",
        "headerColor": "#3498DB",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "include_context",
            "label": "use_context",
            "type": "boolean",
            "description": "When true, include this run's active context messages (context.messages) in the LLM request. If the pin is not connected, the node checkbox is used. Default: false."
          },
          {
            "id": "max_input_tokens",
            "label": "max_input_tokens",
            "type": "number",
            "description": "Optional per-call input token budget (max_input_tokens). When set, overrides the run's default _limits.max_input_tokens for this call."
          },
          {
            "id": "provider",
            "label": "provider",
            "type": "provider",
            "description": "LLM provider id (e.g. LMStudio). If unset, uses the node’s configured provider."
          },
          {
            "id": "model",
            "label": "model",
            "type": "model",
            "description": "LLM model id/name. If unset, uses the node’s configured model."
          },
          {
            "id": "temperature",
            "label": "temperature",
            "type": "number",
            "description": "Sampling temperature (0 = deterministic). If unset, uses the node’s configured temperature."
          },
          {
            "id": "seed",
            "label": "seed",
            "type": "number",
            "description": "Seed for deterministic sampling (-1 = random/unset). If unset, uses the node’s configured seed."
          },
          {
            "id": "system",
            "label": "system",
            "type": "string",
            "description": "Optional system prompt for this single call."
          },
          {
            "id": "prompt",
            "label": "prompt",
            "type": "string",
            "description": "User prompt/content for this single call."
          },
          {
            "id": "tools",
            "label": "tools",
            "type": "tools",
            "description": "Allowlist of tools exposed to the model as ToolSpecs (model may request tool calls; execution is done via a Tool Calls node)."
          },
          {
            "id": "response_schema",
            "label": "structured_output",
            "type": "object",
            "description": "Optional JSON Schema object (type=object) the assistant content must conform to."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "response",
            "label": "response",
            "type": "string",
            "description": "Assistant text content (best-effort). For tool calls, content may be empty."
          },
          {
            "id": "tool_calls",
            "label": "tool_calls",
            "type": "array",
            "description": "Normalized tool call requests (same as result.tool_calls). This pin exists to make wiring into Tool Calls / Emit Event nodes simpler."
          },
          {
            "id": "result",
            "label": "result",
            "type": "object",
            "description": "Full normalized LLM result (content, tool_calls, usage, provider/model metadata, trace_id)."
          }
        ],
        "effectConfig": {
          "provider": "lmstudio",
          "model": "glm-4.6v"
        },
        "pinDefaults": {
          "system": "You are a deep thinker and reflect on the given context to provide a final answer",
          "include_context": true
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-12",
      "type": "concat",
      "position": {
        "x": 224.0,
        "y": 1296.0
      },
      "data": {
        "nodeType": "concat",
        "label": "Concat",
        "icon": "&#x2795;",
        "headerColor": "#E74C3C",
        "inputs": [
          {
            "id": "a",
            "label": "a",
            "type": "string"
          },
          {
            "id": "b",
            "label": "b",
            "type": "string"
          },
          {
            "id": "c",
            "label": "c",
            "type": "string"
          }
        ],
        "outputs": [
          {
            "id": "result",
            "label": "result",
            "type": "string"
          }
        ],
        "concatConfig": {
          "separator": " "
        },
        "pinDefaults": {
          "a": "CONCLUDE the debate on ",
          "c": " and provide the conclusions and perspectives of each speaker"
        }
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    },
    {
      "id": "node-13",
      "type": "answer_user",
      "position": {
        "x": 944.0,
        "y": 1088.0
      },
      "data": {
        "nodeType": "answer_user",
        "label": "Answer User",
        "icon": "&#x1F4AC;",
        "headerColor": "#9B59B6",
        "inputs": [
          {
            "id": "exec-in",
            "label": "",
            "type": "execution"
          },
          {
            "id": "message",
            "label": "message",
            "type": "string"
          },
          {
            "id": "level",
            "label": "level",
            "type": "string",
            "description": "Message level for hosts (message|warning|error). Defaults to 'message' when unset."
          }
        ],
        "outputs": [
          {
            "id": "exec-out",
            "label": "",
            "type": "execution"
          },
          {
            "id": "message",
            "label": "message",
            "type": "string"
          }
        ]
      },
      "label": null,
      "icon": null,
      "headerColor": null,
      "inputs": [],
      "outputs": []
    }
  ],
  "edges": [
    {
      "id": "edge-1768213466544",
      "source": "node-3",
      "sourceHandle": "loop",
      "target": "node-4",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1768213475070",
      "source": "node-4",
      "sourceHandle": "then:0",
      "target": "node-5",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1768213530194",
      "source": "node-4",
      "sourceHandle": "then:1",
      "target": "node-6",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1768214852213",
      "source": "node-1",
      "sourceHandle": "exec-out",
      "target": "node-8",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1768214859033",
      "source": "node-8",
      "sourceHandle": "exec-out",
      "target": "node-3",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1768214888156",
      "source": "node-1",
      "sourceHandle": "topic",
      "target": "node-7",
      "targetHandle": "b",
      "animated": false
    },
    {
      "id": "edge-1768214890406",
      "source": "node-7",
      "sourceHandle": "result",
      "target": "node-8",
      "targetHandle": "content",
      "animated": false
    },
    {
      "id": "edge-1768214911599",
      "source": "node-1",
      "sourceHandle": "max_turns",
      "target": "node-3",
      "targetHandle": "end",
      "animated": false
    },
    {
      "id": "edge-1768214976223",
      "source": "node-1",
      "sourceHandle": "topic",
      "target": "node-9",
      "targetHandle": "b",
      "animated": false
    },
    {
      "id": "edge-1768214994808",
      "source": "node-9",
      "sourceHandle": "result",
      "target": "node-5",
      "targetHandle": "prompt",
      "animated": false
    },
    {
      "id": "edge-1768215043108",
      "source": "node-1",
      "sourceHandle": "topic",
      "target": "node-10",
      "targetHandle": "b",
      "animated": false
    },
    {
      "id": "edge-1768215090708",
      "source": "node-10",
      "sourceHandle": "result",
      "target": "node-6",
      "targetHandle": "prompt",
      "animated": false
    },
    {
      "id": "edge-1768215229452",
      "source": "node-4",
      "sourceHandle": "then:2",
      "target": "node-11",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1768215270015",
      "source": "node-1",
      "sourceHandle": "topic",
      "target": "node-12",
      "targetHandle": "b",
      "animated": false
    },
    {
      "id": "edge-1768215305459",
      "source": "node-12",
      "sourceHandle": "result",
      "target": "node-11",
      "targetHandle": "prompt",
      "animated": false
    },
    {
      "id": "edge-1768215316235",
      "source": "node-11",
      "sourceHandle": "exec-out",
      "target": "node-13",
      "targetHandle": "exec-in",
      "animated": true
    },
    {
      "id": "edge-1768215319642",
      "source": "node-11",
      "sourceHandle": "response",
      "target": "node-13",
      "targetHandle": "message",
      "animated": false
    }
  ],
  "entryNode": "node-1",
  "created_at": "2026-01-12T10:47:05.989010",
  "updated_at": "2026-01-12T11:05:31.335710"
}